{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840dc88f7c14e1fa",
   "metadata": {},
   "source": [
    "# SW중심대학 디지털 경진대회_SW와 생성AI의 만남 : AI부문\n",
    " - 이 AI 경진대회에서는 5초 분량의 오디오 샘플에서 진짜 사람 목소리와 AI가 생성한 가짜 목소리를 정확하게 구분할 수 있는 모델을 개발하는 것이 목표입니다.\n",
    " - 이 작업은 보안, 사기 감지 및 오디오 처리 기술 향상 등 다양한 분야에서 매우 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7df6856755ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    try:\n",
    "        %conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "    except:\n",
    "        %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "except:\n",
    "    try:\n",
    "        %conda install -c conda-forge librosa\n",
    "    except:\n",
    "        %pip install librosa\n",
    "\n",
    "try:\n",
    "    import wespeaker\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/wenet-e2e/wespeaker.git\n",
    "\n",
    "try:\n",
    "    import huggingface_hub\n",
    "except ImportError:\n",
    "    %pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d73a1",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-04-08T18:51:47.130888",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.123877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "모델 학습 및 추론에 사용할 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbadbd56",
   "metadata": {
    "papermill": {
     "duration": 12.650384,
     "end_time": "2024-04-08T18:51:59.788340",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.137956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.pipelines as pipelines\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41377d36410af6b",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc82cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 18 18:26:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    32W / 250W |    965MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    3   N/A  N/A     35486      C   ...ol11/anaconda3/bin/python      963MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64eb379-e527-46c4-8b12-ead8db628070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "device_num = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device_num)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_num = -1  # cpu\n",
    "print(f\"INFO: Using device - {device}:{device_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2de5d",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2024-04-08T18:51:59.803571",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.796330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n",
    "- 딥러닝 모델을 학습하기 전에 설정해야하는 다양한 매개변수를 정의하는 설정 클래스입니다.\n",
    "- 클래스를 사용하여 학습에 필요한 설정 값을 미리 지정합니다.\n",
    "\n",
    "##### 오디오 신호\n",
    "- 우리가 듣는 소리는 공기의 압력 변화로, 이것을 디지털 신호로 변환한 것이 오디오 신호입니다.\n",
    "- 이 신호는 시간에 따라 변하는 진폭 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a32fb60",
   "metadata": {
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-04-08T18:51:59.828208",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.811225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEED = 20240719  # 재현성을 위해 랜덤 시드 고정\n",
    "    NB_NAME = \"final\"  # ipython 노트북 이름 지정\n",
    "    DATA_ROOT = os.path.join(\".\", \"data\")\n",
    "    SAMPLE_RATE = 16000\n",
    "    AUDIO_LENGTH = 5.0  # 테스트 도메인 오디오 길이\n",
    "    BATCH_SIZE = 128\n",
    "    LR = 1e-5\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_seed(seed=SEED):\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "Config.fix_seed()  # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a682d49",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2024-04-08T18:52:31.507909",
     "exception": false,
     "start_time": "2024-04-08T18:52:31.500578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2459913-1bf6-40b9-b07d-402699590b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    download_url = \"https://drive.usercontent.google.com/download?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&export=download&authuser=0&confirm=t&uuid=c40c278b-d74b-4b75-bc79-09e8a3ccffa4&at=APZUnTUvIVFVM9gjGNUCmDb4YZCy%3A1719807236671\"\n",
    "\n",
    "    Train = \"train\"\n",
    "    Test = \"test\"\n",
    "    Unlabeled = \"unlabeled_data\"\n",
    "\n",
    "    cache_dir = os.path.join(\".\", \"cache\")\n",
    "\n",
    "    def download(self, root='./data', filename=\"download.zip\", md5=None):\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            utils.download_and_extract_archive(self.download_url, root, root, filename, md5)\n",
    "            print(\"Extraction completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists in {filepath}\")\n",
    "        return root\n",
    "\n",
    "    def load_from_source(self, train=True, train_size=1.0):\n",
    "        random_state = 1  # fixed random_state\n",
    "\n",
    "        if os.path.isfile(self.get_dataset_path):\n",
    "            df = pd.read_csv(self.get_dataset_path)\n",
    "        else:\n",
    "            files = os.listdir(self.get_dataset_path.replace(\".csv\", \"\"))\n",
    "            df = pd.DataFrame(dict(\n",
    "                id=[\".\".join(*file.split(\".\")[:-1]) for file in files],\n",
    "                path=[f\"./{self.name}/{file}\" for file in files]\n",
    "            ))\n",
    "\n",
    "        if train_size == 1.0 or train_size == 0.0:\n",
    "            df1, df2 = df, df\n",
    "        elif 'label' in df.columns:\n",
    "            df1, df2, _, _ = split(df, df['label'], test_size=float(1-train_size), random_state=random_state)\n",
    "        else:\n",
    "            df1, df2 = split(df, test_size=float(1-train_size), random_state=random_state)\n",
    "\n",
    "        return df1 if train else df2\n",
    "\n",
    "    @property\n",
    "    def get_dataset_path(self):\n",
    "        return os.path.join(self.download_root, self.name + \".csv\")\n",
    "\n",
    "    @property\n",
    "    def submission_form_path(self):\n",
    "        return os.path.join(self.download_root, \"sample_submission.csv\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_cache_file_path(cls, name):\n",
    "        return os.path.join(cls.cache_dir, Config.NB_NAME, name+\".dataset\")\n",
    "\n",
    "    def __init__(self, root=\".\", name=None, train=True, train_size=1.0, transform=None):\n",
    "        convert_path = lambda path: os.path.join(Config.DATA_ROOT, *path.replace(\"./\", \"\").split(\"/\"))\n",
    "        self.download_root = self.download(root)\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "        if isinstance(transform, list) or isinstance(transform, tuple):\n",
    "            self.transforms = transform\n",
    "        else:\n",
    "            self.transforms = [transform] if transform else []\n",
    "\n",
    "        self.raw_data = self.load_from_source(train, train_size)\n",
    "        if 'label' in self.raw_data.columns:\n",
    "            self.label = [(0, 1) if lb == 'real' else (1, 0) for lb in self.raw_data['label'].tolist()]\n",
    "        else:\n",
    "            self.label = None\n",
    "        self.path = list(map(convert_path, self.raw_data['path'].tolist()))\n",
    "        self.data = self.path\n",
    "        if self.name == self.Train and not train:\n",
    "            self.name = \"valid\"\n",
    "\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.mkdir(self.cache_dir)\n",
    "        if not os.path.isdir(os.path.join(self.cache_dir, Config.NB_NAME)):\n",
    "            os.mkdir(os.path.join(self.cache_dir, Config.NB_NAME))\n",
    "        if os.path.isfile(self.get_cache_file_path(self.name)):\n",
    "            self.data, self.label = torch.load(self.get_cache_file_path(self.name))\n",
    "        else:\n",
    "            new_data, new_label = [], []\n",
    "            for data, label in zip(\n",
    "                    tqdm(self.data, desc=f\"Loading {self.name} dataset\"),\n",
    "                    self.label if self.label else [None for _ in range(len(self.data))]\n",
    "            ):\n",
    "                data, label = self.transform(self.load_audio(data), label)\n",
    "                new_data.append(data), new_label.append(label)\n",
    "            self.data, self.label = new_data, (None if None in new_label else new_label)\n",
    "            torch.save([self.data, self.label], self.get_cache_file_path(self.name))\n",
    "\n",
    "        print(f\"INFO: Dataset is loaded - Name: {self.name}, Size: {len(self.path)}\")\n",
    "\n",
    "    def transform(self, *items):\n",
    "        for transform in self.transforms:\n",
    "            items = transform(*items)\n",
    "        return items\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio(path):\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        if sample_rate != Config.SAMPLE_RATE:\n",
    "            waveform = T.Resample(sample_rate, Config.SAMPLE_RATE)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_silence(duration=Config.AUDIO_LENGTH, sample_rate=Config.SAMPLE_RATE):\n",
    "        num_samples = int(duration * sample_rate)\n",
    "        silence = torch.zeros((1, num_samples))  # assume mono channel\n",
    "        return silence\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(waveform, pad_size):\n",
    "        left_pad = 0\n",
    "        if isinstance(pad_size, list) or isinstance(pad_size, tuple):\n",
    "            left_pad, pad_size = pad_size\n",
    "        right_pad = pad_size - waveform.size(1) - left_pad\n",
    "        return F.pad(waveform, (int(left_pad), int(right_pad)))\n",
    "\n",
    "    @staticmethod\n",
    "    def trim(waveform, target_size):\n",
    "        current_length = waveform.size(1)\n",
    "        if current_length <= target_size:\n",
    "            return waveform\n",
    "        cut_length = current_length - target_size\n",
    "        start_point = random.randint(0, cut_length)\n",
    "        trimmed = []\n",
    "        for channel in waveform:\n",
    "            trimmed.append(channel[start_point:start_point+target_size])\n",
    "        trimmed_tensor = torch.stack(trimmed)\n",
    "        return trimmed_tensor\n",
    "\n",
    "    def augmented(self, *args, **kwargs):\n",
    "        return augmented(self, *args, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label:\n",
    "            return self.data[index], self.label[index]\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c61752529619f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.compliance.kaldi as kaldi\n",
    "\n",
    "def compute_fbank(sample_rate=Config.SAMPLE_RATE, num_mel_bins=80, frame_length=25, frame_shift=10, cmn=True):\n",
    "    def convert(waveform, *args):\n",
    "        features = kaldi.fbank(\n",
    "            waveform.float(), num_mel_bins=num_mel_bins, frame_length=frame_length, frame_shift=frame_shift,\n",
    "            sample_frequency=sample_rate\n",
    "        )\n",
    "        return ((features - torch.mean(features, 0)) if cmn else features), *args\n",
    "    return convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399b06da5d31cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet152(nn.Module):\n",
    "    model_id = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "    root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "    if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "    if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sample_rate = 16000\n",
    "        self.embedding_dim = 256\n",
    "        self.pretrained = wespeaker.load_model_local(self.root_dir).model\n",
    "\n",
    "    def forward(self, features):\n",
    "        *_, embedding = self.pretrained(features)\n",
    "        return embedding\n",
    "\n",
    "    def to_embedding(self, *args):\n",
    "        with torch.no_grad():\n",
    "            embedding = self(args[0].to(device).unsqueeze(0))\n",
    "            return embedding.squeeze(0).cpu(), *args[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8967c36f6abc224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in ./data/download.zip\n",
      "INFO: Dataset is loaded - Name: train, Size: 44350\n",
      "File already exists in ./data/download.zip\n",
      "INFO: Dataset is loaded - Name: valid, Size: 11088\n",
      "File already exists in ./data/download.zip\n",
      "INFO: Dataset is loaded - Name: unlabeled_data, Size: 1264\n",
      "File already exists in ./data/download.zip\n",
      "INFO: Dataset is loaded - Name: test, Size: 50000\n"
     ]
    }
   ],
   "source": [
    "to_filter_bank = compute_fbank()\n",
    "to_embedding = ResNet152().to(device).to_embedding\n",
    "to_tensor = lambda *args: (*args[:-1], torch.tensor(args[-1]))\n",
    "\n",
    "train_set = VoiceDataset(root=Config.DATA_ROOT, name=VoiceDataset.Train, train=True, train_size=0.8, transform=[to_filter_bank, to_embedding, to_tensor])\n",
    "valid_set = VoiceDataset(root=Config.DATA_ROOT, name=VoiceDataset.Train, train=False, train_size=0.8, transform=[to_filter_bank, to_embedding, to_tensor])\n",
    "unlabeled_set = VoiceDataset(root=Config.DATA_ROOT, name=VoiceDataset.Unlabeled, transform=[to_filter_bank, to_embedding])\n",
    "test_set = VoiceDataset(root=Config.DATA_ROOT, name=VoiceDataset.Test, transform=[to_filter_bank, to_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6431ba00adb9c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query Dataset for checking: tensor([-1.0575e-01,  1.2401e-02, -3.2578e-02, -7.2848e-02,  1.6257e-01,\n",
      "         3.0282e-02, -2.7707e-02, -5.2055e-02,  2.2901e-02, -1.4740e-01,\n",
      "         8.9606e-02,  2.3146e-01, -8.3596e-02, -3.3466e-02,  5.4436e-02,\n",
      "        -6.8190e-02,  2.7634e-02,  1.0542e-01,  1.2255e-01,  2.4392e-02,\n",
      "         1.8459e-02,  2.3491e-02,  6.8233e-02,  1.0311e-01,  3.1676e-02,\n",
      "         8.1166e-02,  1.3044e-01, -9.2186e-02, -1.4744e-01, -1.7171e-01,\n",
      "        -9.9712e-03,  2.5658e-02, -9.2734e-02, -2.0453e-02, -5.3845e-03,\n",
      "         1.4175e-01,  4.0326e-02,  1.3282e-01, -1.1752e-01,  4.9214e-02,\n",
      "         8.6155e-02,  8.1838e-02,  6.2421e-02,  9.9082e-02,  1.6571e-02,\n",
      "        -1.0959e-01,  2.5400e-02,  1.7021e-02,  1.5112e-02,  1.4608e-01,\n",
      "        -4.1715e-02, -1.3312e-01,  3.5552e-02,  1.7474e-02, -1.8467e-01,\n",
      "         6.7727e-02,  5.9093e-03, -5.9369e-02,  1.0620e-01,  2.3001e-01,\n",
      "        -1.3601e-01, -5.0523e-02,  5.6959e-02,  9.2574e-02, -6.0151e-02,\n",
      "         1.4229e-01, -9.9871e-02,  1.7478e-02, -1.6694e-01,  1.8337e-01,\n",
      "        -1.0007e-01, -1.7570e-01, -1.1386e-01,  1.1000e-01, -4.0832e-02,\n",
      "         4.7277e-02,  3.4868e-02,  2.6336e-01, -1.7731e-01,  7.3367e-02,\n",
      "         5.7639e-02, -1.1689e-01,  6.0286e-02,  5.6444e-02,  3.4441e-02,\n",
      "         9.0721e-03, -1.1266e-02, -8.0721e-02, -1.1359e-01,  4.9836e-02,\n",
      "        -3.5401e-02,  2.0144e-02,  1.6268e-01, -3.8225e-02,  7.6977e-02,\n",
      "        -1.3041e-01,  7.9789e-02, -3.9588e-02, -1.5639e-02, -8.0588e-02,\n",
      "         3.0220e-01,  3.9823e-02, -6.6182e-02,  2.7091e-01,  1.5140e-02,\n",
      "         5.6650e-02, -1.3845e-01, -6.0243e-02,  1.3972e-02,  7.9683e-02,\n",
      "        -6.2969e-02, -1.0281e-01,  8.6542e-02,  1.5457e-02,  1.5136e-01,\n",
      "         3.8339e-02,  5.2595e-02,  9.8912e-03,  3.7084e-03,  1.5014e-01,\n",
      "         2.5165e-02,  1.1487e-01, -9.3099e-02,  5.4334e-02, -8.2256e-02,\n",
      "        -1.2822e-01,  9.0648e-02,  2.1753e-02,  3.4502e-02,  3.7144e-02,\n",
      "        -8.5434e-02, -7.3772e-02,  1.5912e-01,  4.2821e-02,  9.4108e-02,\n",
      "         1.1904e-01,  9.4228e-03, -1.2330e-02,  2.7340e-01, -2.4466e-02,\n",
      "         1.1715e-01,  8.3437e-03, -1.8171e-01,  1.5344e-02, -1.0434e-01,\n",
      "         1.4512e-03,  3.3616e-02,  5.1893e-02, -4.4367e-02,  9.9851e-02,\n",
      "        -7.4416e-03,  2.2254e-01, -9.1474e-03,  1.1482e-01, -5.4540e-02,\n",
      "        -1.5619e-01,  6.4988e-02,  4.7669e-02,  9.0094e-02,  3.4101e-02,\n",
      "         5.3860e-02,  1.2090e-01, -3.6647e-02,  1.5859e-01, -4.5520e-02,\n",
      "        -1.0750e-01,  1.1531e-01,  6.8258e-03,  7.7555e-06,  2.0425e-02,\n",
      "         4.2166e-02,  2.3463e-02, -5.5610e-03, -9.7412e-02, -1.8217e-01,\n",
      "        -6.7089e-02,  8.2240e-02, -1.1995e-01,  1.1004e-01, -8.7922e-02,\n",
      "         2.4214e-02,  1.8943e-01, -1.4606e-03,  1.7480e-02, -7.2634e-03,\n",
      "         6.8938e-02,  1.0227e-01, -7.4674e-02, -4.6149e-02, -8.1790e-02,\n",
      "        -3.4547e-02,  1.1634e-01, -1.1223e-01, -2.4274e-02,  1.4821e-01,\n",
      "        -1.2891e-01,  1.6108e-01,  1.8732e-01,  1.4055e-01, -1.5414e-01,\n",
      "        -1.0809e-01,  1.3540e-01, -9.2109e-02,  1.0275e-01, -1.3885e-02,\n",
      "        -8.1898e-02, -4.6508e-02,  1.1737e-01, -2.2243e-01,  2.9379e-01,\n",
      "        -5.8786e-04,  1.3346e-02,  1.6944e-02,  8.0264e-03, -5.8700e-02,\n",
      "        -7.5525e-02,  7.2254e-02,  1.0460e-01,  5.1313e-02,  1.2274e-01,\n",
      "        -1.3223e-01,  1.6538e-01,  2.2875e-02,  8.9667e-03,  1.8796e-01,\n",
      "        -1.1992e-01,  8.6162e-02, -7.9890e-02, -1.2056e-01,  4.0671e-02,\n",
      "         5.9072e-02, -6.9200e-02, -6.4939e-02,  1.0433e-01, -6.2062e-02,\n",
      "        -1.3399e-01,  1.6233e-01,  3.5135e-02, -2.2866e-01, -7.5754e-02,\n",
      "        -7.0268e-02,  9.4780e-02, -7.5809e-03, -5.1689e-02, -3.3336e-02,\n",
      "         6.0775e-02,  1.2683e-02, -1.8230e-01, -5.0547e-02,  1.8899e-02,\n",
      "         1.2164e-01,  2.5095e-02, -3.4413e-02, -6.9374e-02, -1.9139e-01,\n",
      "        -6.2655e-02]) torch.Size([256]) tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19535</th>\n",
       "      <td>NQJUDUMG</td>\n",
       "      <td>./train/NQJUDUMG.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37414</th>\n",
       "      <td>SGACBBDI</td>\n",
       "      <td>./train/SGACBBDI.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40645</th>\n",
       "      <td>SIBSFMAP</td>\n",
       "      <td>./train/SIBSFMAP.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>LLBQPFAD</td>\n",
       "      <td>./train/LLBQPFAD.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>ZWYRTAOF</td>\n",
       "      <td>./train/ZWYRTAOF.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50057</th>\n",
       "      <td>BDFFJCBX</td>\n",
       "      <td>./train/BDFFJCBX.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>NEFSVUCS</td>\n",
       "      <td>./train/NEFSVUCS.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>MJFGSHIR</td>\n",
       "      <td>./train/MJFGSHIR.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>USIDOXOR</td>\n",
       "      <td>./train/USIDOXOR.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>UNQUGLKV</td>\n",
       "      <td>./train/UNQUGLKV.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                  path label\n",
       "19535  NQJUDUMG  ./train/NQJUDUMG.ogg  fake\n",
       "37414  SGACBBDI  ./train/SGACBBDI.ogg  fake\n",
       "40645  SIBSFMAP  ./train/SIBSFMAP.ogg  fake\n",
       "16487  LLBQPFAD  ./train/LLBQPFAD.ogg  real\n",
       "954    ZWYRTAOF  ./train/ZWYRTAOF.ogg  real\n",
       "...         ...                   ...   ...\n",
       "50057  BDFFJCBX  ./train/BDFFJCBX.ogg  fake\n",
       "32511  NEFSVUCS  ./train/NEFSVUCS.ogg  real\n",
       "5192   MJFGSHIR  ./train/MJFGSHIR.ogg  fake\n",
       "12172  USIDOXOR  ./train/USIDOXOR.ogg  real\n",
       "33003  UNQUGLKV  ./train/UNQUGLKV.ogg  fake\n",
       "\n",
       "[44350 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"INFO: Query Dataset for checking:\", train_set[0][0], train_set[0][0].shape, train_set[0][1])\n",
    "train_set.raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88953010265ad3a7",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc892c78a8a9bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented(dataset, amount=1/4, duration=Config.AUDIO_LENGTH):  # augmentation for (1, 1), (0, 0) samples\n",
    "    if not dataset.label:\n",
    "        raise NotImplementedError(\"ERROR: Augmentation is not available for unlabeled dataset.\")\n",
    "\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    tensor_path = dataset.get_cache_file_path(dataset.name+f\"_augmented{amount}\")\n",
    "\n",
    "    if os.path.isfile(tensor_path):\n",
    "        dataset.data, dataset.label = torch.load(tensor_path)\n",
    "        print(f\"INFO: Augmented dataset({amount}) of {dataset.name} is loaded - Size: {len(dataset.path)}\")\n",
    "        return dataset\n",
    "\n",
    "    waveform_length = int(duration * Config.SAMPLE_RATE)  # fit to test domain audio length\n",
    "\n",
    "    fakes = [odo for odo, label in zip(dataset.path, dataset.label) if label.tolist() == [1, 0]]\n",
    "    reals = [odo for odo, label in zip(dataset.path, dataset.label) if label.tolist() == [0, 1]]\n",
    "    fake_random = lambda l: random.sample(fakes, l)\n",
    "    real_random = lambda l: random.sample(fakes, l)\n",
    "    rand_length = lambda odo: (random.randint(0, waveform_length - odo.size(1)), waveform_length)\n",
    "    append_data, append_label = dataset.data.append, dataset.label.append\n",
    "    append = lambda args: (append_data(args[0]), append_label(args[1]))\n",
    "\n",
    "    # (1, 1) - [fake 1, real 1]\n",
    "    for fake, real in zip(fake_random(int(len(fakes)*amount)), tqdm(real_random(int(len(reals)*amount)), desc=\"Fake 1, Real 1\")):\n",
    "        data1_loaded = dataset.trim(dataset.load_audio(fake), waveform_length)\n",
    "        data2_loaded = dataset.trim(dataset.load_audio(real), waveform_length)\n",
    "        data1_loaded = dataset.pad(data1_loaded, rand_length(data1_loaded))\n",
    "        data2_loaded = dataset.pad(data2_loaded, rand_length(data2_loaded))\n",
    "        append(dataset.transform((data1_loaded + data2_loaded) / 2, (1, 1)))\n",
    "\n",
    "    # (0, 0) - [fake 0, real 0]\n",
    "    for _ in tqdm(range(int(len(fakes)/100*amount)), desc=\"Fake 0, Real 0\"):\n",
    "        append(dataset.transform(dataset.generate_silence(duration=duration), (0, 0)))\n",
    "\n",
    "    torch.save([dataset.data, dataset.label], tensor_path)\n",
    "    print(f\"INFO: Augmented dataset({amount}) of {dataset.name} is saved to\", tensor_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87b2113a2d3a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Augmented dataset(0.25) of train is loaded - Size: 44350\n",
      "INFO: Augmented dataset(0.25) of valid is loaded - Size: 11088\n"
     ]
    }
   ],
   "source": [
    "# (1, 1), (0, 0) label augmentation for 1/4 of train dataset\n",
    "train_aug = train_set.augmented(1/4)\n",
    "valid_aug = valid_set.augmented(1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c359eed10e6ab",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "    - DataLoader는 구축된 데이터셋에서 배치크기(batch_size)에 맞게 데이터를 추출하고, 필요에 따라 섞거나(shuffle=True) 순서대로 반환(shuffle=False)하는 역할을 합니다.\n",
    "    - 훈련 데이터(train_loader)는 일반적으로 섞어서 모델이 데이터에 덜 편향되게 학습하도록하며,\n",
    "      검증 데이터(val_loader)는 모델 성능 평가를 위해 순서대로 사용하고,\n",
    "      테스트 데이터(test_loader)는 최종적인 추론을 위해 사용합니다.\n",
    "\n",
    "    이렇게 DataLoader를 사용함으로써, 효율적인 데이터 처리와 모델 학습 및 평가가 가능해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0eed0ea9ba8e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_loader(data_loader, batch_size=Config.BATCH_SIZE):\n",
    "    while True:\n",
    "        for data in data_loader:\n",
    "            if len(data) != batch_size:\n",
    "                continue\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73f32489efd727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = Config.BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tr_aug_loader = DataLoader(train_aug, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_aug_loader = DataLoader(valid_aug, batch_size=BATCH_SIZE, shuffle=False)\n",
    "unlabeled_loader = DataLoader(unlabeled_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf2531542bfda5",
   "metadata": {},
   "source": [
    "## Define Noise Model\n",
    "\n",
    "##### DomainDifferenceLearner: 학습 도메인과 테스트 도메인의 오디오 특징을 추출하고, 그 차이를 학습합니다.\n",
    "##### NoiseGenerator: 도메인 차이를 입력으로 받아 그에 해당하는 노이즈를 생성합니다.\n",
    "##### DomainDiscriminator: 생성된 노이즈가 추가된 학습 도메인 오디오와 실제 테스트 도메인 오디오를 구분하려 합니다.\n",
    "\n",
    "학습 과정:\n",
    "- 도메인 차이를 학습합니다.\n",
    "- 학습된 차이를 바탕으로 노이즈를 생성합니다.\n",
    "- 생성된 노이즈를 학습 도메인 오디오에 추가합니다.\n",
    "- 판별자를 통해 생성된 노이지 오디오가 테스트 도메인과 유사해지도록 학습합니다.\n",
    "- 도메인 차이 보존 손실을 통해 생성된 노이즈가 실제 도메인 차이를 반영하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4e22f5b6a2c7c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDifferenceLearner(nn.Module):\n",
    "    def __init__(self, input_channel, latent_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channel, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(256, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=2)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ade760adb31f75ae",
   "metadata": {},
   "source": [
    "class NoiseGenerator(nn.Module):\n",
    "    def __init__(self, latent_size, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ee024d6ae30e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channel, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(input_channel, 64, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            spectral_norm(nn.Linear(256 ** 2 // 8, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d6e87865b76be187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_channel': 1, 'latent_size': 128, 'output_size': 256}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = dict(\n",
    "    input_channel=1,\n",
    "    latent_size=128,\n",
    "    output_size=256\n",
    ")\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f7cf684b37ea4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomainDifferenceLearner(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_learner = DomainDifferenceLearner(**model_params)\n",
    "domain_learner.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2c4d32656f22ebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoiseGenerator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_generator = NoiseGenerator(**model_params)\n",
    "noise_generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c190eb8a8b8b8ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomainDiscriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=8192, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_discriminator = DomainDiscriminator(**model_params)\n",
    "domain_discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35ef55a3b525cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_gan = nn.BCELoss()\n",
    "criterion_diff = nn.MSELoss()\n",
    "\n",
    "optimizer_learn = torch.optim.Adam(domain_learner.parameters(), lr=0.0001)\n",
    "optimizer_gen = torch.optim.Adam(noise_generator.parameters(), lr=0.0002)\n",
    "optimizer_disc = torch.optim.Adam(domain_discriminator.parameters(), lr=0.0004)\n",
    "\n",
    "def hinge_loss_d(real_output, fake_output):\n",
    "    return torch.mean(F.relu(1 - real_output)) + torch.mean(F.relu(1 + fake_output))\n",
    "\n",
    "def hinge_loss_g(fake_output):\n",
    "    return -torch.mean(fake_output)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "4a4a04eba935f3d6"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ff81dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones_like(d_interpolates),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e6e329394527c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8dd0a4f5ab43839b8f4f49c0e74b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6954d740794b0fa001c2cfba78a577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Step [346/347], Loss_D: 18.525682, Loss_G: 42.871819   \n",
      "Epoch [10/100], Step [346/347], Loss_D: 0.000016, Loss_G: 57.158884   \n",
      "Epoch [15/100], Step [346/347], Loss_D: 0.000004, Loss_G: 56.693651   \n",
      "Epoch [20/100], Step [346/347], Loss_D: 0.000002, Loss_G: 56.326333   \n",
      "Epoch [25/100], Step [346/347], Loss_D: 0.000001, Loss_G: 55.752930   \n",
      "Epoch [28/100], Step [184/347], Loss_D: 0.000000, Loss_G: 55.099751   "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "log_interval = 5\n",
    "\n",
    "train_domain_loader = tr_aug_loader\n",
    "test_domain_loader = infinite_loader(unlabeled_loader)\n",
    "\n",
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Running Epochs\")\n",
    "train_progress = tqdm(total=len(train_domain_loader), desc=\"Training\")\n",
    "for epoch in epochs:\n",
    "    train_progress.reset(total=len(train_domain_loader))\n",
    "    losses = [[], []]\n",
    "\n",
    "    for (step, (train_audio, _)), test_audio in zip(enumerate(train_domain_loader), test_domain_loader):\n",
    "        train_audio, test_audio = train_audio.unsqueeze(1).to(device), test_audio.unsqueeze(1).to(device)\n",
    "        if train_audio.size(0) != test_audio.size(0):\n",
    "            continue\n",
    "\n",
    "        # Learning domain difference between train and test\n",
    "        optimizer_learn.zero_grad()\n",
    "        train_features = domain_learner(train_audio)\n",
    "        test_features = domain_learner(test_audio)\n",
    "        domain_diff = test_features - train_features\n",
    "\n",
    "        # Generate noise\n",
    "        generated_noise = noise_generator(domain_diff)\n",
    "        noisy_train_audio = train_audio + generated_noise.unsqueeze(1)\n",
    "\n",
    "        # Train discriminator\n",
    "        optimizer_disc.zero_grad()\n",
    "        real_output = domain_discriminator(test_audio)\n",
    "        fake_output = domain_discriminator(noisy_train_audio.detach())\n",
    "\n",
    "        loss_d_real = criterion_gan(real_output, torch.ones_like(real_output))\n",
    "        loss_d_fake = criterion_gan(fake_output, torch.zeros_like(fake_output))\n",
    "        #loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "        loss_d = hinge_loss_d(real_output, fake_output)\n",
    "        #gradient_penalty = compute_gradient_penalty(domain_discriminator, test_audio, noisy_train_audio.detach())\n",
    "        #loss_d = -torch.mean(real_output) + torch.mean(fake_output) + 10 * gradient_penalty\n",
    "        losses[0].append(loss_d.item())\n",
    "        loss_d.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train domain difference learner and noise generator\n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_learn.zero_grad()\n",
    "\n",
    "        fake_output = domain_discriminator(noisy_train_audio)\n",
    "        loss_gan = criterion_gan(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "        # Domain difference retention loss\n",
    "        noisy_train_features = domain_learner(noisy_train_audio)\n",
    "        loss_diff = criterion_diff(noisy_train_features, test_features)\n",
    "\n",
    "        #loss_g = loss_gan + 10 * loss_diff  # put weights on the domain diff retention loss\n",
    "        loss_g = hinge_loss_g(fake_output) + 10 * loss_diff\n",
    "        losses[1].append(loss_g.item())\n",
    "        loss_g.backward()\n",
    "        optimizer_gen.step()\n",
    "        optimizer_learn.step()\n",
    "\n",
    "        train_progress.update(1)\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{step}/{len(train_loader)}], Loss_D: {np.mean(losses[0]):.6f}, Loss_G: {np.mean(losses[1]):.6f}\", end=\"   \")\n",
    "\n",
    "    print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{step}/{len(train_loader)}], Loss_D: {np.mean(losses[0]):.6f}, Loss_G: {np.mean(losses[1]):.6f}\", end=\"\\n\" if epoch % log_interval == 0 else \"   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dd58f",
   "metadata": {},
   "source": [
    "1. 판별자 손실 (Loss_D):\n",
    "- 이상적으로는 약 0.5 주변으로 수렴해야 합니다.\n",
    "- 0.5에 가까워진다는 것은 판별자가 실제 테스트 도메인 오디오와 노이즈가 추가된 학습 도메인 오디오를 구분하기 어려워한다는 의미입니다.\n",
    "- 만약 이 값이 0에 가까워지면 판별자가 너무 강해진 것이고, 1에 가까워지면 너무 약해진 것입니다.\n",
    "\n",
    "2. 생성기 손실 (Loss_G):\n",
    "- 초기에는 높은 값을 가지다가 점차 감소하는 경향을 보여야 합니다.\n",
    "- 하지만 0으로 수렴하지는 않아야 합니다. 너무 낮아지면 모드 붕괴(mode collapse)가 일어날 수 있습니다.\n",
    "\n",
    "3. 도메인 차이 보존 손실 (Loss_diff):\n",
    "- 이 값도 점차 감소해야 하지만, 완전히 0이 되어서는 안 됩니다.\n",
    "- 0에 가까워진다는 것은 생성된 노이즈가 도메인 간 차이를 잘 포착하고 있다는 의미입니다.\n",
    "\n",
    "4. GAN 손실 (Loss_gan):\n",
    "- 이 값은 생성기가 판별자를 얼마나 잘 속이는지를 나타냅니다.\n",
    "- 학습이 진행됨에 따라 이 값이 점차 감소해야 하지만, 너무 빠르게 0에 가까워지면 안 됩니다.\n",
    "\n",
    "##### 이상적인 학습 과정은 다음과 같습니다:\n",
    "- 초기에는 모든 손실 값들이 높습니다.\n",
    "- 학습이 진행됨에 따라 Loss_D는 0.5에 수렴합니다.\n",
    "- Loss_G, Loss_diff, Loss_gan은 점차 감소하지만, 어느 정도 수준에서 안정화됩니다.\n",
    "- 손실 값들이 큰 변동 없이 안정적으로 유지됩니다.\n",
    "\n",
    "구체적인 수치는 데이터셋과 모델 구조에 따라 다를 수 있지만, 대략적인 가이드라인은 다음과 같습니다:\n",
    "- Loss_D: 0.4 ~ 0.6 사이\n",
    "- Loss_G: 초기 값의 10~20% 수준으로 감소 후 안정화\n",
    "- Loss_diff: 초기 값의 5~10% 수준으로 감소 후 안정화\n",
    "- Loss_gan: 2~3 정도로 안정화\n",
    "\n",
    "이러한 경향을 보이면서 동시에 생성된 노이즈가 추가된 학습 도메인 오디오가 실제로 테스트 도메인과 유사해지는지를 주관적으로 평가해보는 것도 중요합니다. 오디오 샘플을 직접 들어보거나 스펙트로그램을 시각적으로 비교해보는 것이 도움이 될 수 있습니다.\n",
    "\n",
    "또한, 학습된 모델을 사용하여 실제 태스크(예: 가짜 음성 탐지)의 성능이 향상되는지를 확인하는 것도 모델의 성공을 판단하는 중요한 지표가 될 수 있습니다."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Freeze layer\n",
    "for param in optimizer_gen.parameters():\n",
    "    param.requires_grad = False"
   ],
   "id": "7efb7be03d08818a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_noise(size):\n",
    "    rand = torch.randn(size)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            noise = noise_generator(rand.to(device))\n",
    "        except NameError:\n",
    "            noise = torch.zeros(size)\n",
    "    return noise.to(device)"
   ],
   "id": "d54011918395e6d2"
  },
  {
   "cell_type": "markdown",
   "id": "3fc3c2e6cb0f11cb",
   "metadata": {},
   "source": [
    "## Define Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18210b482cbe90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multi_label_auc(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for idx in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, idx], y_scores[:, idx])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebcc59c8b2ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_manager(num_epochs, log_interval=5):\n",
    "    cache = [0.0 for _ in range(10)]\n",
    "\n",
    "    def update_state(\n",
    "            progress, epoch=None, step=None, steps=None, train_loss=None,\n",
    "            domain_acc=None, domain_loss=None, valid_acc=None, valid_loss=None, cross_acc=None, cross_loss=None\n",
    "    ):\n",
    "        new_state = [epoch, step, steps, train_loss, domain_acc, domain_loss, valid_acc, valid_loss, cross_acc, cross_loss]\n",
    "        for i, state in enumerate(new_state):\n",
    "            if state:\n",
    "                cache[i] = state\n",
    "        epoch, step, steps, train_loss, domain_acc, domain_loss, valid_acc, valid_loss, cross_acc, cross_loss = cache\n",
    "        progress.update(1)\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{step}/{steps}], Loss: {train_loss:.6f}, Domain: {domain_acc:.6%} | {domain_loss:.6f}, \"\n",
    "            + f\"Valid: {valid_acc:.6%} | {valid_loss:.6f}, Cross Valid: {cross_acc:.6%} | {cross_loss:.6f}\", end=\"   \")\n",
    "\n",
    "        def result():\n",
    "            print(end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")\n",
    "            return cross_acc\n",
    "        return result\n",
    "\n",
    "    return tqdm(range(1, num_epochs+1), desc=\"Running Epochs\"), update_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c4bc7cda01359",
   "metadata": {},
   "source": [
    "### 1. Feature Extraction Layer (ResNet-like structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbeaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.attention_weights = nn.Parameter(torch.randn(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = F.softmax(self.attention_weights, dim=0)\n",
    "        weighted_features = x * attention_scores\n",
    "        return weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder with skip connections\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        self.skip1 = nn.Linear(embedding_size, hidden_size)\n",
    "\n",
    "        self.encoder_block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//2)\n",
    "        )\n",
    "        self.skip2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "\n",
    "        self.encoder_block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size//2, hidden_size//4),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//4)\n",
    "        )\n",
    "        self.skip3 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "\n",
    "        self.encoder_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_size//4, hidden_size//8),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//8)\n",
    "        )\n",
    "        self.skip4 = nn.Linear(hidden_size//4, hidden_size//8)\n",
    "\n",
    "        self.final_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size//8, latent_size),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionLayer(latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder_block1(x) + self.skip1(x)\n",
    "        x2 = self.encoder_block2(x1) + self.skip2(x1)\n",
    "        x3 = self.encoder_block3(x2) + self.skip3(x2)\n",
    "        x4 = self.encoder_block4(x3) + self.skip4(x3)\n",
    "        encoded = self.final_encoder(x4)\n",
    "        attention = self.attention(encoded)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b70ef4edc13d3",
   "metadata": {},
   "source": [
    "### 2. Domain Adaptation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3e4b7ac2426b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdaptationLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_map = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_map(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e96f3e906703be",
   "metadata": {},
   "source": [
    "### 3. Adversarial Domain Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e901e892e8a461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDomainClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # output => domain 0(train), 1(test)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c886fc4d42bde",
   "metadata": {},
   "source": [
    "### 4. Uncertainty-Aware Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0c6ccd919a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyAwareClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feature_dim, 2)\n",
    "        self.uncertainty_threshold = nn.Parameter(torch.tensor([0.5]))\n",
    "\n",
    "    def forward(self, features):\n",
    "        logits = self.classifier(features)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "        uncertainty = -(probabilities * torch.log(probabilities)).sum(dim=1)\n",
    "        uncertain = (uncertainty > self.uncertainty_threshold).float()\n",
    "        \n",
    "        return probabilities, uncertain.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b62f8374aaa68",
   "metadata": {},
   "source": "### 5. Total Fake & Real Voice Detection Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ab1b9d9949940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDetector(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = FeatureExtractor(\n",
    "            embedding_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            latent_size=latent_size\n",
    "        )\n",
    "        self.domain_adapter = DomainAdaptationLayer(feature_dim=latent_size)\n",
    "        self.domain_classifier = AdversarialDomainClassifier(feature_dim=latent_size)\n",
    "        self.classifier = UncertaintyAwareClassifier(latent_size)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        features = self.encoder(embedding)\n",
    "        adapted = self.domain_adapter(features)\n",
    "        output, uncertain = self.classifier(features)\n",
    "        return F.sigmoid(output), self.domain_classifier(adapted), uncertain"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleVoiceDetector(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = FeatureExtractor(\n",
    "            embedding_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            latent_size=latent_size\n",
    "        )\n",
    "        self.classifier = nn.Linear(latent_size, 2)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        features = self.encoder(embedding)\n",
    "        return F.sigmoid(self.classifier(features)), None, None"
   ],
   "id": "7ed44c5167b1fe7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e5e60ef1fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "model_params = dict(\n",
    "    embedding_dim=256,\n",
    "    hidden_size=1024,\n",
    "    latent_size=128\n",
    ")\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d59ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_model = VoiceDetector(**model_params)\n",
    "complex_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd022a8a8d7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleVoiceDetector(**model_params)\n",
    "simple_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "uncertainty_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = simple_model\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=Config.LR)"
   ],
   "id": "ff2edf8aa5304a38"
  },
  {
   "cell_type": "markdown",
   "id": "8c46d66df90d322f",
   "metadata": {},
   "source": "### Training & Validation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple Model",
   "id": "3b5bb02ce1d19b40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ed492debdd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "epochs, update_state = state_manager(num_epochs, log_interval=5)\n",
    "\n",
    "performance = 0\n",
    "tr_ldr, val_ldr, crv_ldr = tr_aug_loader, val_aug_loader, valid_loader\n",
    "tr_len, val_len, crv_len = len(tr_ldr), len(val_ldr), len(crv_ldr)\n",
    "\n",
    "with (tqdm(total=tr_len, desc=\"Training\") as tr_pgrs, tqdm(total=val_len, desc=\"Validation\") as val_pgrs, tqdm(total=crv_len, desc=\"Cross Validation\") as crv_pgrs):\n",
    "    for epoch in epochs:\n",
    "        [p.reset(total=l) for p, l in zip((tr_pgrs, val_pgrs, crv_pgrs), (tr_len, val_len, crv_len))]  # progressbar reset\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for step, train_inputs in enumerate(tr_ldr):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, labels = (data.float().to(device) for data in train_inputs)\n",
    "            outputs, *_ = model(features + generate_noise(features.size()))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            update_state(tr_pgrs, epoch, step, tr_len, loss.item())\n",
    "\n",
    "        # Cross Domain Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for loader, loader_len, progress, cross in zip(\n",
    "                    (val_ldr, crv_ldr), (val_len, crv_len), (val_pgrs, crv_pgrs), (False, True)\n",
    "            ):\n",
    "                val_loss, val_labels, val_outputs = [], [], []\n",
    "\n",
    "                for inputs in loader:\n",
    "                    features, labels = (data.float().to(device) for data in inputs)\n",
    "                    predicted, *_ = model(features)\n",
    "\n",
    "                    val_loss.append(criterion(predicted, labels).item())\n",
    "                    val_labels.append(labels.cpu().numpy())\n",
    "                    val_outputs.append(predicted.cpu().numpy())\n",
    "                    val_acc = multi_label_auc(np.concatenate(val_labels, axis=0), np.concatenate(val_outputs, axis=0))\n",
    "\n",
    "                    if cross:\n",
    "                        result = update_state(progress, cross_acc=val_acc, cross_loss=np.mean(val_loss))\n",
    "                    else:\n",
    "                        update_state(progress, valid_acc=val_acc, valid_loss=np.mean(val_loss))\n",
    "\n",
    "        performance = result()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Complex Model",
   "id": "69254235d923a00f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "epochs, update_state = state_manager(num_epochs, log_interval=5)\n",
    "\n",
    "performance = 0\n",
    "loaders = (tr_aug_loader(), unlabeled_loader, val_aug_loader, valid_loader)\n",
    "lens = list(map(len, loaders))\n",
    "\n",
    "with (tqdm(total=lens[0], desc=\"Training\") as tr_pgrs, tqdm(total=lens[2], desc=\"Validation\") as val_pgrs, tqdm(total=lens[3], desc=\"Cross Validation\") as crv_pgrs):\n",
    "    for epoch in epochs:\n",
    "        [p.reset(total=l) for p, l in zip((tr_pgrs, val_pgrs, crv_pgrs), lens)]  # progressbar reset\n",
    "\n",
    "        # Train & Domain Adapt\n",
    "        model.train()\n",
    "        for step, (train_inputs, adpt_inputs) in enumerate(zip(loaders[0], infinite_loader(loaders[1]))):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, labels = (data.float().to(device) for data in train_inputs)\n",
    "            outputs, train_domain = model(features)\n",
    "            _, test_domain = model(adpt_inputs.float().to(device))\n",
    "            domain_outputs = torch.cat([train_domain, test_domain])\n",
    "            domain_labels = torch.cat([torch.zeros(train_domain.shape), torch.ones(test_domain.shape)]).to(device)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            domain_loss = domain_criterion(domain_outputs, domain_labels)  # induce domain classifier do wrong pred\n",
    "            domain_acc = ((F.softmax(domain_outputs, dim=1) >= 0.5) == domain_labels).sum() / len(domain_labels)\n",
    "\n",
    "            (loss + domain_loss).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            update_state(tr_pgrs, epoch, step, lens[0], loss.item(), domain_acc, domain_loss.item())\n",
    "\n",
    "        # Cross Domain Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for loader, loader_len, progress, cross in zip(loaders[-2:], lens[-2:], (val_pgrs, crv_pgrs), (False, True)):\n",
    "                val_loss, val_labels, val_outputs = [], [], []\n",
    "                \n",
    "                for inputs in loader:\n",
    "                    features, labels = (data.float().to(device) for data in inputs)\n",
    "                    predicted = model(features)\n",
    "\n",
    "                    val_loss.append(criterion(predicted, labels).item())\n",
    "                    val_labels.append(labels.cpu().numpy())\n",
    "                    val_outputs.append(predicted.cpu().numpy())\n",
    "                    val_acc = multi_label_auc(np.concatenate(val_labels, axis=0), np.concatenate(val_outputs, axis=0))\n",
    "\n",
    "                    if cross:\n",
    "                        result = update_state(progress, cross_acc=val_acc, cross_loss=np.mean(val_loss))\n",
    "                    else:\n",
    "                        update_state(progress, valid_acc=val_acc, valid_loss=np.mean(val_loss))\n",
    "\n",
    "        performance = result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e034ea7ae3f5d1a",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b0f23f044b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(\".\", \"models\")):\n",
    "    os.mkdir(os.path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = os.path.join(\".\", \"models\", f\"{Config.NB_NAME}_acc_{performance*100:.6f}.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b0e6-b773-423a-93e4-ce463f4d4d84",
   "metadata": {},
   "source": [
    "## Inference\n",
    "테스트 데이터셋에 대한 추론은 다음 순서로 진행됩니다.\n",
    "\n",
    "1. 모델 및 디바이스 설정\n",
    "    - 모델을 주어진 device(GPU 또는 CPU)로 이동시키고, 평가모드로 전환합니다.\n",
    "2. 예측 수행\n",
    "    - 예측 결과를 저장한 빈 리스트를 초기화하고 test_loader에서 배치별로 데이터를 불러와 예측을 수행합니다.\n",
    "    - 각 배치에 대해 스펙트로그램 데이터를 device로 이동시킵니다.\n",
    "    - 모델 예측 확률(probs)을 계산합니다.\n",
    "    - 예측 확률을 predictions리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features in tqdm(test_loader):\n",
    "        probs, *_ = model(features.to(device))\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        predicted_labels += probs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fae66d-8f54-46d5-9201-0f4b0db76e76",
   "metadata": {},
   "source": [
    "### Submission\n",
    "추론 결과를 제출 양식에 덮어 씌워 CSV 파일로 생성하는 과정은 다음과 같습니다.\n",
    "\n",
    "1. 제출 양식 로드\n",
    "    - pd.read_csv('./sample_submission.csv')를 사용하여 제출을 위한 샘플 형식 파일을 로드합니다.\n",
    "    - 이 파일은 일반적으로 각 테스트 샘플에 대한 ID와 예측해야 하는 필드가 포함된 템플릿 형태를 가지고 있습니다.\n",
    "2. 예측 결과 할당\n",
    "    - submit.iloc[:,1:] = preds 추론함수(inference)에서 반환된 예측결과(preds)를 샘플 제출 파일에 2번째 열부터 할당합니다.\n",
    "3. 제출 파일 저장\n",
    "    - 수정된 제출 파일을 baseline_submit 이란 이름의 CSV 파일로 저장합니다.\n",
    "    - index=False는 파일 저장시 추가적인 index가 발생하지 않도록 설정하여, 제작한 제출 파일과 동일한 형태의 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8314c4-1dce-4f79-9f3d-77d320a3746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(test_set.submission_form_path)\n",
    "submit.iloc[:, 1:] = predicted_labels\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d71bc-6703-40f7-9716-a0ef897eca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dir = \"submissions\"\n",
    "if not os.path.isdir(submission_dir):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = os.path.join(\".\", submission_dir, f\"{Config.NB_NAME}_acc_{performance*100:.6f}_submit.csv\")\n",
    "submit.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50182ad69c2bf4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732842,
     "sourceId": 8066583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1830.928153,
   "end_time": "2024-04-08T19:22:15.265404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T18:51:44.337251",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a8f214ec354c44b73d439565382278": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06a1ede084cd487ebf3c469be657b53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80013ce73542415e82be091acccb89fe",
        "IPY_MODEL_d280070ca871485fbd2b7d34b1c9fd10",
        "IPY_MODEL_8212bde7695f494cbabea66983e4cf29"
       ],
       "layout": "IPY_MODEL_c4da594b806c4c2bbff6e8cdaf6088eb"
      }
     },
     "37e28ba3d8564da4a3257c3729310584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80013ce73542415e82be091acccb89fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_95e72a34a4374fd5b4b147772085bb7c",
       "placeholder": "​",
       "style": "IPY_MODEL_37e28ba3d8564da4a3257c3729310584",
       "value": "model.safetensors: 100%"
      }
     },
     "8212bde7695f494cbabea66983e4cf29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9e4e04bb60e40d6b46d782f8156d05f",
       "placeholder": "​",
       "style": "IPY_MODEL_b1fa83d0511a4d8a910b8fdb40d32c29",
       "value": " 36.5M/36.5M [00:01&lt;00:00, 41.1MB/s]"
      }
     },
     "95e72a34a4374fd5b4b147772085bb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1fa83d0511a4d8a910b8fdb40d32c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4da594b806c4c2bbff6e8cdaf6088eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d280070ca871485fbd2b7d34b1c9fd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01a8f214ec354c44b73d439565382278",
       "max": 36494688,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcd2393d73d14514851a7d9ef50315fc",
       "value": 36494688
      }
     },
     "d9e4e04bb60e40d6b46d782f8156d05f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcd2393d73d14514851a7d9ef50315fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
