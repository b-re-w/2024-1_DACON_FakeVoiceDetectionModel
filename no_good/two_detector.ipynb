{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840dc88f7c14e1fa",
   "metadata": {},
   "source": [
    "# SW중심대학 디지털 경진대회_SW와 생성AI의 만남 : AI부문\n",
    " - 이 AI 경진대회에서는 5초 분량의 오디오 샘플에서 진짜 사람 목소리와 AI가 생성한 가짜 목소리를 정확하게 구분할 수 있는 모델을 개발하는 것이 목표입니다.\n",
    " - 이 작업은 보안, 사기 감지 및 오디오 처리 기술 향상 등 다양한 분야에서 매우 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7df6856755ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    try:\n",
    "        %conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "    except:\n",
    "        %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "except:\n",
    "    try:\n",
    "        %conda install -c conda-forge librosa\n",
    "    except:\n",
    "        %pip install librosa\n",
    "\n",
    "try:\n",
    "    import wespeaker\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/wenet-e2e/wespeaker.git\n",
    "\n",
    "try:\n",
    "    import huggingface_hub\n",
    "except ImportError:\n",
    "    %pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d73a1",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-04-08T18:51:47.130888",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.123877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "모델 학습 및 추론에 사용할 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbadbd56",
   "metadata": {
    "papermill": {
     "duration": 12.650384,
     "end_time": "2024-04-08T18:51:59.788340",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.137956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.pipelines as pipelines\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41377d36410af6b",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc82cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 16:46:40 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64eb379-e527-46c4-8b12-ead8db628070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1  # cpu\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2de5d",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2024-04-08T18:51:59.803571",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.796330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n",
    "- 딥러닝 모델을 학습하기 전에 설정해야하는 다양한 매개변수를 정의하는 설정 클래스입니다.\n",
    "- 클래스를 사용하여 학습에 필요한 설정 값을 미리 지정합니다.\n",
    "\n",
    "##### 오디오 신호\n",
    "- 우리가 듣는 소리는 공기의 압력 변화로, 이것을 디지털 신호로 변환한 것이 오디오 신호입니다.\n",
    "- 이 신호는 시간에 따라 변하는 진폭 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a32fb60",
   "metadata": {
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-04-08T18:51:59.828208",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.811225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\" Configuration Class \"\"\"\n",
    "    SEED = 20240719  # 재현성을 위해 랜덤 시드 고정\n",
    "    NB_NAME = \"transfer_learning\"  # ipython 노트북 이름 지정\n",
    "    ROOT_FOLDER = os.path.join(\".\", \"data\")\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6700bf8e-7f43-4eac-9bea-25eb1d95fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(Config.SEED)  # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a682d49",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2024-04-08T18:52:31.507909",
     "exception": false,
     "start_time": "2024-04-08T18:52:31.500578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2459913-1bf6-40b9-b07d-402699590b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    download_url = \"https://drive.usercontent.google.com/download?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&export=download&authuser=0&confirm=t&uuid=c40c278b-d74b-4b75-bc79-09e8a3ccffa4&at=APZUnTUvIVFVM9gjGNUCmDb4YZCy%3A1719807236671\"\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root='./data', filename=\"download.zip\", md5=None):\n",
    "        cls.download_root = root\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            utils.download_and_extract_archive(cls.download_url, root, root, filename, md5)\n",
    "            print(\"Extraction completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists in {filepath}\")\n",
    "\n",
    "    @property\n",
    "    def get_dataset_path(self):\n",
    "        filename = \"train.csv\" if self.is_train else \"test.csv\"\n",
    "        if self.custom_csv:\n",
    "            filename = self.custom_csv + \".csv\"\n",
    "        return os.path.join(self.download_root, filename)\n",
    "\n",
    "    @property\n",
    "    def submission_form_path(cls):\n",
    "        return os.path.join(cls.download_root, \"sample_submission.csv\")\n",
    "\n",
    "    def __init__(self, root=\"./data\", train=True, split_ratio=1, transform=None, custom_csv=None):\n",
    "        \"\"\"\n",
    "        Voice Dataset for Contrastive Learning\n",
    "        \n",
    "        :param root: The path to the data directory\n",
    "        :param train: is train or test\n",
    "        :param split_ratio: split ratio for train(can be 0.5 or above) and valid(can be lower than 0.5) set\n",
    "        :param transform: data transformer\n",
    "        :param target_transform: label transformer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.download(root)\n",
    "        self.download_root = root\n",
    "        self.is_train = train\n",
    "        self.custom_csv = custom_csv\n",
    "        self.name = (\"train\" if train else \"test\") if not custom_csv else custom_csv\n",
    "\n",
    "        raw_data = self._load_data(self.get_dataset_path, split_ratio if split_ratio >= 0.5 else 1-split_ratio)\n",
    "        if not self.is_train or split_ratio >= 0.5:\n",
    "            self.raw_data, _ = raw_data\n",
    "        else:\n",
    "            _, self.raw_data = raw_data\n",
    "            if \"train\" not in self.name:\n",
    "                print(f\"Warning: The name of dataset should start with 'train' for training set. (current - {self.name})\")\n",
    "            self.name = self.name.replace(\"train\", \"valid\")\n",
    "\n",
    "        self.data0 = self.raw_data['path'].tolist()\n",
    "        self.data1 = self.raw_data['path'].tolist()\n",
    "\n",
    "        if 'label' in self.raw_data.columns:\n",
    "            self.fake_label = [0 if lb == 'real' else 1 for lb in self.raw_data['label'].tolist()]\n",
    "            self.real_label = [1 if lb == 'real' else 0 for lb in self.raw_data['label'].tolist()]\n",
    "        else:\n",
    "            if 'real' in self.raw_data.columns and 'fake' in self.raw_data.columns:\n",
    "                self.fake_label = self.raw_data['fake'].tolist()\n",
    "                self.real_label = self.raw_data['real'].tolist()\n",
    "            else:\n",
    "                self.fake_label = None\n",
    "                self.real_label = None\n",
    "\n",
    "        self.transforms(transform)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_data(dataset_path, split_ratio=1):\n",
    "        random_state = 1  # fixed random_state\n",
    "\n",
    "        df = pd.read_csv(dataset_path)\n",
    "\n",
    "        if split_ratio == 1 or split_ratio == 0:\n",
    "            return (df, None) if split_ratio == 1 else (None, df)\n",
    "\n",
    "        if 'label' in df.columns:\n",
    "            df1, df2, _, _ = split(df, df['label'], test_size=1-split_ratio, random_state=random_state)\n",
    "        else:\n",
    "            df1, df2 = split(df, test_size=1-split_ratio, random_state=random_state)\n",
    "        return df1, df2\n",
    "\n",
    "    def transforms(self, transform=None):\n",
    "        if transform is not None:\n",
    "            if not isinstance(transform, list) and not isinstance(transform, tuple):\n",
    "                transform = [transform]\n",
    "            for t in transform:\n",
    "                self.data0, self.data1, self.fake_label, self.real_label = t(self.data0, self.data1, self.fake_label, self.real_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.fake_label is not None:\n",
    "            return self.data0[index], self.data1[index], self.fake_label[index], self.real_label[index]\n",
    "        return self.data0[index], self.data1[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8967c36f6abc224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "Loaded Dataset - train(44350), valid(11088), unlabeled(1264) test(50000)\n",
      "Query Dataset for checking: ('./train/NQJUDUMG.ogg', './train/NQJUDUMG.ogg', 1, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19535</th>\n",
       "      <td>NQJUDUMG</td>\n",
       "      <td>./train/NQJUDUMG.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37414</th>\n",
       "      <td>SGACBBDI</td>\n",
       "      <td>./train/SGACBBDI.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40645</th>\n",
       "      <td>SIBSFMAP</td>\n",
       "      <td>./train/SIBSFMAP.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>LLBQPFAD</td>\n",
       "      <td>./train/LLBQPFAD.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>ZWYRTAOF</td>\n",
       "      <td>./train/ZWYRTAOF.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50057</th>\n",
       "      <td>BDFFJCBX</td>\n",
       "      <td>./train/BDFFJCBX.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>NEFSVUCS</td>\n",
       "      <td>./train/NEFSVUCS.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>MJFGSHIR</td>\n",
       "      <td>./train/MJFGSHIR.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>USIDOXOR</td>\n",
       "      <td>./train/USIDOXOR.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>UNQUGLKV</td>\n",
       "      <td>./train/UNQUGLKV.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                  path label\n",
       "19535  NQJUDUMG  ./train/NQJUDUMG.ogg  fake\n",
       "37414  SGACBBDI  ./train/SGACBBDI.ogg  fake\n",
       "40645  SIBSFMAP  ./train/SIBSFMAP.ogg  fake\n",
       "16487  LLBQPFAD  ./train/LLBQPFAD.ogg  real\n",
       "954    ZWYRTAOF  ./train/ZWYRTAOF.ogg  real\n",
       "...         ...                   ...   ...\n",
       "50057  BDFFJCBX  ./train/BDFFJCBX.ogg  fake\n",
       "32511  NEFSVUCS  ./train/NEFSVUCS.ogg  real\n",
       "5192   MJFGSHIR  ./train/MJFGSHIR.ogg  fake\n",
       "12172  USIDOXOR  ./train/USIDOXOR.ogg  real\n",
       "33003  UNQUGLKV  ./train/UNQUGLKV.ogg  fake\n",
       "\n",
       "[44350 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "train_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=split_ratio)\n",
    "train_augmented = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=split_ratio, custom_csv=\"train_augmented\")\n",
    "valid_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=1-split_ratio)\n",
    "valid_augmented = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=1-split_ratio, custom_csv=\"train_augmented\")\n",
    "unlabeled_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=False, custom_csv=\"unlabeled_data\")\n",
    "test_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=False)\n",
    "\n",
    "print(f\"Loaded Dataset - train({len(train_dataset)}), valid({len(valid_dataset)}), unlabeled({len(unlabeled_dataset)}) test({len(test_dataset)})\")\n",
    "print(\"Query Dataset for checking:\", train_dataset[0])\n",
    "train_dataset.raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7888fecea819346",
   "metadata": {},
   "source": [
    "#### Data Transformation\n",
    "By using \n",
    "[TorchAudio Models](https://pytorch.org/audio/stable/models.html) |\n",
    "[TorchAudio Pretrained Models](https://pytorch.org/audio/stable/pipelines.html#module-torchaudio.pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b480f1089e2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPipelines:\n",
    "    \"\"\" Audio Pipelines - Pretrained Embeddings \"\"\"\n",
    "    \n",
    "    wav2vec_bundle = pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "    resnet_bundle = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    device_setting = (device, DEVICE_NUM)\n",
    "    \n",
    "    def __init__(self, audio_cache_dir=\"audio_cache\", nb_name=Config.NB_NAME):\n",
    "        self.audio_cache_dir = audio_cache_dir\n",
    "        if not os.path.isdir(audio_cache_dir):\n",
    "            os.mkdir(audio_cache_dir)\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name))\n",
    "        self.wav2vec = self.get_wav2vec(audio_cache_dir=audio_cache_dir)\n",
    "        self.resnet = self.get_resnet(audio_cache_dir=audio_cache_dir)\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name, self.wav2vec.name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name, self.wav2vec.name))\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name, self.resnet.name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name, self.resnet.name))\n",
    "\n",
    "    @classmethod\n",
    "    def get_wav2vec(cls, audio_cache_dir=\".\"):\n",
    "        sr = cls.wav2vec_bundle.sample_rate  # Wav2Vec2 Model uses sample rate 16kHz\n",
    "        wav2vec_model = cls.wav2vec_bundle.get_model()\n",
    "        wav2vec_model.to(cls.device_setting[0])\n",
    "        print(f\"INFO: Wav2Vec Model Loaded on {cls.device_setting[0]}:{cls.device_setting[1]}\")\n",
    "        wav2vec_model.eval()\n",
    "        \n",
    "        def wav2vec(path):\n",
    "            waveform, sample_rate = torchaudio.load(path, normalize=True)\n",
    "            if sample_rate != sr:\n",
    "                resampler = T.Resample(sample_rate, sr)\n",
    "                waveform = resampler(waveform)\n",
    "            with torch.no_grad():\n",
    "                embedding, _ = wav2vec(waveform.to(cls.device_setting[0]))\n",
    "            return embedding\n",
    "        \n",
    "        wav2vec.__dict__['name'] = str(cls.wav2vec_bundle._path).split(\".\")[0]\n",
    "        wav2vec.__dict__['cache'] = audio_cache_dir\n",
    "        return wav2vec\n",
    "\n",
    "    @classmethod\n",
    "    def get_resnet(cls, audio_cache_dir=\".\"):\n",
    "        model_id = cls.resnet_bundle\n",
    "        model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "    \n",
    "        root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "        if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "            os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "        if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "            os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "    \n",
    "        resnet_model = wespeaker.load_model_local(root_dir)\n",
    "        resnet_model.set_gpu(-1 if cls.device_setting[0] == torch.device('cpu') else cls.device_setting[1])\n",
    "        print(f\"INFO: ResNet Model Loaded on {resnet_model.device}\")\n",
    "    \n",
    "        def resnet(path):\n",
    "            return resnet_model.extract_embedding(path)\n",
    "\n",
    "        resnet.__dict__['name'] = model_name\n",
    "        resnet.__dict__['cache'] = audio_cache_dir\n",
    "        return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6485434baef869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embedding(dataset_name, pretrained, d_idx):\n",
    "    convert_path = lambda path: os.path.join(Config.ROOT_FOLDER, *path.replace(\"./\", \"\").split(\"/\"))\n",
    "    embedding_path = os.path.join(pretrained.cache, Config.NB_NAME, pretrained.name, f\"{dataset_name}.embedding\")\n",
    "\n",
    "    def convert(*args):\n",
    "        *datas_list, labels = args\n",
    "        if not os.path.isfile(embedding_path):\n",
    "            new_datas = [pretrained(convert_path(path)) for path in tqdm(datas_list[d_idx], desc=f\"Convert {dataset_name} dataset with {pretrained.name}\")]\n",
    "            torch.save(new_datas, embedding_path)\n",
    "            print(\"INFO: Voice Embedding saved.\")\n",
    "        else:\n",
    "            new_datas = torch.load(embedding_path)\n",
    "            print(f\"INFO: Pretrained {pretrained.name} embedding for {dataset_name} dataset is loaded.\")\n",
    "        datas_list[d_idx] = new_datas\n",
    "        return *datas_list, labels\n",
    "    return convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d04b035add7ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIM_SIZE = 1024\n",
    "\n",
    "def flatten_tensor(adaptive_pool=nn.AdaptiveAvgPool1d(WV_DIM_SIZE), d_idx=1):\n",
    "    def flatten(*args):\n",
    "        *datas_list, labels = args\n",
    "        datas_list[d_idx] = [adaptive_pool(torch.flatten(t).unsqueeze(0)).squeeze(0) for t in datas_list[d_idx]]\n",
    "        return *datas_list, labels\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2a071509e465a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Wav2Vec Model Loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ResNet Model Loaded on cuda:0\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for train dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for train dataset is loaded.\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for train_augmented dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for train_augmented dataset is loaded.\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for valid dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for valid dataset is loaded.\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for valid_augmented dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for valid_augmented dataset is loaded.\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for unlabeled_data dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for unlabeled_data dataset is loaded.\n",
      "INFO: Pretrained voxceleb_resnet152_LM embedding for test dataset is loaded.\n",
      "INFO: Pretrained wav2vec2_fairseq_base_ls960_asr_ls960 embedding for test dataset is loaded.\n"
     ]
    }
   ],
   "source": [
    "to_tensor = lambda *args: (*args[:-1], list(map(torch.tensor, args[-1])))  # label to tensor\n",
    "\n",
    "apl = AudioPipelines()  # Create Audio Pipeline for converting audio to embeddings\n",
    "\n",
    "for dataset in [train_dataset, train_augmented, valid_dataset, valid_augmented]:\n",
    "    dataset.transforms(transform=[\n",
    "        to_embedding(dataset.name, apl.resnet, d_idx=0),\n",
    "        to_embedding(dataset.name, apl.wav2vec, d_idx=1),\n",
    "        flatten_tensor(d_idx=1),\n",
    "        to_tensor\n",
    "    ])\n",
    "\n",
    "for dataset in [unlabeled_dataset, test_dataset]:\n",
    "    dataset.transforms(transform=[\n",
    "        to_embedding(dataset.name, apl.resnet, d_idx=0),\n",
    "        to_embedding(dataset.name, apl.wav2vec, d_idx=1),\n",
    "        flatten_tensor(d_idx=1)\n",
    "    ])\n",
    "\n",
    "del apl  # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4739bbf8ec8a6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset 0: (1, tensor(0)) [tensor([-0.2622, -0.1319, -0.0539,  0.0124,  0.1426,  0.1331,  0.0709,  0.0193,\n",
      "         0.0863, -0.2342,  0.0471,  0.2622, -0.0538,  0.0060,  0.0194, -0.0204,\n",
      "        -0.1776,  0.0919,  0.1826,  0.0205, -0.0145, -0.0908,  0.0730,  0.1513,\n",
      "         0.0507,  0.1620,  0.0702, -0.1969, -0.0679, -0.1909, -0.0514, -0.0315,\n",
      "        -0.1292, -0.1566,  0.0784,  0.2276,  0.0624,  0.0883, -0.0481,  0.2119,\n",
      "         0.1538, -0.0134,  0.1566,  0.0766, -0.0213, -0.1333, -0.0209, -0.0596,\n",
      "         0.0964,  0.1612, -0.1159, -0.2910, -0.0109,  0.1471, -0.2822, -0.0772,\n",
      "         0.1346, -0.0208,  0.0997,  0.2245,  0.0230,  0.0578, -0.0675,  0.0491,\n",
      "        -0.1563,  0.0842, -0.1240, -0.0403, -0.0722,  0.0857, -0.1086, -0.0910,\n",
      "        -0.1778,  0.1218, -0.0394,  0.0036,  0.1889,  0.4586, -0.2083,  0.1329,\n",
      "        -0.2099,  0.0219,  0.1260,  0.0328,  0.1316,  0.0938, -0.0038,  0.0322,\n",
      "        -0.1676,  0.0230, -0.0543, -0.0173,  0.0925,  0.0832,  0.0672, -0.0914,\n",
      "         0.0046, -0.0786, -0.0370, -0.0079,  0.3056,  0.1254, -0.1912,  0.2776,\n",
      "        -0.1283,  0.0051, -0.2316, -0.1201,  0.1218,  0.0544,  0.0589, -0.1102,\n",
      "         0.1907,  0.0384,  0.1419,  0.0912,  0.1457,  0.0447,  0.1717,  0.3549,\n",
      "        -0.1454,  0.2011, -0.2856,  0.0424, -0.0270, -0.2465,  0.1676,  0.0780,\n",
      "         0.1618, -0.0773, -0.3644, -0.1135,  0.1079,  0.0879, -0.0030,  0.1102,\n",
      "        -0.1842, -0.0385,  0.3156,  0.0095,  0.2222,  0.0756, -0.1778,  0.0608,\n",
      "        -0.2043, -0.1714,  0.1097,  0.0457, -0.1283,  0.1045, -0.0078,  0.2684,\n",
      "        -0.0694,  0.0225, -0.2242, -0.1619,  0.0282,  0.1706,  0.1921, -0.0620,\n",
      "        -0.0315,  0.2236, -0.0173,  0.3383, -0.1247, -0.0784,  0.2159, -0.1297,\n",
      "         0.0233, -0.1080, -0.0077,  0.1154,  0.2171, -0.0951, -0.3181, -0.0984,\n",
      "         0.0972, -0.2432,  0.1580, -0.1005,  0.0075,  0.2600,  0.0057, -0.0341,\n",
      "         0.0949,  0.1423,  0.1020, -0.1131, -0.0957,  0.1616,  0.0321,  0.1117,\n",
      "        -0.3699,  0.0544,  0.3219, -0.1143,  0.1116,  0.2030,  0.1423, -0.2294,\n",
      "        -0.0777,  0.0341, -0.2342,  0.0787,  0.0886, -0.1722, -0.1240,  0.1838,\n",
      "        -0.1355,  0.3528,  0.0644, -0.0089,  0.0573,  0.0518, -0.0884, -0.1127,\n",
      "         0.0563,  0.1938, -0.1026,  0.1936, -0.2228,  0.2247,  0.1001,  0.0547,\n",
      "         0.2180, -0.1044,  0.1581, -0.0154, -0.1177, -0.0897, -0.1192, -0.0438,\n",
      "        -0.0697, -0.0258, -0.0029, -0.2258,  0.1768, -0.0438, -0.2677, -0.0969,\n",
      "        -0.0603,  0.1538,  0.0219, -0.0514, -0.0887,  0.0892, -0.0532, -0.1720,\n",
      "        -0.1020,  0.0746,  0.0665, -0.0499, -0.0501,  0.0055, -0.2329, -0.0873]), tensor([ 1.8710, -1.5975, -1.0627,  ..., -4.9998, -7.9781, -7.1742],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for (*data, fake, real), i in zip(train_dataset, range(1)):\n",
    "    print(f\"Train Dataset {i}: {(fake, real)}\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef836d6cfa4590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Dataset 0: (1, tensor(0)) [tensor([-0.0336,  0.0182,  0.1605, -0.0633, -0.2657, -0.0786,  0.2894, -0.0773,\n",
      "        -0.0861,  0.1535,  0.2038, -0.1084, -0.0763,  0.0494,  0.2047, -0.0693,\n",
      "         0.2637,  0.0098,  0.0902,  0.0144,  0.0416,  0.0160,  0.1329,  0.1867,\n",
      "        -0.0150,  0.0670,  0.0060, -0.2174,  0.0260, -0.1373,  0.1242,  0.1294,\n",
      "        -0.0187,  0.1399,  0.0461,  0.1157,  0.1255,  0.0183, -0.0848,  0.0068,\n",
      "        -0.0793,  0.0474, -0.0455, -0.1096, -0.0644,  0.0303, -0.0370, -0.1810,\n",
      "        -0.1520,  0.0405, -0.0606, -0.0048,  0.0287, -0.0114, -0.0170, -0.1477,\n",
      "         0.0111, -0.0047,  0.1893, -0.2278, -0.1438,  0.0524,  0.1691, -0.2530,\n",
      "         0.0281, -0.0036, -0.0240, -0.1658, -0.1500,  0.1094,  0.2081, -0.0912,\n",
      "        -0.1184,  0.0304,  0.0684,  0.0259,  0.1747,  0.2028,  0.0706, -0.1569,\n",
      "        -0.0617,  0.0943,  0.1673,  0.1042,  0.0533, -0.1656, -0.0861,  0.0449,\n",
      "        -0.1007,  0.0041, -0.2378, -0.0718,  0.2067, -0.0689,  0.2285,  0.1158,\n",
      "        -0.1382, -0.2085, -0.1158, -0.1507, -0.0776, -0.1635,  0.1782, -0.1219,\n",
      "         0.0882, -0.2651, -0.2081, -0.0798, -0.3159, -0.0010, -0.0349, -0.1495,\n",
      "         0.0342, -0.0743, -0.1706,  0.3176, -0.0536,  0.1175, -0.1061,  0.1667,\n",
      "        -0.1476, -0.0263, -0.1038,  0.0313,  0.0527, -0.0304, -0.0934, -0.0342,\n",
      "         0.1532,  0.0277, -0.0879, -0.1310,  0.0714, -0.1659, -0.2882, -0.0117,\n",
      "         0.0362,  0.1572,  0.0138, -0.0220, -0.0592,  0.0538, -0.0059,  0.1761,\n",
      "         0.2373, -0.0639, -0.0273, -0.1435, -0.0520, -0.0330, -0.1753,  0.0101,\n",
      "        -0.0358, -0.0784, -0.0197,  0.1785, -0.1360,  0.2087,  0.1679, -0.2217,\n",
      "         0.0629,  0.0448,  0.0288, -0.1541,  0.0124,  0.0725, -0.2781, -0.0657,\n",
      "         0.1346, -0.0436,  0.0779, -0.0346, -0.0703, -0.1231, -0.0382, -0.0110,\n",
      "         0.0592,  0.0098,  0.0218, -0.1532, -0.1680,  0.0621,  0.0176, -0.0119,\n",
      "         0.0928,  0.1903, -0.1683,  0.0159, -0.0305, -0.0400, -0.0021,  0.1540,\n",
      "        -0.0917, -0.1229,  0.1374,  0.0454, -0.2075, -0.1173, -0.0453, -0.0989,\n",
      "         0.1817, -0.0661, -0.0523, -0.0293,  0.0654, -0.2538,  0.0578, -0.0416,\n",
      "        -0.1357,  0.1588,  0.0562, -0.1041, -0.0613,  0.0457,  0.1867,  0.3683,\n",
      "        -0.1204, -0.2878, -0.1038,  0.1195, -0.1003, -0.0294,  0.0061,  0.2305,\n",
      "         0.0055,  0.2623, -0.0342,  0.2570,  0.1209,  0.0861, -0.0603, -0.0397,\n",
      "         0.0990,  0.0577,  0.0349, -0.0443,  0.0581,  0.1346,  0.1197,  0.0243,\n",
      "        -0.0368,  0.0176,  0.1302,  0.0376,  0.1320, -0.1654,  0.2800, -0.1564,\n",
      "         0.1993,  0.0070, -0.0371,  0.0805, -0.2404, -0.1061,  0.0025, -0.0023]), tensor([ 3.1887, -1.0826, -0.2877,  ..., -4.6342, -4.8370, -4.7765],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for (*data, fake, real), i in zip(valid_dataset, range(1)):\n",
    "    print(f\"Valid Dataset {i}: {(fake, real)}\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4306aa86c180ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnLabeled Dataset 0: [(tensor([-9.8545e-02,  7.8067e-02,  9.8155e-02, -9.5028e-02, -1.5494e-03,\n",
      "        -1.0997e-01,  4.6318e-03, -3.1125e-02,  3.5486e-02,  1.2958e-01,\n",
      "        -1.1017e-01, -2.9643e-02,  1.3663e-01,  4.7201e-02,  2.7647e-02,\n",
      "        -5.2182e-03, -1.6107e-01, -2.5661e-03,  4.1623e-02,  5.6934e-02,\n",
      "        -3.3284e-02, -1.0894e-01,  7.9592e-02,  9.2778e-02, -4.4489e-02,\n",
      "         3.8495e-02,  1.1325e-01,  1.0612e-01,  5.6692e-02,  1.5595e-02,\n",
      "         5.0284e-02, -1.3181e-02, -2.6522e-03, -8.2216e-02, -1.3975e-01,\n",
      "         1.2072e-01,  4.0033e-03, -8.3844e-03, -2.4211e-01, -1.3957e-01,\n",
      "        -1.1047e-03,  7.7923e-03, -5.3082e-02,  2.5970e-01,  1.4133e-01,\n",
      "        -1.1541e-01,  1.2204e-01, -3.9140e-02, -8.0197e-02,  4.5311e-02,\n",
      "         7.7239e-02,  6.0349e-02, -5.3912e-02,  2.1894e-02, -3.7784e-02,\n",
      "        -9.2405e-02, -1.3264e-02,  5.6325e-04,  1.4982e-01, -5.2745e-02,\n",
      "        -1.5333e-02,  4.1237e-02,  1.1999e-03, -4.5370e-02,  6.2777e-02,\n",
      "         3.4919e-02,  3.9585e-02, -1.0599e-01,  2.8432e-02,  3.4092e-02,\n",
      "         2.3772e-02,  1.3047e-01, -1.0017e-03, -1.9497e-04, -1.8092e-01,\n",
      "        -1.6531e-01, -1.0770e-01,  4.1734e-02, -7.0004e-02,  6.7777e-02,\n",
      "        -1.1393e-03,  7.0360e-02,  7.8760e-02, -2.5306e-02, -1.4774e-01,\n",
      "        -2.6467e-01, -1.9044e-02, -2.9073e-03, -8.3583e-02,  4.9560e-02,\n",
      "        -1.2836e-01, -1.7380e-02,  7.9980e-02, -2.4088e-01,  1.3804e-01,\n",
      "        -4.8239e-02, -1.8968e-01,  2.1178e-02, -4.1473e-02,  6.5101e-02,\n",
      "         1.2489e-01,  4.5510e-02, -5.4941e-02, -5.2704e-02, -1.0107e-01,\n",
      "        -7.5482e-02,  3.6953e-02,  1.4370e-02,  1.9529e-02,  9.6964e-02,\n",
      "        -1.5188e-01,  5.0006e-02, -4.7907e-02,  9.6791e-02,  7.3013e-02,\n",
      "         3.6285e-02,  3.9219e-02,  1.3818e-01, -6.6441e-02,  1.8895e-01,\n",
      "         1.4474e-01,  8.8515e-02, -6.8889e-02,  5.5967e-02,  1.0787e-01,\n",
      "         1.1053e-01,  1.3858e-02,  7.0281e-03, -2.9458e-02, -2.9976e-02,\n",
      "         1.4000e-01,  7.7663e-02, -1.7502e-01,  7.6525e-02,  1.2360e-01,\n",
      "        -5.7971e-02,  7.4771e-02, -2.6668e-01, -1.7791e-02, -1.2916e-01,\n",
      "        -4.8273e-02, -1.2103e-02, -1.3615e-01, -7.0535e-02,  4.0928e-04,\n",
      "         1.5745e-02, -1.3993e-02, -1.6074e-03,  1.0460e-01,  4.1407e-02,\n",
      "        -9.8853e-02,  1.6405e-01,  5.9121e-03,  1.4126e-01,  9.3875e-02,\n",
      "        -1.5832e-01,  1.7844e-01,  3.7519e-02, -3.7899e-03,  2.8510e-02,\n",
      "        -5.7809e-02,  7.0664e-02,  2.3892e-02, -4.6221e-02,  6.5584e-02,\n",
      "         1.1249e-02, -2.1668e-01, -9.9939e-02,  1.0701e-02,  2.3390e-03,\n",
      "        -5.2661e-02, -6.0305e-02, -1.7471e-02, -7.2537e-02,  3.5997e-02,\n",
      "        -3.8696e-02, -1.4927e-01, -1.5427e-03,  4.9235e-02,  8.1350e-02,\n",
      "         1.8639e-02, -3.9593e-02, -1.2224e-01,  2.2275e-02,  1.1276e-01,\n",
      "        -1.2727e-01,  1.8948e-02,  4.9667e-02, -6.0059e-02,  2.9033e-02,\n",
      "         6.8154e-02, -8.3094e-02,  8.2345e-02, -5.4174e-03,  4.7263e-03,\n",
      "        -6.5867e-02,  9.6170e-02,  4.4731e-02,  8.2458e-03,  5.1085e-02,\n",
      "         1.4384e-01,  4.5518e-02,  3.0180e-02,  2.9571e-02,  1.5921e-01,\n",
      "        -8.7167e-02, -6.2062e-02, -5.7742e-02, -9.8300e-02,  1.2139e-01,\n",
      "        -1.4155e-01, -2.8681e-02, -4.8689e-02, -5.8412e-02,  2.3240e-02,\n",
      "         1.6482e-01, -3.1730e-02, -8.8439e-02,  2.4033e-02,  9.1721e-02,\n",
      "         3.9064e-02,  1.2714e-01,  1.3133e-02,  1.4214e-01, -3.2575e-02,\n",
      "        -5.6694e-02,  1.5578e-01, -2.7586e-02,  3.9347e-02,  5.1438e-03,\n",
      "         3.3578e-02, -1.6853e-01,  5.7177e-03, -6.9837e-02,  3.4396e-02,\n",
      "        -3.8898e-02, -7.0544e-02, -7.1695e-02,  5.5841e-02, -1.8456e-01,\n",
      "         4.1701e-02,  6.7293e-02,  1.4079e-02, -1.0488e-01,  4.4868e-02,\n",
      "         5.6280e-02, -1.0147e-02,  6.7691e-02,  5.9360e-03, -6.3555e-02,\n",
      "         1.0149e-01,  1.9752e-02, -5.9840e-02,  3.8797e-02, -2.8547e-02,\n",
      "        -1.5890e-01]), tensor([ 1.2373, -1.0803, -4.8613,  ..., -1.8677, -3.7382, -5.9804],\n",
      "       device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "for *data, i in zip(unlabeled_dataset, range(1)):\n",
    "    print(f\"UnLabeled Dataset {i}:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e03074038fd8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset 0: [(tensor([-0.0440,  0.0194, -0.0009, -0.0082, -0.0174, -0.1296, -0.0795, -0.0763,\n",
      "         0.1436,  0.0484, -0.0266, -0.0678,  0.0314,  0.0616, -0.0423, -0.0676,\n",
      "        -0.1374,  0.0819,  0.0537,  0.0678, -0.0810, -0.1357,  0.0359,  0.0167,\n",
      "         0.0724,  0.0702,  0.1178,  0.1597,  0.0781,  0.0037,  0.1121,  0.0362,\n",
      "        -0.0505, -0.0568,  0.0253,  0.0175, -0.0293,  0.0337, -0.1793, -0.0305,\n",
      "        -0.0153, -0.0454, -0.0522,  0.2037,  0.1051, -0.0733, -0.0042,  0.0441,\n",
      "        -0.0011,  0.1169,  0.0618,  0.0202, -0.0422,  0.0272, -0.0136, -0.1012,\n",
      "        -0.0886,  0.0227,  0.0316, -0.0563, -0.0129,  0.0821, -0.0032, -0.0357,\n",
      "         0.0695,  0.0099,  0.0551, -0.0480, -0.0132, -0.0537,  0.0250,  0.2138,\n",
      "        -0.0260,  0.0234, -0.1294, -0.1327, -0.1087,  0.0559, -0.1272, -0.0230,\n",
      "        -0.0462,  0.0715,  0.0991,  0.1146, -0.1166, -0.2101, -0.0384, -0.0236,\n",
      "         0.0557,  0.0326, -0.1009, -0.0410,  0.0916, -0.1731,  0.1195, -0.1009,\n",
      "        -0.1256,  0.0016,  0.0170, -0.0374,  0.0932,  0.0203, -0.1070, -0.0417,\n",
      "        -0.1352,  0.0137,  0.0309,  0.0729, -0.0187, -0.0463, -0.1815,  0.0707,\n",
      "        -0.0094,  0.1393,  0.1179,  0.0571, -0.0164,  0.0886, -0.0306,  0.0764,\n",
      "         0.1090,  0.0385, -0.0403,  0.1124, -0.0065, -0.0057, -0.0984, -0.0552,\n",
      "         0.0460,  0.0326,  0.1203,  0.1118, -0.1615,  0.1227,  0.0789,  0.0103,\n",
      "         0.0781, -0.2997, -0.0106, -0.0955, -0.0403,  0.0094, -0.0938, -0.0375,\n",
      "        -0.0870,  0.0190, -0.0588, -0.0499,  0.0513,  0.0806, -0.0685,  0.1064,\n",
      "         0.0333,  0.0868,  0.0588, -0.1050,  0.1661,  0.0348,  0.0090, -0.0502,\n",
      "        -0.0729,  0.0582, -0.0538, -0.0429,  0.0281, -0.0453, -0.0845, -0.1102,\n",
      "        -0.0183,  0.0406, -0.0353,  0.0054,  0.0391,  0.0177,  0.0303, -0.0807,\n",
      "        -0.1484,  0.0837,  0.0087,  0.0646, -0.0724,  0.0126, -0.0150,  0.0668,\n",
      "         0.1495,  0.0301, -0.0290,  0.0342, -0.1137,  0.0493,  0.0400, -0.1022,\n",
      "         0.1477, -0.0691, -0.0675, -0.1357,  0.1518,  0.0749, -0.0092,  0.0640,\n",
      "         0.2324,  0.0823,  0.0524,  0.0031, -0.0815, -0.0011, -0.0265,  0.0016,\n",
      "        -0.0165,  0.0570, -0.0947,  0.0067,  0.0574,  0.0163, -0.0874, -0.0534,\n",
      "        -0.0590, -0.0497,  0.0532, -0.0312,  0.0272,  0.0820, -0.0138,  0.0542,\n",
      "        -0.1042, -0.1438,  0.2203,  0.0725,  0.0317, -0.0060, -0.0075, -0.1015,\n",
      "        -0.0214, -0.0400, -0.0007,  0.0156, -0.0531, -0.1187, -0.0675, -0.0597,\n",
      "         0.1184,  0.0954, -0.0686, -0.0573, -0.0501,  0.0691, -0.0148,  0.0856,\n",
      "        -0.0094, -0.0756,  0.0263, -0.0344, -0.0677,  0.0581, -0.0261, -0.1378]), tensor([ 0.3268, -1.9210, -2.0683,  ..., -1.6165, -3.3817, -4.4731],\n",
      "       device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "for *data, i in zip(test_dataset, range(1)):\n",
    "    print(f\"Test Dataset {i}:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c359eed10e6ab",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "    - DataLoader는 구축된 데이터셋에서 배치크기(batch_size)에 맞게 데이터를 추출하고, 필요에 따라 섞거나(shuffle=True) 순서대로 반환(shuffle=False)하는 역할을 합니다.\n",
    "    - 훈련 데이터(train_loader)는 일반적으로 섞어서 모델이 데이터에 덜 편향되게 학습하도록하며,\n",
    "      검증 데이터(val_loader)는 모델 성능 평가를 위해 순서대로 사용하고,\n",
    "      테스트 데이터(test_loader)는 최종적인 추론을 위해 사용합니다.\n",
    "\n",
    "    이렇게 DataLoader를 사용함으로써, 효율적인 데이터 처리와 모델 학습 및 평가가 가능해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f32489efd727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = Config.BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_aug_loader = DataLoader(train_augmented, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_aug_loader = DataLoader(valid_augmented, batch_size=BATCH_SIZE, shuffle=False)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf2531542bfda5",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Fake Detector: not_fake(0) ~ fake(1)\n",
    "\n",
    "Real Detector: not_real(0) ~ real(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c4bc7cda01359",
   "metadata": {},
   "source": [
    "### 1. Feature Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fcbeaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.attention_weights = nn.Parameter(torch.randn(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = F.softmax(self.attention_weights, dim=0)\n",
    "        weighted_features = x * attention_scores\n",
    "        return weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7e309cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm1d(out_features)\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        self.conv2 = nn.Linear(out_features, out_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd28b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDetector(nn.Module):  # v1\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder with skip connections\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        self.skip1 = nn.Linear(embedding_size, hidden_size)\n",
    "\n",
    "        self.encoder_block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//2)\n",
    "        )\n",
    "        self.skip2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "\n",
    "        self.encoder_block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size//2, hidden_size//4),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//4)\n",
    "        )\n",
    "        self.skip3 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "\n",
    "        self.encoder_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_size//4, hidden_size//8),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(hidden_size//8)\n",
    "        )\n",
    "        self.skip4 = nn.Linear(hidden_size//4, hidden_size//8)\n",
    "\n",
    "        self.final_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size//8, latent_size),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionLayer(latent_size)\n",
    "        self.fc = nn.Linear(latent_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder_block1(x) + self.skip1(x)\n",
    "        x2 = self.encoder_block2(x1) + self.skip2(x1)\n",
    "        x3 = self.encoder_block3(x2) + self.skip3(x2)\n",
    "        x4 = self.encoder_block4(x3) + self.skip4(x3)\n",
    "        encoded = self.final_encoder(x4)\n",
    "        attention = self.attention(encoded)\n",
    "        out = self.fc(attention)\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7742f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDetector(nn.Module):  # v2\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(hidden_size, hidden_size, 3)\n",
    "        self.layer2 = self._make_layer(hidden_size, hidden_size//2, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(hidden_size//2, hidden_size//4, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(hidden_size//4, hidden_size//8, 3, stride=2)\n",
    "\n",
    "        self.final_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size//8, latent_size),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionLayer(latent_size)\n",
    "        self.fc = nn.Linear(latent_size, 1)\n",
    "\n",
    "    def _make_layer(self, in_features, out_features, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_features != out_features:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.BatchNorm1d(out_features)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_features, out_features, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_features, out_features))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        encoded = self.final_encoder(x)\n",
    "        attention = self.attention(encoded)\n",
    "        out = self.fc(attention)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4595cd4067030",
   "metadata": {},
   "source": [
    "### 2. Total Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "825ab1b9d9949940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeVoiceDetectionModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.fake_detector = FeatureDetector(embedding_size, hidden_size, latent_size)\n",
    "        self.real_detector = FeatureDetector(embedding_size, hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        fakes = self.fake_detector(x[0])\n",
    "        reals = self.real_detector(x[1])\n",
    "        return fakes, reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e38e5e60ef1fe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_size': 256, 'hidden_size': 1024, 'latent_size': 128}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model parameters\n",
    "model_params = dict(\n",
    "    embedding_size=256,\n",
    "    hidden_size=1024,\n",
    "    latent_size=128\n",
    ")\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3d59ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeVoiceDetectionModel(\n",
       "  (fake_detector): FeatureDetector(\n",
       "    (initial_layer): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (final_encoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (attention): AttentionLayer()\n",
       "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (real_detector): FeatureDetector(\n",
       "    (initial_layer): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (conv2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (final_encoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (attention): AttentionLayer()\n",
       "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = FakeVoiceDetectionModel(**model_params)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6561d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryCrossEntropy\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "fake_optimizer = torch.optim.Adam(params=model.fake_detector.parameters(), lr=Config.LR)\n",
    "real_optimizer = torch.optim.Adam(params=model.real_detector.parameters(), lr=Config.LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d452900af87e03b",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62d2cc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ac54651a18412d98b9928af07ca7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epochs:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec779fe7eae47d0835a2996e14dad09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5596e8b359947cebe0fedbd4afaedf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/150], Step [258/697], Loss: 0.308590 | 0.104877, Valid Acc: 73.406422% (80.167146% | 66.645699%), Valid Loss: 0.528877 (0.843418 | 0.214336)"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "log_interval = 5\n",
    "\n",
    "last_val_acc, last_val_loss = 0, 0\n",
    "train_len, *valid_len = map(len, (train_aug_loader, valid_loader, valid_aug_loader))\n",
    "\n",
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Running Epochs\")\n",
    "with tqdm(total=train_len, desc=\"Training\") as train_progress, tqdm(total=sum(valid_len), desc=\"Validation\") as valid_progress:\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_len)\n",
    "        valid_progress.reset(total=sum(valid_len))\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for i, inputs in enumerate(train_aug_loader):\n",
    "            fake_optimizer.zero_grad(), real_optimizer.zero_grad()\n",
    "\n",
    "            *features, fake_labels, real_labels = (data.float().to(device) for data in inputs)\n",
    "            fakes, reals = model(features[0], features[0])\n",
    "\n",
    "            fake_loss, real_loss = criterion(fakes, fake_labels.view(-1, 1)), criterion(reals, real_labels.view(-1, 1))\n",
    "            fake_loss.backward(), real_loss.backward()\n",
    "            fake_optimizer.step(), real_optimizer.step()\n",
    "\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {fake_loss.item():.6f} | {real_loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        acc, lss, mean = [[], []], [[], []], np.mean\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, loader in enumerate([valid_loader, valid_aug_loader]):\n",
    "                for inputs in loader:\n",
    "                    *features, fake_labels, real_labels = (data.float().to(device) for data in inputs)\n",
    "                    fakes, reals = model(features[0], features[0])\n",
    "                    \n",
    "                    [lst.extend((pred >= 0.5).view(-1).tolist()) for lst, pred in zip(acc, (fakes, reals))]\n",
    "                    [lst.append(criterion(o, l.view(-1, 1)).item()) for lst, o, l in zip(lss, (fakes, reals), (fake_labels, real_labels))]\n",
    "\n",
    "                    valid_progress.update(1)\n",
    "        \n",
    "        last_val_acc, last_val_loss = np.mean(list(map(mean, acc))), np.mean(list(map(mean, lss)))\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{train_len}/{train_len}], Loss: {fake_loss.item():.6f} | {real_loss.item():.6f}, \"\n",
    "            + f\"Valid Acc: {last_val_acc:.6%} ({mean(acc[0]):.6%} | {mean(acc[1]):.6%}), \"\n",
    "            + f\"Valid Loss: {last_val_loss:.6f} ({mean(lss[0]):.6f} | {mean(lss[1]):.6f})\", end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e034ea7ae3f5d1a",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f49b0f23f044b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models/transfer_learning_acc_73.452852.pt\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(os.path.join(\".\", \"models\")):\n",
    "    os.mkdir(os.path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = os.path.join(\".\", \"models\", f\"{Config.NB_NAME}_acc_{last_val_acc*100:.6f}.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b0e6-b773-423a-93e4-ce463f4d4d84",
   "metadata": {},
   "source": [
    "## Inference\n",
    "테스트 데이터셋에 대한 추론은 다음 순서로 진행됩니다.\n",
    "\n",
    "1. 모델 및 디바이스 설정\n",
    "    - 모델을 주어진 device(GPU 또는 CPU)로 이동시키고, 평가모드로 전환합니다.\n",
    "2. 예측 수행\n",
    "    - 예측 결과를 저장한 빈 리스트를 초기화하고 test_loader에서 배치별로 데이터를 불러와 예측을 수행합니다.\n",
    "    - 각 배치에 대해 스펙트로그램 데이터를 device로 이동시킵니다.\n",
    "    - 모델 예측 확률(probs)을 계산합니다.\n",
    "    - 예측 확률을 predictions리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "321d8ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec4986b443045e6962d04f17e76f03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m----> 7\u001b[0m         fakes, reals \u001b[38;5;241m=\u001b[39m model(features\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      8\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((fakes, reals), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      9\u001b[0m         predicted_labels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features in tqdm(test_loader):\n",
    "        fakes, reals = model(features.to(device))\n",
    "        probs = torch.cat((fakes, reals), dim=1).cpu().detach().numpy()\n",
    "        predicted_labels += probs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fae66d-8f54-46d5-9201-0f4b0db76e76",
   "metadata": {},
   "source": [
    "### Submission\n",
    "추론 결과를 제출 양식에 덮어 씌워 CSV 파일로 생성하는 과정은 다음과 같습니다.\n",
    "\n",
    "1. 제출 양식 로드\n",
    "    - pd.read_csv('./sample_submission.csv')를 사용하여 제출을 위한 샘플 형식 파일을 로드합니다.\n",
    "    - 이 파일은 일반적으로 각 테스트 샘플에 대한 ID와 예측해야 하는 필드가 포함된 템플릿 형태를 가지고 있습니다.\n",
    "2. 예측 결과 할당\n",
    "    - submit.iloc[:,1:] = preds 추론함수(inference)에서 반환된 예측결과(preds)를 샘플 제출 파일에 2번째 열부터 할당합니다.\n",
    "3. 제출 파일 저장\n",
    "    - 수정된 제출 파일을 baseline_submit 이란 이름의 CSV 파일로 저장합니다.\n",
    "    - index=False는 파일 저장시 추가적인 index가 발생하지 않도록 설정하여, 제작한 제출 파일과 동일한 형태의 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8314c4-1dce-4f79-9f3d-77d320a3746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(test_dataset.submission_form_path)\n",
    "submit.iloc[:, 1:] = predicted_labels\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d71bc-6703-40f7-9716-a0ef897eca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dir = \"submissions\"\n",
    "if not os.path.isdir(submission_dir):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = os.path.join(\".\", submission_dir, f\"{Config.NB_NAME}_acc_{last_val_acc*100:.6f}_submit.csv\")\n",
    "submit.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5b004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732842,
     "sourceId": 8066583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1830.928153,
   "end_time": "2024-04-08T19:22:15.265404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T18:51:44.337251",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a8f214ec354c44b73d439565382278": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06a1ede084cd487ebf3c469be657b53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80013ce73542415e82be091acccb89fe",
        "IPY_MODEL_d280070ca871485fbd2b7d34b1c9fd10",
        "IPY_MODEL_8212bde7695f494cbabea66983e4cf29"
       ],
       "layout": "IPY_MODEL_c4da594b806c4c2bbff6e8cdaf6088eb"
      }
     },
     "37e28ba3d8564da4a3257c3729310584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80013ce73542415e82be091acccb89fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_95e72a34a4374fd5b4b147772085bb7c",
       "placeholder": "​",
       "style": "IPY_MODEL_37e28ba3d8564da4a3257c3729310584",
       "value": "model.safetensors: 100%"
      }
     },
     "8212bde7695f494cbabea66983e4cf29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9e4e04bb60e40d6b46d782f8156d05f",
       "placeholder": "​",
       "style": "IPY_MODEL_b1fa83d0511a4d8a910b8fdb40d32c29",
       "value": " 36.5M/36.5M [00:01&lt;00:00, 41.1MB/s]"
      }
     },
     "95e72a34a4374fd5b4b147772085bb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1fa83d0511a4d8a910b8fdb40d32c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4da594b806c4c2bbff6e8cdaf6088eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d280070ca871485fbd2b7d34b1c9fd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01a8f214ec354c44b73d439565382278",
       "max": 36494688,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcd2393d73d14514851a7d9ef50315fc",
       "value": 36494688
      }
     },
     "d9e4e04bb60e40d6b46d782f8156d05f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcd2393d73d14514851a7d9ef50315fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
