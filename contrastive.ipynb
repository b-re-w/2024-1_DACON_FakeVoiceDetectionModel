{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SW중심대학 디지털 경진대회_SW와 생성AI의 만남 : AI부문\n",
    " - 이 AI 경진대회에서는 5초 분량의 오디오 샘플에서 진짜 사람 목소리와 AI가 생성한 가짜 목소리를 정확하게 구분할 수 있는 모델을 개발하는 것이 목표입니다.\n",
    " - 이 작업은 보안, 사기 감지 및 오디오 처리 기술 향상 등 다양한 분야에서 매우 중요합니다."
   ],
   "id": "840dc88f7c14e1fa"
  },
  {
   "cell_type": "markdown",
   "id": "361d73a1",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-04-08T18:51:47.130888",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.123877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "모델 학습 및 추론에 사용할 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "bbadbd56",
   "metadata": {
    "papermill": {
     "duration": 12.650384,
     "end_time": "2024-04-08T18:51:59.788340",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.137956",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:21.377138Z",
     "start_time": "2024-07-04T12:32:18.288314Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2d80cf24-13e8-480c-94eb-2982bb52510d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:21.386684Z",
     "start_time": "2024-07-04T12:32:21.380143Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f64eb379-e527-46c4-8b12-ead8db628070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:21.429274Z",
     "start_time": "2024-07-04T12:32:21.388259Z"
    }
   },
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using device -\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device - cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "a0d2de5d",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2024-04-08T18:51:59.803571",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.796330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n",
    "- 딥러닝 모델을 학습하기 전에 설정해야하는 다양한 매개변수를 정의하는 설정 클래스입니다.\n",
    "- 클래스를 사용하여 학습에 필요한 설정 값을 미리 지정합니다.\n",
    "\n",
    "##### 오디오 신호\n",
    "- 우리가 듣는 소리는 공기의 압력 변화로, 이것을 디지털 신호로 변환한 것이 오디오 신호입니다.\n",
    "- 이 신호는 시간에 따라 변하는 진폭 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a32fb60",
   "metadata": {
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-04-08T18:51:59.828208",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.811225",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:21.434163Z",
     "start_time": "2024-07-04T12:32:21.430281Z"
    }
   },
   "source": [
    "class CONFIG:\n",
    "    \"\"\" Configuration Class \"\"\"\n",
    "    SEED = 202102545  # 재현성을 위해 랜덤 시드 고정\n",
    "    \n",
    "    \"\"\" SR(Sample Rate)\n",
    "    - 오디오 데이터의 샘플링 레이트를 설정합니다.\n",
    "    - 높은 샘플링 레이트는 더 높은 주파수의 소리를 캡처할 수 있지만, 처리에 더 많은 계산 자원이 필요합니다.\n",
    "    - 오디오 데이터의 초당 샘플 수를 정의합니다.\n",
    "    \"\"\"\n",
    "    SR = 32000\n",
    "\n",
    "    \"\"\" ROOT_FOLDER\n",
    "    - 데이터셋의 루트 폴더 경로를 설정합니다.\n",
    "    \"\"\"\n",
    "    ROOT_FOLDER = os.path.join(\".\", \"data\")\n",
    "    \n",
    "    \"\"\" BATCH_SIZE\n",
    "    - 학습 시 한 번에 처리할 데이터 샘플의 수를 정의합니다\n",
    "    - 큰 배치 크기는 메모리 사용량을 증가시키지만, 학습 속도를 높입니다.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    \"\"\" N_EPOCHS\n",
    "    - 전체 데이터셋을 학습할 횟수를 정의합니다.\n",
    "    - 에폭 수가 너무 적으면 과소적합이 발생할 수 있고, 너무 많으면 과적합이 발생할 수 있습니다.\n",
    "    \"\"\"\n",
    "    N_EPOCHS = 200\n",
    "    \n",
    "    \"\"\" LR (Learning Rate)\n",
    "    - 모델의 가중치를 업데이트할 때 사용되는 학습 속도를 정의합니다.\n",
    "    - 학습률이 너무 크면 학습이 불안정해질 수 있고, 너무 작으면 학습 속도가 느려집니다.\n",
    "    \"\"\"\n",
    "    LR = 1e-5"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "6700bf8e-7f43-4eac-9bea-25eb1d95fb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:21.447615Z",
     "start_time": "2024-07-04T12:32:21.436175Z"
    }
   },
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\" Fixed RandomSeed\n",
    "    아래의 코드는 머신러닝이나 딥러닝 모델을 훈련할 때, 결과의 재현성을 보장하기 위해 사용되는 함수입니다.\n",
    "    이 함수는 다양한 랜덤 시드를 고정하여, 실행할 때마다 동일한 결과를 얻기 위해 사용됩니다.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)  # Seed 고정"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8a682d49",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2024-04-08T18:52:31.507909",
     "exception": false,
     "start_time": "2024-04-08T18:52:31.500578",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Dataset"
  },
  {
   "cell_type": "code",
   "id": "d2459913-1bf6-40b9-b07d-402699590b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:24.915966Z",
     "start_time": "2024-07-04T12:32:21.448624Z"
    }
   },
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "split_seed = CONFIG.SEED\n",
    "\n",
    "\n",
    "class ContrastingVoiceDataset(Dataset):\n",
    "    download_url = \"https://drive.usercontent.google.com/download?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&export=download&authuser=0&confirm=t&uuid=c40c278b-d74b-4b75-bc79-09e8a3ccffa4&at=APZUnTUvIVFVM9gjGNUCmDb4YZCy%3A1719807236671\"\n",
    "    \n",
    "    @classmethod\n",
    "    def download(cls, root='./data', filename=\"download.zip\", md5=None):\n",
    "        cls.download_root = root\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            download_and_extract_archive(cls.download_url, root, root, filename, md5)\n",
    "            print(\"Extraction completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists in {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_dataset_path(cls, root, train=True):\n",
    "        return os.path.join(root, \"train.csv\" if train else \"test.csv\")\n",
    "\n",
    "    @property\n",
    "    def submission_form_path(cls):\n",
    "        return os.path.join(cls.download_root, \"sample_submission.csv\")\n",
    "    \n",
    "    def __init__(self, root=\"./data\", train=True, split_ratio=1, transform=None):\n",
    "        \"\"\"\n",
    "        Voice Dataset for Contrastive Learning\n",
    "        \n",
    "        :param root: The path to the data directory\n",
    "        :param train: is train or test\n",
    "        :param split_ratio: split ratio for train(can be 0.5 or above) and valid(can be lower than 0.5) set\n",
    "        :param transform: data transformer\n",
    "        :param target_transform: label transformer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.download(root)\n",
    "        self.download_root = root\n",
    "        self.is_train = train\n",
    "        self.multi_label = False\n",
    "\n",
    "        raw_data = self._load_data(self.get_dataset_path(root, train), split_ratio if split_ratio >= 0.5 else 1-split_ratio)\n",
    "        if split_ratio >= 0.5:\n",
    "            self.raw_data, _ = raw_data\n",
    "        else:\n",
    "            _, self.raw_data = raw_data\n",
    "        \n",
    "        self.ids = self.raw_data.get('id')\n",
    "        self.data = self.raw_data.get('path')\n",
    "        if self.data is None:\n",
    "            self.multi_label = True\n",
    "            self.data = [self.raw_data['path0'], self.raw_data['path1']]\n",
    "            self.label = [self.raw_data['label0'], self.raw_data['label1']]\n",
    "        self.label = self.raw_data.get('label')\n",
    "\n",
    "        self.transforms(transform)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_data(dataset_path, split_ratio):\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        \n",
    "        if split_ratio == 1 or split_ratio == 0:\n",
    "            return (df, None) if split_ratio == 1 else (None, df)\n",
    "            \n",
    "        df1, df2, _, _ = split(df, df['label'], test_size=1-split_ratio, random_state=split_seed)\n",
    "        return df1, df2\n",
    "    \n",
    "    def transforms(self, transform=None):\n",
    "        if transform is not None:\n",
    "            if not isinstance(transform, list) and not isinstance(transform, tuple):\n",
    "                transform = [transform]\n",
    "            for t in transform:\n",
    "                self.data, self.label = t(self.data, self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            if self.multi_label:\n",
    "                return [d[index] for d in self.data], [l[index] for l in self.label], self.ids[index]\n",
    "            return self.data[index], self.label[index], self.ids[index]\n",
    "        return self.data[index], self.ids[index]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:25.071179Z",
     "start_time": "2024-07-04T12:32:24.916973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=True, split_ratio=0.8)\n",
    "valid_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=True, split_ratio=0.2)\n",
    "test_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=False, split_ratio=1)\n",
    "\n",
    "print(\"Query Dataset for checking:\", train_dataset[0])\n",
    "train_dataset.raw_data"
   ],
   "id": "8967c36f6abc224b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in .\\data\\download.zip\n",
      "File already exists in .\\data\\download.zip\n",
      "File already exists in .\\data\\download.zip\n",
      "Query Dataset for checking: ('./train/RUNQPNJF.ogg', 'real', 'RUNQPNJF')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             id                  path label\n",
       "23745  BIQYKAWL  ./train/BIQYKAWL.ogg  fake\n",
       "8355   WTCXWLEU  ./train/WTCXWLEU.ogg  fake\n",
       "34884  MRZQEWBF  ./train/MRZQEWBF.ogg  real\n",
       "14462  OHLGZHAF  ./train/OHLGZHAF.ogg  fake\n",
       "43295  FRZNSAKS  ./train/FRZNSAKS.ogg  fake\n",
       "...         ...                   ...   ...\n",
       "7636   FFZRTCWE  ./train/FFZRTCWE.ogg  fake\n",
       "44556  CRTOENWR  ./train/CRTOENWR.ogg  fake\n",
       "19320  IHSKSRCJ  ./train/IHSKSRCJ.ogg  real\n",
       "15989  HGESQVRG  ./train/HGESQVRG.ogg  fake\n",
       "46035  ZANVRAYM  ./train/ZANVRAYM.ogg  fake\n",
       "\n",
       "[44350 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23745</th>\n",
       "      <td>BIQYKAWL</td>\n",
       "      <td>./train/BIQYKAWL.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>WTCXWLEU</td>\n",
       "      <td>./train/WTCXWLEU.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34884</th>\n",
       "      <td>MRZQEWBF</td>\n",
       "      <td>./train/MRZQEWBF.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14462</th>\n",
       "      <td>OHLGZHAF</td>\n",
       "      <td>./train/OHLGZHAF.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43295</th>\n",
       "      <td>FRZNSAKS</td>\n",
       "      <td>./train/FRZNSAKS.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7636</th>\n",
       "      <td>FFZRTCWE</td>\n",
       "      <td>./train/FFZRTCWE.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44556</th>\n",
       "      <td>CRTOENWR</td>\n",
       "      <td>./train/CRTOENWR.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19320</th>\n",
       "      <td>IHSKSRCJ</td>\n",
       "      <td>./train/IHSKSRCJ.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15989</th>\n",
       "      <td>HGESQVRG</td>\n",
       "      <td>./train/HGESQVRG.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46035</th>\n",
       "      <td>ZANVRAYM</td>\n",
       "      <td>./train/ZANVRAYM.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44350 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:25.147363Z",
     "start_time": "2024-07-04T12:32:25.072185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply seperated dataset\n",
    "ContrastingVoiceDataset.get_dataset_path = lambda self, root, train=True: os.path.join(root, \"train.csv\" if train else \"test_separated.csv\")\n",
    "test_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=False, split_ratio=1)"
   ],
   "id": "ba85cb259492f9fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in .\\data\\download.zip\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data Transformation",
   "id": "d7888fecea819346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:26.096257Z",
     "start_time": "2024-07-04T12:32:25.148368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "\n",
    "def get_resnet152():\n",
    "    model_id = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "    root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "\n",
    "    import os\n",
    "    if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "    if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "\n",
    "    resnet = wespeaker.load_model_local(root_dir)\n",
    "    resnet.set_gpu(-1 if device == torch.device('cpu') else 0)\n",
    "\n",
    "    def resnet152(pcm, sample_rate=None):\n",
    "        if isinstance(pcm, str):\n",
    "            return resnet.extract_embedding(pcm)\n",
    "        else:\n",
    "            pass  # TODO: 메모리에 로드된 상태의 오디오 처리 코드 필요\n",
    "            #return extract_embedding(resnet, pcm, sample_rate)\n",
    "\n",
    "    print(f\"ResNet152 Model Loaded on {resnet.device}\")\n",
    "    return resnet152"
   ],
   "id": "b02f83258987e0c8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:26.105452Z",
     "start_time": "2024-07-04T12:32:26.098264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_embedding_file = \"train_embedding.pt\"\n",
    "valid_embedding_file = \"valid_embedding.pt\"\n",
    "test_embedding_file = \"test_embedding.pt\"\n",
    "\n",
    "\n",
    "def get_pretrained_embedding():\n",
    "    if not os.path.isfile(train_embedding_file) \\\n",
    "            or not os.path.isfile(valid_embedding_file) \\\n",
    "            or not os.path.isfile(test_embedding_file):\n",
    "        return get_resnet152()\n",
    "    else:\n",
    "        train_embedding = torch.load(train_embedding_file)\n",
    "        valid_embedding = torch.load(valid_embedding_file)\n",
    "        test_embedding = torch.load(test_embedding_file)\n",
    "        dataset_list = {\n",
    "            len(train_embedding): train_embedding,\n",
    "            len(valid_embedding): valid_embedding,\n",
    "            len(test_embedding): test_embedding\n",
    "        }\n",
    "        print(\"INFO: Pretrained Voice Embedding loaded.\", dataset_list.keys())\n",
    "        \n",
    "        def load_embedding(dataset):\n",
    "            length = len(dataset)\n",
    "            return dataset_list[length]\n",
    "        \n",
    "        load_embedding.__dict__['pretrained'] = True\n",
    "        return load_embedding"
   ],
   "id": "551b0bd63cae9387",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:27.951789Z",
     "start_time": "2024-07-04T12:32:26.107056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_embedding(pretrained=get_pretrained_embedding(), sample_rate=CONFIG.SR):\n",
    "    get_pth = lambda path: os.path.join(CONFIG.ROOT_FOLDER, *path[1:].split(\"/\"))\n",
    "    \n",
    "    if not pretrained:\n",
    "        def extract_embedding(datas, labels):  # TODO: 임베딩 코드 추가 필요\n",
    "            return [torchaudio.load(data) for data in datas], labels\n",
    "        return extract_embedding\n",
    "    \n",
    "    def pretrained_embedding(dataset, labels):\n",
    "        if pretrained.__dict__.get('pretrained'):\n",
    "            new_dataset = pretrained(dataset)\n",
    "            print(\"INFO: Voice Embedding extracted.\")\n",
    "        else:\n",
    "            if isinstance(dataset, list) and isinstance(dataset[0], pd.Series):\n",
    "                new_dataset = [[] * len(dataset)]\n",
    "                \n",
    "                iterator = iter(dataset)\n",
    "\n",
    "                dataset1 = next(iterator)\n",
    "\n",
    "                for data in zip(tqdm(dataset1), *iterator):\n",
    "                    for path, nd in zip(data, new_dataset):\n",
    "                        nd.append(pretrained(get_pth(path), sample_rate))\n",
    "\n",
    "                dataset_size = len(new_dataset[0])\n",
    "            else:\n",
    "                new_dataset = []\n",
    "\n",
    "                for data in tqdm(dataset):\n",
    "                    new_dataset.append(pretrained(get_pth(data), sample_rate))\n",
    "\n",
    "                dataset_size = len(new_dataset)\n",
    "\n",
    "            torch.save(new_dataset, \"nonamed.pt\")\n",
    "            if dataset_size == len(train_dataset.raw_data):\n",
    "                os.rename(\"nonamed.pt\", \"train_embedding.pt\")\n",
    "            elif dataset_size == len(valid_dataset.raw_data):\n",
    "                os.rename(\"nonamed.pt\", \"valid_embedding.pt\")\n",
    "            elif dataset_size == len(test_dataset.raw_data):\n",
    "                os.rename(\"nonamed.pt\", \"test_embedding.pt\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Dataset Size - Could not find relevant dataset sized {dataset_size}.\")\n",
    "                \n",
    "            print(\"INFO: Voice Embedding saved.\")\n",
    "                \n",
    "        return new_dataset, labels  # [pretrained(get_pth(path), sample_rate) for path in dataset]\n",
    "    \n",
    "    return pretrained_embedding"
   ],
   "id": "d8c02a7d-dfb6-4f8b-8df1-db2abaa1cb5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet152 Model Loaded on cuda:0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:27.956708Z",
     "start_time": "2024-07-04T12:32:27.953805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_tensor_label(datas, labels):\n",
    "    return datas, [torch.tensor([0]) if lb == \"fake\" else torch.tensor([1]) for lb in labels]"
   ],
   "id": "2a20f2533794d230",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T12:32:27.974125Z",
     "start_time": "2024-07-04T12:32:27.957065Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.data[0]",
   "id": "45496ef1b4dcf60f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ./test_separated/TEST_00000/0.ogg\n",
       "1        ./test_separated/TEST_00001/0.ogg\n",
       "2        ./test_separated/TEST_00002/0.ogg\n",
       "3        ./test_separated/TEST_00003/0.ogg\n",
       "4        ./test_separated/TEST_00004/0.ogg\n",
       "                       ...                \n",
       "49995    ./test_separated/TEST_49995/0.ogg\n",
       "49996    ./test_separated/TEST_49996/0.ogg\n",
       "49997    ./test_separated/TEST_49997/0.ogg\n",
       "49998    ./test_separated/TEST_49998/0.ogg\n",
       "49999    ./test_separated/TEST_49999/0.ogg\n",
       "Name: path0, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-04T12:32:27.975145Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.transforms(transform=to_embedding())",
   "id": "a3d10e33169d15dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10591/50000 [10:04<28:03, 23.41it/s] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset.transforms(transform=[to_embedding(), to_tensor_label])\n",
    "valid_dataset.transforms(transform=[to_embedding(), to_tensor_label])\n",
    "test_dataset.transforms(transform=to_embedding())"
   ],
   "id": "6a2a071509e465a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for dataset, index in zip(train_dataset, range(5)):\n",
    "    print(f\"Dataset {index}: {'FAKE' if dataset[1] == torch.tensor([0]) else 'REAL'}\", dataset[0])"
   ],
   "id": "4739bbf8ec8a6c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for dataset, index in zip(valid_dataset, range(5)):\n",
    "    print(f\"Dataset {index}: {'FAKE' if dataset[1] == torch.tensor([0]) else 'REAL'}\", dataset[0])"
   ],
   "id": "6ef836d6cfa4590f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 라벨 분리\n",
    "\n",
    "import copy\n",
    "\n",
    "real_dataset = copy.deepcopy(train_dataset)\n",
    "fake_dataset = copy.deepcopy(train_dataset)\n",
    "\n",
    "def data_filter(target):\n",
    "    def filter_data(dataset, labels):\n",
    "        filtered = [(data, label) for data, label in zip(dataset, labels) if label == target]\n",
    "        transposed = list(zip(*filtered))\n",
    "        return transposed\n",
    "    return filter_data\n",
    "\n",
    "real_dataset.transforms(transform=data_filter(torch.tensor([1])))\n",
    "fake_dataset.transforms(transform=data_filter(torch.tensor([0])))"
   ],
   "id": "bc707239b611a9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for dataset, index in zip(real_dataset, range(5)):\n",
    "    print(f\"Dataset {index}: {'FAKE' if dataset[1] == torch.tensor([0]) else 'REAL'}\", dataset[0])"
   ],
   "id": "913d3c2274418754",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "    - DataLoader는 구축된 데이터셋에서 배치크기(batch_size)에 맞게 데이터를 추출하고, 필요에 따라 섞거나(shuffle=True) 순서대로 반환(shuffle=False)하는 역할을 합니다.\n",
    "    - 훈련 데이터(train_loader)는 일반적으로 섞어서 모델이 데이터에 덜 편향되게 학습하도록하며,\n",
    "      검증 데이터(val_loader)는 모델 성능 평가를 위해 순서대로 사용하고,\n",
    "      테스트 데이터(test_loader)는 최종적인 추론을 위해 사용합니다.\n",
    "\n",
    "    이렇게 DataLoader를 사용함으로써, 효율적인 데이터 처리와 모델 학습 및 평가가 가능해집니다."
   ],
   "id": "270e0ffa984bb3a"
  },
  {
   "cell_type": "code",
   "id": "dff1c7df-fbe7-4a61-9f66-c55138697eab",
   "metadata": {},
   "source": [
    "BATCH_SIZE = CONFIG.BATCH_SIZE\n",
    "\n",
    "real_loader = DataLoader(real_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "augmt_loader = DataLoader(real_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "fake_loader = DataLoader(fake_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "effb3435-cdb7-4a31-b7ef-fc16237cfc4a",
   "metadata": {},
   "source": "## Define Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VoiceEncoder(nn.Module):\n",
    "    \"\"\" Voice Encoder Model \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        out = self.fc2(h1)\n",
    "        return F.sigmoid(out)"
   ],
   "id": "897283254be72389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ConstrastiveDistanceFunction(nn.Module):\n",
    "    \"\"\" Contrastive Distance Function \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_size, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, latent_size * 2)\n",
    "        self.fc2 = nn.Linear(latent_size * 2, latent_size)\n",
    "        self.out = nn.Linear(latent_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, anchor, comparative):\n",
    "        #anchor = self.fc2(self.relu(self.fc1(anchor)))\n",
    "        #comparative = self.fc2(self.relu(self.fc1(comparative)))\n",
    "        #euclidean_distance = F.pairwise_distance(anchor, comparative)\n",
    "        #print(euclidean_distance.shape)\n",
    "        #loss = torch.mean((1 - self.margin) * euclidean_distance + self.margin * torch.clamp(self.margin - euclidean_distance, min=0.0))\n",
    "        #return F.sigmoid(euclidean_distance)#euclidean_distance#loss\n",
    "        combined = anchor + comparative\n",
    "        output = self.relu(self.fc2(self.relu(self.fc1(combined))))\n",
    "        return F.sigmoid(self.out(output))"
   ],
   "id": "608c75bc69efb15f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aba60869-b8a5-46c2-b185-00131161a158",
   "metadata": {},
   "source": [
    "class BinaryDiscriminator(nn.Module):\n",
    "    \"\"\" Binary Discriminator Model using Contrastive Learning \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_size, latent_size, margin):\n",
    "        super().__init__()\n",
    "        self.encoder = VoiceEncoder(embedding_dim, hidden_size, latent_size)\n",
    "        self.distance = ConstrastiveDistanceFunction(latent_size, margin)\n",
    "\n",
    "    def forward(self, anchor, comparative):\n",
    "        anchor = self.encoder(anchor)\n",
    "        #print(anchor.shape)\n",
    "        return [self.distance(anchor, self.encoder(comp)) for comp in comparative]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 모델 파라미터 지정\n",
    "model_params = dict(\n",
    "    embedding_dim=len(train_dataset[0][0]),\n",
    "    hidden_size=512,\n",
    "    latent_size=128,\n",
    "    margin=0.5\n",
    ")\n",
    "model_params"
   ],
   "id": "b803a71eb2decea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 모델 생성\n",
    "voice_discrimination_model = BinaryDiscriminator(**model_params)\n",
    "discriminator = voice_discrimination_model\n",
    "discriminator.to(device)"
   ],
   "id": "8669c3af6468c5d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# BinaryCrossEntropy\n",
    "criterion = nn.BCELoss().to(device)"
   ],
   "id": "26c3d8aa65edbba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=CONFIG.LR)"
   ],
   "id": "958745152fe04f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28c4a8c-0219-46bd-bd46-09d0327fe7eb",
   "metadata": {},
   "source": "## Train & Validation"
  },
  {
   "cell_type": "code",
   "id": "2a7253de-ce9a-45a8-b71f-7752e427941c",
   "metadata": {},
   "source": [
    "epochs = CONFIG.N_EPOCHS\n",
    "batch_size = CONFIG.BATCH_SIZE\n",
    "train_amount = len(real_loader)\n",
    "valid_amount = len(valid_loader)\n",
    "anchor_sample = next(iter(real_loader))[0][0].to(device)\n",
    "\n",
    "#best_val_score = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    discriminator.train()\n",
    "    train_loss = [0, 0]\n",
    "    \n",
    "    for i, (real, augmt, fake) in enumerate(zip(real_loader, augmt_loader, fake_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        anchor, _ = real\n",
    "        positive, _ = augmt\n",
    "        negative, _ = fake\n",
    "        \n",
    "        if anchor.shape[0] != positive.shape[0] or anchor.shape[0] != negative.shape[0]:\n",
    "            continue\n",
    "\n",
    "        output = discriminator(anchor.to(device), [positive.to(device), negative.to(device)])\n",
    "        \n",
    "        positive_loss = criterion(output[0], torch.ones(output[0].shape).to(device))\n",
    "        negative_loss = criterion(output[1], torch.zeros(output[1].shape).to(device))\n",
    "        loss = positive_loss + negative_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss[0] += positive_loss.item()\n",
    "        train_loss[1] += negative_loss.item()\n",
    "        \n",
    "        print(f\"\\rEpoch [{epoch+1}/{epochs}], Step: [{i+1}/{train_amount}], Train Loss: {train_loss[0]/(i+1):.5f} | {train_loss[1]/(i+1):.5f}\", end=\"\")\n",
    "\n",
    "    # Validation\n",
    "    discriminator.eval()\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for feature, label, _ in valid_loader:\n",
    "            feature, label = feature.to(device), label.to(device)\n",
    "            \n",
    "            predicted = discriminator(anchor_sample, [feature])[0]\n",
    "            predicted = (predicted >= 0.5).float()\n",
    "            \n",
    "            loss = criterion(predicted, label.float())\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += torch.eq(predicted, label).sum().item() / batch_size\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss[0]/train_amount:.5f} | {train_loss[1]/train_amount:.5f} => Valid Loss : {valid_loss/valid_amount:.5f}, Valid ACC : {valid_acc/valid_amount:.5%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Save\n",
    "torch.save(discriminator.state_dict(), \"contrastive_model_acc_99.67568.pt\")"
   ],
   "id": "f49b0f23f044b1f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a978b0e6-b773-423a-93e4-ce463f4d4d84",
   "metadata": {},
   "source": [
    "### Inference\n",
    "테스트 데이터셋에 대한 추론은 다음 순서로 진행됩니다.\n",
    "\n",
    "1. 모델 및 디바이스 설정\n",
    "    - 모델을 주어진 device(GPU 또는 CPU)로 이동시키고, 평가모드로 전환합니다.\n",
    "2. 예측 수행\n",
    "    - 예측 결과를 저장한 빈 리스트를 초기화하고 test_loader에서 배치별로 데이터를 불러와 예측을 수행합니다.\n",
    "    - 각 배치에 대해 스펙트로그램 데이터를 device로 이동시킵니다.\n",
    "    - 모델 예측 확률(probs)을 계산합니다.\n",
    "    - 예측 확률을 predictions리스트에 추가합니다."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Model\n",
    "discriminator = BinaryDiscriminator(**model_params)\n",
    "discriminator.load_state_dict(torch.load(\"contrastive_model_acc_99.67568.pt\"))\n",
    "discriminator.to(device)"
   ],
   "id": "a7711cfa292ee987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from speechbrain.inference.VAD import VAD\n",
    "\n",
    "def vad_filter(\n",
    "        dataset, use_preset=True,\n",
    "        filter_model=VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir='./.cache/vad-crdnn-libriparty', run_opts={\"device\":\"cuda\"})\n",
    "):\n",
    "    def query_preset(_id: str, index: int, *args, **kwargs):\n",
    "        return dataset.label[index][int(_id.replace(\"TEST_\", \"\"))]\n",
    "    \n",
    "    def vad(_id: str, index: int, activation_th=0.4):\n",
    "        file_path = os.path.join(\".\", \"test_separated\", _id, f\"{index}\", \".16hz.ogg\")\n",
    "        boundaries = filter_model.get_speech_segments(file_path.replace(\"ogg\", \"16hz.ogg\"), activation_th=activation_th)\n",
    "        label = \"noise\"\n",
    "        last_end = 0\n",
    "        for i in range(boundaries.shape[0]):\n",
    "            begin_value = boundaries[i, 0]\n",
    "            end_value = boundaries[i, 1]\n",
    "            if last_end == begin_value:\n",
    "                label = \"speech\"\n",
    "            last_end = end_value\n",
    "        return label\n",
    "    \n",
    "    return query_preset if use_preset else vad"
   ],
   "id": "61716d1efabf39a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5889b493-d760-4cac-9ced-c3715195e8be",
   "metadata": {},
   "source": [
    "predicted_labels = []\n",
    "anchor_sample = next(iter(real_dataset))[0][0].to(device)\n",
    "vad = vad_filter(test_dataset, use_preset=True)\n",
    "\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(iter(test_loader)):\n",
    "        if len(inputs) == 3:\n",
    "            (feature0, feature1), _, (id0, id1) = inputs\n",
    "        else:\n",
    "            (feature0, feature1), (id0, id1) = inputs\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for feature in [feature0, feature1]:\n",
    "            feature = feature.to(device)\n",
    "            if vad(feature) == \"speech\":\n",
    "                features.append(feature)\n",
    "\n",
    "        predicted = discriminator(features)\n",
    "        \n",
    "        if len(predicted) == 1:\n",
    "            if predicted[0] >= 0.5:\n",
    "                positive = 1\n",
    "                negative = 0\n",
    "        else:\n",
    "            is_positive0 = predicted[0] >= 0.5\n",
    "            is_positive1 = predicted[1] >= 0.5\n",
    "            \n",
    "            if is_positive0 and is_positive1:\n",
    "                positive = 1\n",
    "                negative = 0\n",
    "            elif is_positive0 and not is_positive1:\n",
    "                positive = 1\n",
    "                negative = 1 - predicted[1]\n",
    "            elif not is_positive0 and is_positive1:\n",
    "                positive = 1 - predicted[0]\n",
    "                negative = 1\n",
    "            else:\n",
    "                positive = 0\n",
    "                negative = 1\n",
    "        \n",
    "        predicted_labels += (positive, negative)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8fae66d-8f54-46d5-9201-0f4b0db76e76",
   "metadata": {},
   "source": [
    "### Submission\n",
    "추론 결과를 제출 양식에 덮어 씌워 CSV 파일로 생성하는 과정은 다음과 같습니다.\n",
    "\n",
    "1. 제출 양식 로드\n",
    "    - pd.read_csv('./sample_submission.csv')를 사용하여 제출을 위한 샘플 형식 파일을 로드합니다.\n",
    "    - 이 파일은 일반적으로 각 테스트 샘플에 대한 ID와 예측해야 하는 필드가 포함된 템플릿 형태를 가지고 있습니다.\n",
    "2. 예측 결과 할당\n",
    "    - submit.iloc[:,1:] = preds 추론함수(inference)에서 반환된 예측결과(preds)를 샘플 제출 파일에 2번째 열부터 할당합니다.\n",
    "3. 제출 파일 저장\n",
    "    - 수정된 제출 파일을 baseline_submit 이란 이름의 CSV 파일로 저장합니다.\n",
    "    - index=False는 파일 저장시 추가적인 index가 발생하지 않도록 설정하여, 제작한 제출 파일과 동일한 형태의 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f8314c4-1dce-4f79-9f3d-77d320a3746e",
   "metadata": {},
   "source": [
    "submit = pd.read_csv(test_dataset.submission_form_path)\n",
    "submit.iloc[:, 1:] = predicted_labels\n",
    "submit.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e28d71bc-6703-40f7-9716-a0ef897eca83",
   "metadata": {},
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732842,
     "sourceId": 8066583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1830.928153,
   "end_time": "2024-04-08T19:22:15.265404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T18:51:44.337251",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a8f214ec354c44b73d439565382278": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06a1ede084cd487ebf3c469be657b53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80013ce73542415e82be091acccb89fe",
        "IPY_MODEL_d280070ca871485fbd2b7d34b1c9fd10",
        "IPY_MODEL_8212bde7695f494cbabea66983e4cf29"
       ],
       "layout": "IPY_MODEL_c4da594b806c4c2bbff6e8cdaf6088eb"
      }
     },
     "37e28ba3d8564da4a3257c3729310584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80013ce73542415e82be091acccb89fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_95e72a34a4374fd5b4b147772085bb7c",
       "placeholder": "​",
       "style": "IPY_MODEL_37e28ba3d8564da4a3257c3729310584",
       "value": "model.safetensors: 100%"
      }
     },
     "8212bde7695f494cbabea66983e4cf29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9e4e04bb60e40d6b46d782f8156d05f",
       "placeholder": "​",
       "style": "IPY_MODEL_b1fa83d0511a4d8a910b8fdb40d32c29",
       "value": " 36.5M/36.5M [00:01&lt;00:00, 41.1MB/s]"
      }
     },
     "95e72a34a4374fd5b4b147772085bb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1fa83d0511a4d8a910b8fdb40d32c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4da594b806c4c2bbff6e8cdaf6088eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d280070ca871485fbd2b7d34b1c9fd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01a8f214ec354c44b73d439565382278",
       "max": 36494688,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcd2393d73d14514851a7d9ef50315fc",
       "value": 36494688
      }
     },
     "d9e4e04bb60e40d6b46d782f8156d05f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcd2393d73d14514851a7d9ef50315fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
