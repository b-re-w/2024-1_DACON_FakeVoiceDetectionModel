{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840dc88f7c14e1fa",
   "metadata": {},
   "source": [
    "# SW중심대학 디지털 경진대회_SW와 생성AI의 만남 : AI부문\n",
    " - 이 AI 경진대회에서는 5초 분량의 오디오 샘플에서 진짜 사람 목소리와 AI가 생성한 가짜 목소리를 정확하게 구분할 수 있는 모델을 개발하는 것이 목표입니다.\n",
    " - 이 작업은 보안, 사기 감지 및 오디오 처리 기술 향상 등 다양한 분야에서 매우 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d73a1",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-04-08T18:51:47.130888",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.123877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "모델 학습 및 추론에 사용할 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbadbd56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:40.181888Z",
     "start_time": "2024-07-04T17:28:40.177479Z"
    },
    "papermill": {
     "duration": 12.650384,
     "end_time": "2024-04-08T18:51:59.788340",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.137956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d80cf24-13e8-480c-94eb-2982bb52510d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:40.758201Z",
     "start_time": "2024-07-04T17:28:40.754942Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64eb379-e527-46c4-8b12-ead8db628070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:41.213128Z",
     "start_time": "2024-07-04T17:28:41.209280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 1\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc82cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 12 02:14:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    34W / 250W |    733MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5625      C   ...brew/anaconda3/bin/python      731MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2de5d",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2024-04-08T18:51:59.803571",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.796330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n",
    "- 딥러닝 모델을 학습하기 전에 설정해야하는 다양한 매개변수를 정의하는 설정 클래스입니다.\n",
    "- 클래스를 사용하여 학습에 필요한 설정 값을 미리 지정합니다.\n",
    "\n",
    "##### 오디오 신호\n",
    "- 우리가 듣는 소리는 공기의 압력 변화로, 이것을 디지털 신호로 변환한 것이 오디오 신호입니다.\n",
    "- 이 신호는 시간에 따라 변하는 진폭 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a32fb60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:42.098985Z",
     "start_time": "2024-07-04T17:28:42.095152Z"
    },
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-04-08T18:51:59.828208",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.811225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    \"\"\" Configuration Class \"\"\"\n",
    "    SEED = 202102545  # 재현성을 위해 랜덤 시드 고정\n",
    "    \n",
    "    \"\"\" SR(Sample Rate)\n",
    "    - 오디오 데이터의 샘플링 레이트를 설정합니다.\n",
    "    - 높은 샘플링 레이트는 더 높은 주파수의 소리를 캡처할 수 있지만, 처리에 더 많은 계산 자원이 필요합니다.\n",
    "    - 오디오 데이터의 초당 샘플 수를 정의합니다.\n",
    "    \"\"\"\n",
    "    SR = 32000\n",
    "\n",
    "    \"\"\" ROOT_FOLDER\n",
    "    - 데이터셋의 루트 폴더 경로를 설정합니다.\n",
    "    \"\"\"\n",
    "    ROOT_FOLDER = os.path.join(\".\", \"data\")\n",
    "    \n",
    "    \"\"\" BATCH_SIZE\n",
    "    - 학습 시 한 번에 처리할 데이터 샘플의 수를 정의합니다\n",
    "    - 큰 배치 크기는 메모리 사용량을 증가시키지만, 학습 속도를 높입니다.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    \"\"\" N_EPOCHS\n",
    "    - 전체 데이터셋을 학습할 횟수를 정의합니다.\n",
    "    - 에폭 수가 너무 적으면 과소적합이 발생할 수 있고, 너무 많으면 과적합이 발생할 수 있습니다.\n",
    "    \"\"\"\n",
    "    N_EPOCHS = 15\n",
    "    \n",
    "    \"\"\" LR (Learning Rate)\n",
    "    - 모델의 가중치를 업데이트할 때 사용되는 학습 속도를 정의합니다.\n",
    "    - 학습률이 너무 크면 학습이 불안정해질 수 있고, 너무 작으면 학습 속도가 느려집니다.\n",
    "    \"\"\"\n",
    "    LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6700bf8e-7f43-4eac-9bea-25eb1d95fb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:42.485699Z",
     "start_time": "2024-07-04T17:28:42.478184Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\" Fixed RandomSeed\n",
    "    아래의 코드는 머신러닝이나 딥러닝 모델을 훈련할 때, 결과의 재현성을 보장하기 위해 사용되는 함수입니다.\n",
    "    이 함수는 다양한 랜덤 시드를 고정하여, 실행할 때마다 동일한 결과를 얻기 위해 사용됩니다.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)  # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a682d49",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2024-04-08T18:52:31.507909",
     "exception": false,
     "start_time": "2024-04-08T18:52:31.500578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2459913-1bf6-40b9-b07d-402699590b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:43.639401Z",
     "start_time": "2024-07-04T17:28:43.628630Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class ContrastingVoiceDataset(Dataset):\n",
    "    download_url = \"https://drive.usercontent.google.com/download?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&export=download&authuser=0&confirm=t&uuid=c40c278b-d74b-4b75-bc79-09e8a3ccffa4&at=APZUnTUvIVFVM9gjGNUCmDb4YZCy%3A1719807236671\"\n",
    "    \n",
    "    @classmethod\n",
    "    def download(cls, root='./data', filename=\"download.zip\", md5=None):\n",
    "        cls.download_root = root\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            utils.download_and_extract_archive(cls.download_url, root, root, filename, md5)\n",
    "            print(\"Extraction completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists in {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_dataset_path(cls, root, train=True):\n",
    "        return os.path.join(root, \"train.csv\" if train else \"test.csv\")\n",
    "\n",
    "    @property\n",
    "    def submission_form_path(cls):\n",
    "        return os.path.join(cls.download_root, \"sample_submission.csv\")\n",
    "    \n",
    "    def __init__(self, root=\"./data\", train=True, split_ratio=1, transform=None):\n",
    "        \"\"\"\n",
    "        Voice Dataset for Contrastive Learning\n",
    "        \n",
    "        :param root: The path to the data directory\n",
    "        :param train: is train or test\n",
    "        :param split_ratio: split ratio for train(can be 0.5 or above) and valid(can be lower than 0.5) set\n",
    "        :param transform: data transformer\n",
    "        :param target_transform: label transformer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.download(root)\n",
    "        self.download_root = root\n",
    "        self.is_train = train\n",
    "\n",
    "        raw_data = self._load_data(self.get_dataset_path(root, train), split_ratio if split_ratio >= 0.5 else 1-split_ratio)\n",
    "        if split_ratio >= 0.5:\n",
    "            self.raw_data, _ = raw_data\n",
    "        else:\n",
    "            _, self.raw_data = raw_data\n",
    "            \n",
    "        self.multi_label = 'path' not in self.raw_data.columns\n",
    "        \n",
    "        self.ids = self.raw_data['id']\n",
    "        if self.multi_label:\n",
    "            self.data = [self.raw_data['path0'], self.raw_data['path1']]\n",
    "            self.label = [self.raw_data['label0'], self.raw_data['label1']]\n",
    "        else:\n",
    "            self.data = self.raw_data['path']\n",
    "            if 'label' in self.raw_data.columns:\n",
    "                self.label = self.raw_data['label']\n",
    "            else:\n",
    "                self.label = None\n",
    "\n",
    "        self.transforms(transform)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_data(dataset_path, split_ratio):\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        \n",
    "        if split_ratio == 1 or split_ratio == 0:\n",
    "            return (df, None) if split_ratio == 1 else (None, df)\n",
    "            \n",
    "        df1, df2, _, _ = split(df, df['label'], test_size=1-split_ratio, random_state=202102545)\n",
    "        return df1, df2\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_id_from_path(path: str):\n",
    "        return [pth for pth in path.replace(\"/0.ogg\", \".ogg\").split(\"/\") if '.ogg' in pth][0].replace(\".ogg\", \"\")\n",
    "\n",
    "    def transforms(self, transform=None):\n",
    "        if transform is not None:\n",
    "            if not isinstance(transform, list) and not isinstance(transform, tuple):\n",
    "                transform = [transform]\n",
    "            for t in transform:\n",
    "                self.ids, self.data, self.label = t(self.ids, self.data, self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.multi_label:\n",
    "            return len(self.data[0])\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            if self.multi_label:\n",
    "                return [d[index] for d in self.data], [l[index] for l in self.label], self.ids[index]\n",
    "            return self.data[index], self.label[index], self.ids[index]\n",
    "        return self.data[index], self.ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5b79398623de5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:43.919104Z",
     "start_time": "2024-07-04T17:28:43.914939Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_list(multi_label=False):\n",
    "    def to_list_inner(ids, datas, labels):\n",
    "        ids = [_id for _id in ids]\n",
    "        if not multi_label:\n",
    "            datas, labels = [datas], [labels]\n",
    "        datas = [[d for d in data] for data in datas]\n",
    "        try:\n",
    "            labels = [[l for l in label] for label in labels]\n",
    "        except TypeError:\n",
    "            pass\n",
    "        if not multi_label:\n",
    "            datas, labels = datas[0], labels[0]\n",
    "        return ids, datas, labels\n",
    "    return to_list_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8967c36f6abc224b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:44.388141Z",
     "start_time": "2024-07-04T17:28:44.185222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "File already exists in ./data/download.zip\n",
      "Query Dataset for checking: ('./train/BIQYKAWL.ogg', 'fake', 'BIQYKAWL')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23745</th>\n",
       "      <td>BIQYKAWL</td>\n",
       "      <td>./train/BIQYKAWL.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>WTCXWLEU</td>\n",
       "      <td>./train/WTCXWLEU.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34884</th>\n",
       "      <td>MRZQEWBF</td>\n",
       "      <td>./train/MRZQEWBF.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14462</th>\n",
       "      <td>OHLGZHAF</td>\n",
       "      <td>./train/OHLGZHAF.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43295</th>\n",
       "      <td>FRZNSAKS</td>\n",
       "      <td>./train/FRZNSAKS.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7636</th>\n",
       "      <td>FFZRTCWE</td>\n",
       "      <td>./train/FFZRTCWE.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44556</th>\n",
       "      <td>CRTOENWR</td>\n",
       "      <td>./train/CRTOENWR.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19320</th>\n",
       "      <td>IHSKSRCJ</td>\n",
       "      <td>./train/IHSKSRCJ.ogg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15989</th>\n",
       "      <td>HGESQVRG</td>\n",
       "      <td>./train/HGESQVRG.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46035</th>\n",
       "      <td>ZANVRAYM</td>\n",
       "      <td>./train/ZANVRAYM.ogg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                  path label\n",
       "23745  BIQYKAWL  ./train/BIQYKAWL.ogg  fake\n",
       "8355   WTCXWLEU  ./train/WTCXWLEU.ogg  fake\n",
       "34884  MRZQEWBF  ./train/MRZQEWBF.ogg  real\n",
       "14462  OHLGZHAF  ./train/OHLGZHAF.ogg  fake\n",
       "43295  FRZNSAKS  ./train/FRZNSAKS.ogg  fake\n",
       "...         ...                   ...   ...\n",
       "7636   FFZRTCWE  ./train/FFZRTCWE.ogg  fake\n",
       "44556  CRTOENWR  ./train/CRTOENWR.ogg  fake\n",
       "19320  IHSKSRCJ  ./train/IHSKSRCJ.ogg  real\n",
       "15989  HGESQVRG  ./train/HGESQVRG.ogg  fake\n",
       "46035  ZANVRAYM  ./train/ZANVRAYM.ogg  fake\n",
       "\n",
       "[44350 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=True, split_ratio=0.8, transform=to_list())\n",
    "valid_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=True, split_ratio=0.2, transform=to_list())\n",
    "test_dataset = ContrastingVoiceDataset(root=CONFIG.ROOT_FOLDER, train=False, split_ratio=1, transform=to_list())\n",
    "\n",
    "print(\"Query Dataset for checking:\", train_dataset[0])\n",
    "train_dataset.raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7888fecea819346",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02f83258987e0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:45.353458Z",
     "start_time": "2024-07-04T17:28:45.347697Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "\n",
    "def get_resnet152():\n",
    "    model_id = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "    root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "\n",
    "    import os\n",
    "    if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "    if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "\n",
    "    resnet = wespeaker.load_model_local(root_dir)\n",
    "    resnet.set_gpu(-1 if device == torch.device('cpu') else 0)\n",
    "\n",
    "    def resnet152(pcm, sample_rate=None):\n",
    "        if isinstance(pcm, str):\n",
    "            return resnet.extract_embedding(pcm)\n",
    "        else:\n",
    "            pass  # TODO: 메모리에 로드된 상태의 오디오 처리 코드 필요\n",
    "            #return extract_embedding(resnet, pcm, sample_rate)\n",
    "\n",
    "    print(f\"ResNet152 Model Loaded on {resnet.device}\")\n",
    "    return resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551b0bd63cae9387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:45.902950Z",
     "start_time": "2024-07-04T17:28:45.897113Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embedding_file = \"train_embedding.pt\"\n",
    "valid_embedding_file = \"valid_embedding.pt\"\n",
    "\n",
    "\n",
    "def get_pretrained_embedding():\n",
    "    if not os.path.isfile(train_embedding_file) \\\n",
    "            or not os.path.isfile(valid_embedding_file):\n",
    "        return get_resnet152()\n",
    "    else:\n",
    "        train_embedding = torch.load(train_embedding_file)\n",
    "        valid_embedding = torch.load(valid_embedding_file)\n",
    "        dataset_list = {\n",
    "            len(train_embedding): train_embedding,\n",
    "            len(valid_embedding): valid_embedding,\n",
    "        }\n",
    "        print(\"INFO: Pretrained Voice Embedding loaded.\", dataset_list.keys())\n",
    "        \n",
    "        def load_embedding(dataset):\n",
    "            return dataset_list[len(dataset)]\n",
    "        \n",
    "        load_embedding.__dict__['pretrained'] = True\n",
    "        return load_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c02a7d-dfb6-4f8b-8df1-db2abaa1cb5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:54.087611Z",
     "start_time": "2024-07-04T17:28:46.038856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pretrained Voice Embedding loaded. dict_keys([44350, 11088])\n"
     ]
    }
   ],
   "source": [
    "def to_embedding(pretrained=get_pretrained_embedding(), sample_rate=CONFIG.SR):\n",
    "    get_pth = lambda path: os.path.join(CONFIG.ROOT_FOLDER, *path[1:].split(\"/\"))\n",
    "    \n",
    "    if not pretrained:\n",
    "        def extract_embedding(ids, datas, labels):  # TODO: 임베딩 코드 추가 필요\n",
    "            return ids, [torchaudio.load(data) for data in datas], labels\n",
    "        return extract_embedding\n",
    "    \n",
    "    def pretrained_embedding(ids, dataset, labels):\n",
    "        if pretrained.__dict__.get('pretrained'):\n",
    "            new_dataset = pretrained(dataset)\n",
    "            print(\"INFO: Voice Embedding extracted.\")\n",
    "        else:\n",
    "            new_dataset = []\n",
    "\n",
    "            for data in tqdm(dataset):\n",
    "                new_dataset.append(pretrained(get_pth(data), sample_rate))\n",
    "\n",
    "            dataset_size = len(new_dataset)\n",
    "\n",
    "            torch.save(new_dataset, \"nonamed.pt\")\n",
    "            if dataset_size == len(train_dataset.raw_data):\n",
    "                os.rename(\"nonamed.pt\", \"train_embedding.pt\")\n",
    "            elif dataset_size == len(valid_dataset.raw_data):\n",
    "                os.rename(\"nonamed.pt\", \"valid_embedding.pt\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Dataset Size - Could not find relevant dataset sized {dataset_size}.\")\n",
    "                \n",
    "            print(\"INFO: Voice Embedding saved.\")\n",
    "                \n",
    "        return ids, new_dataset, labels  # [pretrained(get_pth(path), sample_rate) for path in dataset]\n",
    "    \n",
    "    return pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a20f2533794d230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:54.347859Z",
     "start_time": "2024-07-04T17:28:54.344705Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_tensor_label(ids, datas, labels):\n",
    "    if labels:\n",
    "        labels = [torch.tensor([1, 0]) if lb == \"fake\" else torch.tensor([0, 1]) for lb in labels]  # [fake, real]\n",
    "    return ids, datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2a071509e465a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:28:54.555362Z",
     "start_time": "2024-07-04T17:28:54.348865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Voice Embedding extracted.\n",
      "INFO: Voice Embedding extracted.\n"
     ]
    }
   ],
   "source": [
    "train_dataset.transforms(transform=[to_embedding(), to_tensor_label])\n",
    "valid_dataset.transforms(transform=[to_embedding(), to_tensor_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4739bbf8ec8a6c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:00.992302Z",
     "start_time": "2024-07-04T17:29:00.982089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: FAKE tensor([-1.8954e-01, -1.5126e-01, -1.9169e-01, -4.9767e-02, -2.0188e-02,\n",
      "         1.5027e-01, -1.2862e-01, -8.1413e-02,  9.6770e-02, -4.1492e-02,\n",
      "        -1.3344e-01,  2.2745e-01, -9.4607e-02,  2.3628e-02, -9.2205e-02,\n",
      "        -5.7748e-02,  6.2188e-02,  1.4654e-01,  8.1550e-03, -2.6165e-01,\n",
      "         5.2159e-02, -3.8855e-02, -1.5270e-01, -1.3968e-01, -1.9361e-01,\n",
      "         3.3684e-01,  1.1788e-01, -1.7271e-02, -2.3159e-01,  1.0430e-01,\n",
      "         1.0551e-01, -6.3770e-02,  2.0027e-01,  1.1449e-01, -1.3287e-01,\n",
      "        -1.8658e-01,  1.6191e-01, -1.0753e-01, -7.6320e-02,  7.8052e-02,\n",
      "         1.2526e-01, -1.3934e-02, -1.0400e-01, -1.3194e-01,  1.5096e-02,\n",
      "        -2.8469e-02, -8.2221e-02, -9.4001e-02, -2.0316e-01,  1.8863e-01,\n",
      "        -7.0268e-02, -1.8608e-01,  4.7674e-02, -9.2132e-02, -2.4339e-01,\n",
      "        -2.8908e-03, -4.8591e-02, -2.5877e-01,  7.3877e-02, -4.3380e-02,\n",
      "         3.3759e-02, -6.5845e-02, -2.2777e-02, -5.2064e-02,  2.6466e-01,\n",
      "         8.9427e-02, -7.9455e-02,  1.7452e-01, -1.3425e-01, -2.9020e-02,\n",
      "        -1.5479e-01, -2.0307e-01, -4.4050e-02,  4.8118e-02,  4.2728e-02,\n",
      "         2.2446e-01,  1.0268e-01, -1.3212e-01,  2.4964e-01, -6.6730e-02,\n",
      "         1.1519e-02, -4.7426e-02,  1.4212e-01, -4.2034e-01, -1.2370e-01,\n",
      "         9.4701e-02,  6.6795e-02, -2.7367e-01,  2.8834e-02, -1.7856e-02,\n",
      "        -7.1611e-02,  7.5390e-02,  1.0781e-01, -6.4714e-02,  1.0956e-02,\n",
      "        -2.7675e-02,  7.4523e-02, -3.7864e-03, -1.3713e-01, -1.7932e-01,\n",
      "         2.1886e-01,  5.3375e-02,  5.5242e-03,  1.5051e-01,  1.3885e-01,\n",
      "        -3.1971e-02,  4.9733e-02, -2.0234e-01, -1.9718e-01,  3.7351e-02,\n",
      "         1.4374e-02,  9.5399e-02,  3.0633e-02, -2.3125e-01, -1.2895e-01,\n",
      "        -1.7931e-01,  1.3118e-01,  7.1048e-03,  4.0371e-03,  1.3790e-01,\n",
      "         7.4192e-02, -2.3913e-01, -4.0438e-02, -1.0533e-01, -1.6518e-01,\n",
      "        -2.4224e-02, -3.0656e-03,  1.6246e-01, -1.3989e-01, -1.4814e-01,\n",
      "        -1.1734e-02,  1.0850e-01,  8.2474e-02, -2.1804e-01,  8.6089e-02,\n",
      "        -7.3547e-02,  6.7276e-02,  2.0268e-01,  6.2567e-03, -3.1129e-01,\n",
      "        -9.1326e-02, -2.7650e-01, -1.0625e-03,  3.9346e-01, -2.5377e-01,\n",
      "        -7.6900e-02,  5.2424e-02, -3.6978e-01, -1.1253e-01,  3.1326e-01,\n",
      "         2.3898e-02, -1.6402e-01,  2.8746e-02,  4.8696e-02,  1.2569e-01,\n",
      "        -8.8536e-02,  1.0820e-01,  2.6753e-01,  4.1955e-02,  1.0160e-01,\n",
      "        -1.8190e-01, -9.2929e-02,  2.5430e-05,  1.5066e-02, -1.0016e-01,\n",
      "         2.8577e-02,  1.7772e-01,  1.7911e-01,  1.0131e-01,  8.6486e-02,\n",
      "        -1.1321e-01,  5.2792e-02, -1.5718e-01, -6.8814e-03,  5.4382e-03,\n",
      "         5.7395e-02,  2.0787e-02, -6.2848e-02, -5.0350e-02,  3.4838e-01,\n",
      "        -1.3544e-01,  3.7607e-02, -5.4538e-02, -2.2649e-01,  1.4252e-01,\n",
      "         3.5603e-02, -5.1635e-02,  4.8465e-02, -9.1450e-02, -1.4971e-01,\n",
      "        -1.5068e-01,  1.9166e-01, -1.0946e-01, -2.3140e-01, -1.8635e-01,\n",
      "         9.0373e-02,  1.1495e-01, -3.8742e-02,  2.1487e-01, -8.8225e-02,\n",
      "        -2.6981e-01,  1.8713e-01, -1.5273e-01,  2.0055e-01, -5.4212e-02,\n",
      "         6.9522e-02, -1.6101e-01,  1.1321e-01, -1.4165e-01, -6.0233e-02,\n",
      "         2.9212e-02, -1.5117e-01, -5.7281e-02, -3.0739e-01, -1.3560e-01,\n",
      "         2.3074e-01, -8.5934e-02, -1.7675e-02,  1.2702e-01, -6.1722e-02,\n",
      "         7.5711e-02,  5.0511e-02,  5.7220e-02, -1.3352e-01, -8.9721e-02,\n",
      "         1.5265e-04, -4.3355e-02,  1.8958e-01, -2.0969e-01,  2.4037e-01,\n",
      "         2.2973e-01,  1.4501e-01,  2.7194e-01,  1.4876e-01, -4.9135e-02,\n",
      "        -2.6537e-01, -8.9760e-02, -9.3591e-02, -1.1540e-01, -2.1714e-02,\n",
      "         7.1053e-02,  3.7729e-02,  1.2054e-01, -3.3973e-02, -8.1895e-02,\n",
      "         5.8978e-02, -2.0796e-01, -1.4748e-01,  1.9119e-01, -3.1489e-03,\n",
      "        -3.3989e-02, -1.0247e-01,  1.4632e-01,  2.6202e-02,  6.3703e-02,\n",
      "         3.3186e-02])\n",
      "Dataset 1: FAKE tensor([-0.1280, -0.0117,  0.0499,  0.1432, -0.0839, -0.0461,  0.0597, -0.0484,\n",
      "        -0.0440,  0.0561,  0.1248,  0.0525, -0.0055, -0.0767,  0.2200,  0.0345,\n",
      "        -0.0287, -0.0254,  0.0921, -0.0678,  0.0496, -0.0520,  0.1001,  0.1588,\n",
      "        -0.0133, -0.0790, -0.0758, -0.0349,  0.0797, -0.0391,  0.0133,  0.1737,\n",
      "        -0.0878, -0.1414,  0.0413, -0.0631, -0.0398,  0.0206, -0.0665, -0.0073,\n",
      "        -0.0987, -0.0213, -0.0246, -0.0854,  0.0113, -0.0218, -0.0544, -0.1703,\n",
      "        -0.1991,  0.1604, -0.0784,  0.1592,  0.0866,  0.0386, -0.0166,  0.0048,\n",
      "         0.2097, -0.0291,  0.0681,  0.0367,  0.0279, -0.1053,  0.1933,  0.0219,\n",
      "        -0.1701, -0.0917,  0.1156,  0.1536, -0.0068,  0.0977, -0.1523,  0.0574,\n",
      "         0.0370, -0.0493, -0.0155,  0.1300,  0.1074, -0.1507, -0.0259,  0.0599,\n",
      "        -0.0534,  0.2267,  0.0117,  0.2548,  0.1134, -0.1902, -0.0575,  0.1409,\n",
      "        -0.1365, -0.1432, -0.0212, -0.0711, -0.0367,  0.0995,  0.2367, -0.1342,\n",
      "        -0.0959, -0.0883, -0.2670,  0.0184, -0.0159, -0.0538, -0.0177,  0.0189,\n",
      "        -0.0304,  0.0698, -0.0273, -0.0422, -0.0802, -0.0845,  0.0439, -0.0798,\n",
      "        -0.0161, -0.0812, -0.0311,  0.0869,  0.0416,  0.0624,  0.0080, -0.1860,\n",
      "         0.0104,  0.0756,  0.0988,  0.1729, -0.0810, -0.1302, -0.1404, -0.1051,\n",
      "        -0.0094,  0.1996,  0.1536, -0.0023, -0.0184, -0.0214, -0.1480,  0.0364,\n",
      "        -0.2092,  0.1864, -0.0296,  0.0978,  0.1556, -0.0439, -0.0454,  0.0279,\n",
      "         0.0960, -0.0354, -0.0431,  0.0461, -0.0435, -0.1814, -0.0810, -0.0818,\n",
      "        -0.1142, -0.0125, -0.1729,  0.0603, -0.0324,  0.1054,  0.1625, -0.1767,\n",
      "         0.1534,  0.1804, -0.1681, -0.0211, -0.0258,  0.0525, -0.1419,  0.1627,\n",
      "        -0.0333,  0.1630,  0.2240,  0.0263, -0.1366,  0.0450, -0.0231, -0.0191,\n",
      "         0.1462,  0.0948,  0.0690, -0.2244, -0.1224, -0.0919, -0.0484, -0.1332,\n",
      "         0.0387,  0.0653, -0.1754, -0.0345,  0.0883,  0.0988, -0.0728,  0.0626,\n",
      "         0.1231,  0.0841, -0.0504,  0.1508, -0.0070,  0.0071,  0.2182, -0.0615,\n",
      "        -0.0759,  0.0883,  0.1729,  0.0797,  0.1500, -0.1850, -0.0035, -0.1053,\n",
      "        -0.2581,  0.3279,  0.0526,  0.0141, -0.1963,  0.0364, -0.0782,  0.0411,\n",
      "         0.0384,  0.0739,  0.1453, -0.0437, -0.0323,  0.1853,  0.0194, -0.2770,\n",
      "        -0.1624,  0.0460,  0.0070, -0.0026,  0.0717, -0.0129,  0.0874, -0.1579,\n",
      "         0.0148,  0.0454, -0.0914,  0.0252,  0.2251, -0.0907, -0.0495,  0.0640,\n",
      "         0.0220,  0.0542,  0.1720, -0.2105,  0.2048, -0.0617,  0.2734,  0.0913,\n",
      "        -0.0176,  0.0242,  0.1617, -0.1447, -0.0919, -0.1134, -0.0446, -0.0418])\n",
      "Dataset 2: REAL tensor([ 0.0529,  0.1010,  0.1172, -0.0631,  0.0632, -0.1159, -0.0335, -0.0172,\n",
      "         0.1637,  0.0391,  0.0796, -0.0766,  0.0501, -0.0188,  0.0757, -0.1223,\n",
      "        -0.2077,  0.0897,  0.1898,  0.1035, -0.0897, -0.2672,  0.0721,  0.2452,\n",
      "         0.2348,  0.2213, -0.0099,  0.1075,  0.0690, -0.2120,  0.0235,  0.0167,\n",
      "        -0.1776, -0.0702, -0.0985,  0.2446, -0.0017, -0.0182, -0.2660,  0.0888,\n",
      "         0.0833,  0.1668, -0.1144,  0.2026,  0.0562, -0.1339, -0.0277,  0.0940,\n",
      "        -0.0397,  0.0998, -0.0096, -0.0504, -0.1266,  0.0529, -0.0065, -0.0421,\n",
      "        -0.1613, -0.1036,  0.0413, -0.1035,  0.0032,  0.1689,  0.0485,  0.0773,\n",
      "         0.0421,  0.0869,  0.1642, -0.2338, -0.0700, -0.0171,  0.0859,  0.1754,\n",
      "        -0.0498, -0.0659, -0.2626, -0.3053, -0.1249,  0.3110, -0.2882,  0.1583,\n",
      "        -0.1290,  0.1530,  0.1997,  0.0978, -0.0706, -0.3306, -0.1232,  0.1099,\n",
      "        -0.0240,  0.2895, -0.1534,  0.0213,  0.0170, -0.0752,  0.2857, -0.1547,\n",
      "        -0.3040,  0.0814,  0.0462,  0.1052,  0.1497, -0.0568, -0.1524, -0.0669,\n",
      "        -0.0821, -0.0125,  0.2078,  0.0554, -0.1579, -0.0542, -0.1574,  0.0291,\n",
      "         0.0553,  0.1344,  0.1641,  0.1579,  0.1031,  0.1412, -0.0005,  0.1176,\n",
      "         0.1068,  0.0631, -0.1329, -0.0141,  0.0636,  0.0548, -0.1874, -0.0593,\n",
      "         0.0228,  0.0590,  0.1187,  0.1356, -0.0442,  0.0060,  0.1191, -0.0481,\n",
      "        -0.0060, -0.1548, -0.0073, -0.1284,  0.0955, -0.0268, -0.1960, -0.0413,\n",
      "        -0.2996, -0.0690, -0.1802, -0.0116,  0.1737, -0.1187, -0.0989,  0.3258,\n",
      "         0.0862,  0.2322, -0.1078, -0.2218,  0.2485, -0.0009,  0.0191, -0.1251,\n",
      "        -0.0698,  0.2945, -0.1167,  0.0587,  0.0518, -0.0687, -0.1334, -0.3887,\n",
      "        -0.1085,  0.0959, -0.0336,  0.0971,  0.0598, -0.0925,  0.1035, -0.1732,\n",
      "        -0.1699, -0.1430,  0.0708,  0.1265, -0.0209, -0.0285, -0.0116,  0.1048,\n",
      "         0.2179, -0.0606,  0.0327,  0.0549, -0.0709,  0.1233,  0.0513,  0.1000,\n",
      "         0.0544, -0.0087, -0.0886, -0.0331,  0.1621,  0.2634,  0.0480,  0.0492,\n",
      "         0.2967,  0.0538,  0.0062,  0.0198,  0.2476, -0.0665, -0.1171, -0.1010,\n",
      "        -0.0318,  0.1481, -0.2134, -0.0498, -0.1719, -0.0166, -0.0033,  0.0716,\n",
      "        -0.0304, -0.0574, -0.0699,  0.1704,  0.0970,  0.2067,  0.0025,  0.1013,\n",
      "         0.1973, -0.1624,  0.2692,  0.0127,  0.1413, -0.0159, -0.0107, -0.1448,\n",
      "         0.0621, -0.1280,  0.2457,  0.0627,  0.0642, -0.1074,  0.1373, -0.1001,\n",
      "         0.0818, -0.0089, -0.0901, -0.1467,  0.0096, -0.1027, -0.0255, -0.0531,\n",
      "        -0.0621,  0.0303,  0.1169, -0.0655, -0.0792,  0.1999, -0.0488, -0.2642])\n",
      "Dataset 3: FAKE tensor([ 0.1009,  0.0788,  0.0382,  0.0063, -0.2123, -0.0909, -0.0321,  0.1999,\n",
      "        -0.1074, -0.1885,  0.0526, -0.1033, -0.0224,  0.0623,  0.1904, -0.0155,\n",
      "        -0.0132, -0.2278, -0.0443,  0.1717, -0.0372, -0.1662, -0.0838, -0.1172,\n",
      "        -0.0121,  0.0156,  0.1051, -0.0240,  0.0571,  0.1015,  0.0572, -0.2297,\n",
      "         0.1509, -0.1821,  0.0231,  0.1465, -0.1878,  0.1515,  0.0934, -0.1884,\n",
      "         0.0961, -0.0897, -0.0291, -0.0252, -0.0375,  0.1011, -0.0507,  0.0163,\n",
      "         0.1364,  0.0939, -0.0869,  0.0057, -0.0117,  0.0982,  0.0075, -0.0248,\n",
      "         0.0161, -0.0095,  0.0657, -0.0175,  0.0220, -0.0908, -0.0803,  0.0934,\n",
      "         0.0556, -0.0838,  0.1179,  0.0294,  0.1997,  0.2416, -0.1829, -0.0594,\n",
      "         0.1639, -0.0521,  0.0523, -0.1275,  0.1193, -0.1553,  0.0014,  0.0584,\n",
      "        -0.0125,  0.0666,  0.0409, -0.1897,  0.0356,  0.1640,  0.1158, -0.0333,\n",
      "        -0.1521,  0.0268, -0.0489, -0.1760, -0.0626, -0.0742,  0.1164, -0.0207,\n",
      "         0.1887, -0.0149, -0.1887,  0.1601,  0.1047,  0.0936, -0.1638, -0.1710,\n",
      "         0.1676,  0.0819,  0.0581, -0.0148, -0.1514,  0.0610, -0.0707, -0.1021,\n",
      "         0.1187,  0.0641,  0.1433,  0.0744,  0.1462, -0.1074, -0.0884, -0.3389,\n",
      "         0.0395, -0.0262, -0.0582,  0.1081, -0.2012,  0.0045, -0.0894, -0.0941,\n",
      "        -0.0389,  0.1087,  0.0405,  0.1224, -0.1907, -0.1263,  0.0603,  0.0088,\n",
      "         0.1023,  0.0924, -0.0653, -0.0707,  0.0448,  0.0886, -0.0056, -0.0457,\n",
      "        -0.0473, -0.0777,  0.1453,  0.0788, -0.0248, -0.0819, -0.0943,  0.1308,\n",
      "        -0.1563, -0.1390, -0.0568,  0.0067, -0.0484,  0.1390,  0.0564,  0.0048,\n",
      "         0.0141,  0.1514, -0.0568,  0.0283,  0.0292,  0.0545, -0.0871,  0.3761,\n",
      "         0.1373, -0.0155,  0.0117, -0.1022, -0.1304,  0.2656,  0.0112, -0.1654,\n",
      "        -0.2407,  0.0868,  0.0997, -0.0207, -0.0814,  0.0298, -0.0007, -0.1936,\n",
      "         0.0583,  0.1033,  0.0147, -0.0207,  0.1016,  0.0434, -0.0300, -0.1180,\n",
      "        -0.1374,  0.0896, -0.2858,  0.0825,  0.0486, -0.0539, -0.0577, -0.1894,\n",
      "         0.1541,  0.0902, -0.0229, -0.1351,  0.0560, -0.0957, -0.0700, -0.1760,\n",
      "         0.0532,  0.0126,  0.0122,  0.0862, -0.1120, -0.1469, -0.0924,  0.0904,\n",
      "        -0.0188,  0.0777,  0.0391,  0.0197, -0.0031,  0.0678,  0.0754,  0.0241,\n",
      "        -0.0226, -0.0565,  0.0043, -0.1144,  0.2086,  0.1990,  0.3136,  0.0547,\n",
      "         0.1680, -0.1078,  0.1213,  0.0505,  0.1769, -0.1051, -0.1023,  0.2016,\n",
      "         0.0718, -0.0475,  0.1084,  0.1281, -0.0287, -0.0648, -0.0325,  0.0494,\n",
      "         0.1566, -0.0712,  0.1146, -0.3255, -0.2393,  0.0668, -0.0483, -0.2030])\n",
      "Dataset 4: FAKE tensor([-0.1101, -0.0062, -0.0650,  0.0300, -0.1767, -0.1026,  0.0206,  0.0329,\n",
      "        -0.0224,  0.1554,  0.2560,  0.0042, -0.0410,  0.1706, -0.1502,  0.0886,\n",
      "        -0.1520,  0.0050, -0.1093, -0.1057,  0.1080, -0.0007,  0.0094, -0.0183,\n",
      "        -0.2599,  0.0592,  0.1233,  0.0105,  0.0129, -0.0295, -0.0515,  0.0981,\n",
      "        -0.0836,  0.0617, -0.0700, -0.0771, -0.0713, -0.1665, -0.0038,  0.0626,\n",
      "        -0.0311,  0.0804, -0.1489, -0.1697,  0.1625, -0.1202,  0.0369, -0.0038,\n",
      "         0.0409,  0.3061, -0.0049, -0.0074, -0.0681, -0.1979, -0.2345,  0.0849,\n",
      "        -0.0221, -0.0541,  0.0459, -0.0640, -0.1291, -0.0938,  0.0044,  0.1395,\n",
      "        -0.0425, -0.1157,  0.1687, -0.1059, -0.0410,  0.1127, -0.0465,  0.0341,\n",
      "         0.0699, -0.1193, -0.3327,  0.1118, -0.2333, -0.0132,  0.0754, -0.1218,\n",
      "        -0.0702, -0.0075, -0.0364, -0.0207,  0.0672, -0.0161, -0.1859, -0.0547,\n",
      "        -0.0153,  0.0160, -0.1566, -0.1648, -0.0006,  0.0845,  0.0480, -0.0952,\n",
      "        -0.0162, -0.0147,  0.0485, -0.1044, -0.0902,  0.0437,  0.0514,  0.0745,\n",
      "        -0.1332,  0.0531,  0.1154,  0.0614, -0.1901, -0.0737, -0.0476, -0.1373,\n",
      "        -0.0239,  0.0259, -0.1572, -0.0452,  0.0069,  0.2331, -0.0501,  0.0426,\n",
      "         0.2107, -0.0916, -0.0399, -0.1369, -0.0027,  0.1187,  0.0212,  0.0315,\n",
      "         0.1043, -0.0797, -0.0631, -0.0620,  0.0354, -0.0616, -0.1208,  0.0721,\n",
      "         0.1601,  0.1452, -0.0181, -0.0306, -0.0029, -0.0913,  0.0559,  0.2073,\n",
      "         0.0102, -0.0215, -0.1944, -0.1310, -0.0293, -0.0140, -0.1590,  0.0288,\n",
      "         0.0898, -0.1238, -0.0583, -0.1305, -0.0088,  0.0887,  0.1441, -0.0695,\n",
      "        -0.0421,  0.0131, -0.0105,  0.0676,  0.0906, -0.0886, -0.0588,  0.1310,\n",
      "         0.0665,  0.0448,  0.0207, -0.0108, -0.0989,  0.0439, -0.0881, -0.0491,\n",
      "         0.1482,  0.0648, -0.1962,  0.1071, -0.0061,  0.1234,  0.0978,  0.1744,\n",
      "        -0.0081, -0.0251, -0.1676, -0.0440,  0.0041, -0.0591,  0.1222,  0.0836,\n",
      "        -0.1080, -0.1393, -0.0222,  0.0201,  0.0772, -0.1044,  0.0311, -0.0889,\n",
      "        -0.2376, -0.1352, -0.0653,  0.2243, -0.0121,  0.0911, -0.0328, -0.0202,\n",
      "        -0.1326, -0.0229,  0.0326, -0.0224,  0.0663,  0.0278,  0.0497,  0.0807,\n",
      "        -0.1284,  0.0811,  0.1130,  0.2603,  0.1224, -0.0080,  0.1143, -0.0786,\n",
      "        -0.0999,  0.2226, -0.0777,  0.1109, -0.1332,  0.0775,  0.0275, -0.0325,\n",
      "        -0.0885,  0.0175,  0.1117, -0.2182, -0.1160,  0.0913,  0.1262, -0.1835,\n",
      "        -0.0802, -0.0741,  0.1413,  0.0576, -0.0098, -0.0429, -0.0405,  0.0087,\n",
      "        -0.1150,  0.0458,  0.0506,  0.0165, -0.1428, -0.1777,  0.1246,  0.1364])\n"
     ]
    }
   ],
   "source": [
    "for dataset, index in zip(train_dataset, range(5)):\n",
    "    print(f\"Dataset {index}: {'FAKE' if dataset[1][0] == torch.tensor([1]) else 'REAL'}\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef836d6cfa4590f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:01.003483Z",
     "start_time": "2024-07-04T17:29:00.993307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: REAL tensor([ 0.0724, -0.1702,  0.0588, -0.0646, -0.0232,  0.0529,  0.0260, -0.0317,\n",
      "         0.0384,  0.0115,  0.0973, -0.1264, -0.1521,  0.0985, -0.1118, -0.1622,\n",
      "        -0.0021, -0.1185, -0.0803,  0.1039,  0.0132, -0.0267,  0.0188, -0.0915,\n",
      "         0.0882, -0.0660, -0.0154, -0.0652, -0.0497,  0.2414, -0.0733, -0.0123,\n",
      "        -0.1224, -0.0921,  0.1879, -0.0109, -0.0182,  0.0160,  0.1230,  0.0801,\n",
      "         0.0036,  0.0912, -0.0067, -0.0827,  0.0433,  0.0309,  0.0245,  0.0412,\n",
      "        -0.0136, -0.0418, -0.0161,  0.1403, -0.0064, -0.0755, -0.0824,  0.0289,\n",
      "        -0.2233, -0.0769,  0.0059, -0.0435, -0.0988,  0.0930, -0.0204,  0.0090,\n",
      "        -0.0963,  0.1296,  0.1252,  0.0526,  0.0266,  0.0555, -0.0423, -0.0080,\n",
      "         0.0532, -0.0445, -0.1370,  0.0151, -0.0973,  0.0818, -0.0098, -0.0076,\n",
      "        -0.1265,  0.2511, -0.0711,  0.0671,  0.0637,  0.0328,  0.0781, -0.0059,\n",
      "         0.0129,  0.1937, -0.0829, -0.0067, -0.1427,  0.0659,  0.0203,  0.0583,\n",
      "        -0.0811, -0.0144, -0.0876, -0.0831, -0.0259, -0.0326, -0.0198,  0.0258,\n",
      "         0.0390, -0.1034, -0.1177, -0.0060, -0.1182,  0.0382, -0.0528, -0.0560,\n",
      "        -0.0539,  0.0034,  0.0875,  0.0891, -0.0211,  0.0242,  0.1108, -0.0052,\n",
      "        -0.0155,  0.0723, -0.0528, -0.0828,  0.0611, -0.0254,  0.0345, -0.1302,\n",
      "        -0.0046,  0.0551,  0.1061,  0.1925,  0.0693, -0.0259, -0.0394, -0.0929,\n",
      "         0.0023, -0.0781, -0.0515, -0.1501,  0.0863, -0.1015,  0.0009, -0.0913,\n",
      "         0.0875, -0.0492, -0.0294, -0.0513,  0.0981, -0.0094, -0.0093,  0.1357,\n",
      "         0.1152,  0.0504, -0.0655, -0.0521,  0.1467, -0.0521, -0.0098, -0.0185,\n",
      "        -0.1018, -0.0523, -0.0067, -0.1079,  0.0014,  0.0835,  0.0894,  0.0468,\n",
      "         0.0045, -0.0767, -0.0298,  0.0345,  0.0424,  0.0029, -0.0728, -0.0453,\n",
      "         0.0484,  0.0912, -0.2155,  0.0338, -0.0945, -0.0566,  0.0683,  0.0017,\n",
      "        -0.1650,  0.0598,  0.0692, -0.0004, -0.0944,  0.0428, -0.0636, -0.0422,\n",
      "         0.1107,  0.1030,  0.0195,  0.1045, -0.1021,  0.1772,  0.0313,  0.1053,\n",
      "         0.1536,  0.0972,  0.0345,  0.0538,  0.1343,  0.1005, -0.0443,  0.0243,\n",
      "        -0.0131, -0.0296, -0.0189, -0.1698,  0.0106,  0.0630, -0.0095, -0.0392,\n",
      "         0.1002, -0.0058,  0.1169,  0.0519, -0.0772,  0.0274,  0.0281, -0.0426,\n",
      "        -0.0320,  0.0853,  0.1361,  0.0206,  0.0440, -0.1293,  0.0685, -0.0343,\n",
      "         0.0153, -0.0864, -0.1321,  0.0309,  0.1113,  0.0347, -0.0531,  0.0033,\n",
      "        -0.0125,  0.0375,  0.0066, -0.0402,  0.0568,  0.0874,  0.1416, -0.0519,\n",
      "        -0.0553, -0.0560,  0.0353,  0.1680, -0.1025,  0.0158,  0.0441, -0.0535])\n",
      "Dataset 1: FAKE tensor([-0.0350,  0.0629,  0.0757,  0.0708, -0.0283, -0.0261,  0.1233, -0.0020,\n",
      "         0.1085,  0.0197,  0.0341, -0.1025, -0.0008, -0.0108, -0.0237,  0.1623,\n",
      "        -0.0057,  0.2385,  0.1444, -0.0157, -0.0705, -0.2362, -0.0308,  0.2146,\n",
      "        -0.0338, -0.0300, -0.0475,  0.0719,  0.0480, -0.0736, -0.1788, -0.0585,\n",
      "         0.0121, -0.0592, -0.0665,  0.2237,  0.1087, -0.1686,  0.1607,  0.1062,\n",
      "        -0.0270,  0.0237, -0.0947,  0.0897, -0.0383,  0.0459,  0.0732,  0.0751,\n",
      "        -0.2165,  0.1879, -0.2104,  0.1614, -0.0005,  0.0684, -0.0591, -0.1427,\n",
      "         0.1608, -0.1554,  0.1463,  0.0337,  0.1246,  0.0232,  0.1932, -0.0511,\n",
      "        -0.1202,  0.0519,  0.0840,  0.1857, -0.1231, -0.0871,  0.0428,  0.1725,\n",
      "        -0.2659,  0.0137, -0.0101,  0.0990,  0.0927,  0.0281, -0.2497,  0.0377,\n",
      "        -0.0161,  0.2716,  0.0314,  0.2324,  0.0945, -0.1119,  0.0208,  0.1666,\n",
      "        -0.0855, -0.1633, -0.0295, -0.1851,  0.2000,  0.0051,  0.1150, -0.1403,\n",
      "        -0.0746, -0.1444, -0.0600,  0.0949,  0.2250, -0.0674,  0.0092, -0.1404,\n",
      "        -0.1091,  0.1445, -0.0371, -0.0786,  0.1007,  0.1352,  0.1604, -0.0198,\n",
      "         0.1457, -0.1049,  0.1911,  0.0844,  0.2815,  0.0140,  0.0172, -0.1745,\n",
      "         0.0185, -0.2073, -0.0534,  0.0367,  0.0531, -0.0081, -0.0921, -0.2038,\n",
      "         0.1329,  0.1669, -0.1250,  0.1757, -0.0008, -0.0782, -0.0474, -0.0773,\n",
      "        -0.0375,  0.2554,  0.0613,  0.1088, -0.0095,  0.0472,  0.0054,  0.0128,\n",
      "        -0.1015, -0.0368, -0.0216,  0.0408, -0.1529, -0.1194, -0.0939,  0.0502,\n",
      "        -0.0886, -0.0540, -0.1297, -0.0089,  0.0806,  0.0013,  0.2570, -0.0110,\n",
      "         0.0175,  0.1003, -0.0671, -0.0448, -0.0794,  0.0135, -0.1032,  0.0690,\n",
      "         0.0379,  0.0311,  0.0015,  0.0217, -0.0879,  0.0673, -0.1220,  0.1192,\n",
      "         0.0267, -0.0838,  0.0680, -0.0739, -0.0014, -0.0684, -0.0685, -0.0368,\n",
      "         0.1196,  0.0782, -0.0972, -0.1493,  0.1174,  0.0676,  0.0963,  0.0945,\n",
      "        -0.0336,  0.0356, -0.0129,  0.1563,  0.0951,  0.1332, -0.0908,  0.0643,\n",
      "         0.0449,  0.0711,  0.1010, -0.1293,  0.0650, -0.2291, -0.1136, -0.2150,\n",
      "        -0.1716,  0.2944,  0.0083,  0.0205, -0.1455,  0.0006, -0.0068,  0.1046,\n",
      "         0.0067,  0.0065,  0.0548, -0.1984, -0.0134,  0.1971,  0.0204, -0.2845,\n",
      "        -0.0575,  0.1008,  0.0070, -0.0118,  0.0692,  0.0660,  0.1326, -0.0915,\n",
      "        -0.0505,  0.0492,  0.0987,  0.0011,  0.1675, -0.1221,  0.0259, -0.0179,\n",
      "         0.0207, -0.2252, -0.0854, -0.2694,  0.0899, -0.1108,  0.2000,  0.0464,\n",
      "         0.0028,  0.1433,  0.0926, -0.0833,  0.2183, -0.0042, -0.1248, -0.1461])\n",
      "Dataset 2: REAL tensor([ 1.9111e-02,  1.3312e-01,  3.8573e-02, -2.8905e-02, -5.6965e-02,\n",
      "        -2.3382e-01, -1.3960e-01,  8.3127e-02,  3.0660e-01,  2.0407e-01,\n",
      "        -9.5541e-02, -5.5373e-02,  2.0637e-01,  1.2316e-01,  7.1575e-02,\n",
      "        -1.4695e-01,  2.2245e-02,  5.7467e-03,  1.0356e-01,  1.5889e-01,\n",
      "         2.8479e-02,  1.2565e-01, -2.2566e-01,  3.3265e-01,  7.9865e-02,\n",
      "         8.9414e-02, -6.8511e-02, -4.8284e-03, -2.8972e-02,  8.2845e-02,\n",
      "        -6.2680e-02,  1.3517e-01, -2.0292e-01, -8.3389e-02,  3.0241e-02,\n",
      "        -8.5483e-02,  1.2958e-01,  5.2376e-02, -2.5652e-01,  4.7461e-02,\n",
      "        -6.2391e-02, -7.7510e-02, -2.7646e-01,  2.8222e-01, -2.8260e-02,\n",
      "        -1.2766e-01, -2.8795e-02,  1.4737e-01,  2.9080e-02,  1.8953e-01,\n",
      "         6.5597e-02,  1.8270e-01,  2.6169e-02,  1.6693e-02,  9.0737e-03,\n",
      "         1.5894e-01, -1.4346e-01,  5.4748e-03, -9.9500e-02,  6.3479e-02,\n",
      "         2.3904e-02, -1.4001e-02, -2.0097e-01, -4.7555e-02,  1.6258e-01,\n",
      "        -1.0478e-02,  9.1534e-02, -2.7555e-01, -5.2913e-02, -3.1919e-02,\n",
      "         1.1897e-01,  1.8530e-01, -2.1121e-03, -1.5545e-01, -2.5489e-01,\n",
      "        -2.3870e-01, -9.4000e-02,  2.2811e-02, -1.6105e-01,  2.3758e-02,\n",
      "        -1.9589e-01,  1.6880e-02,  1.4275e-01,  2.4349e-01, -1.0089e-01,\n",
      "        -2.9633e-01, -1.3113e-01,  9.2016e-02, -1.8407e-01,  2.1856e-01,\n",
      "        -5.5088e-02,  1.5207e-01,  3.3023e-02, -1.3242e-01,  1.3804e-01,\n",
      "        -3.7022e-02, -2.0519e-01, -6.0571e-02, -8.4466e-02,  6.1360e-02,\n",
      "         1.6308e-01, -1.4937e-01, -1.8470e-01, -1.2407e-02, -2.1212e-01,\n",
      "         4.1766e-02,  1.2381e-01,  1.0556e-03, -6.0012e-02,  9.8097e-03,\n",
      "        -1.6030e-02, -1.2329e-02,  2.0454e-01,  8.6623e-02,  6.1271e-02,\n",
      "         2.2969e-01, -1.0467e-01,  1.7694e-01, -1.1490e-02,  1.4747e-01,\n",
      "         2.0222e-01,  1.8962e-01, -2.2993e-01,  8.8830e-02,  8.4336e-02,\n",
      "         4.8064e-02, -3.1669e-01, -3.9068e-02,  6.5214e-02,  1.9977e-01,\n",
      "         2.0538e-01,  3.2727e-01, -2.4297e-01,  9.2099e-02,  1.0028e-02,\n",
      "        -1.4628e-01, -8.8194e-02, -2.6072e-01,  5.5356e-02, -3.3124e-01,\n",
      "         1.6310e-01,  8.9069e-02,  3.4230e-03, -4.0504e-02, -7.9600e-02,\n",
      "        -1.8349e-01, -8.1136e-03, -1.6262e-01, -4.7968e-02, -5.2527e-03,\n",
      "         6.7010e-02,  2.5721e-01,  4.3776e-02,  2.5058e-01,  5.7037e-02,\n",
      "        -1.3419e-01,  2.4223e-01,  3.4469e-01, -8.1760e-03,  6.1752e-02,\n",
      "         5.9272e-02,  1.9135e-01, -2.0065e-01,  5.0638e-02,  6.1626e-02,\n",
      "        -3.0587e-02, -2.0507e-01, -3.1298e-01, -4.8730e-02,  1.1481e-01,\n",
      "        -9.6539e-02, -2.5188e-02,  6.8097e-02, -3.3501e-04,  6.8976e-02,\n",
      "        -1.4741e-01, -2.3235e-01,  1.7320e-01,  2.2520e-01,  1.7046e-01,\n",
      "        -2.5923e-02,  8.7360e-02, -1.8616e-02,  2.6572e-02,  1.3961e-01,\n",
      "        -2.7392e-02, -1.2362e-01,  8.3111e-02, -1.8566e-01, -5.7172e-02,\n",
      "         6.5132e-02, -7.0199e-02,  2.6173e-02,  2.6877e-02, -9.8836e-02,\n",
      "        -1.3072e-01,  1.1087e-01,  1.6012e-01,  5.9011e-02,  1.7909e-01,\n",
      "         2.5037e-01,  6.2884e-02,  5.4805e-02,  5.1899e-02,  1.3875e-01,\n",
      "        -6.4790e-02, -1.7711e-01,  1.5708e-01,  7.9601e-03,  8.5891e-02,\n",
      "        -4.8715e-02, -1.8169e-01, -1.8481e-01,  2.2086e-01, -5.5944e-02,\n",
      "         2.9229e-02, -1.1908e-03, -4.6688e-02, -4.5490e-02,  2.9805e-01,\n",
      "         1.6189e-01,  1.9735e-01, -6.8565e-02, -8.2029e-02,  5.4623e-02,\n",
      "        -9.6008e-02,  2.6742e-01,  5.1538e-02,  1.6918e-01,  3.1568e-02,\n",
      "        -6.1241e-02,  1.1846e-01,  1.6444e-01, -1.1114e-01,  4.1018e-02,\n",
      "         1.0998e-01,  6.6063e-02, -9.4433e-02,  3.1686e-03, -2.5141e-01,\n",
      "         2.1193e-02,  2.3935e-01,  1.0629e-03, -2.0195e-01,  3.4861e-02,\n",
      "         1.9036e-01,  6.5000e-02,  2.1603e-01,  4.4434e-02, -9.9397e-02,\n",
      "        -6.9433e-02, -1.1033e-01, -2.5223e-01,  3.3541e-02, -6.8448e-02,\n",
      "        -5.6510e-02])\n",
      "Dataset 3: REAL tensor([-3.6520e-02, -1.4758e-02, -1.1824e-01, -5.3948e-02,  4.9864e-02,\n",
      "        -1.2769e-01, -2.9480e-02,  3.7523e-02,  4.7301e-02, -4.6389e-02,\n",
      "        -6.3080e-02,  1.1005e-01,  3.9483e-02, -5.7590e-02, -6.3614e-04,\n",
      "         1.6955e-02,  4.3735e-02,  4.4491e-03,  1.7113e-02, -1.1880e-02,\n",
      "        -1.2467e-01, -1.6733e-01, -2.5162e-02,  6.6958e-02, -1.0272e-02,\n",
      "        -1.1709e-01, -1.1885e-01,  6.5371e-02,  1.7425e-01,  4.7594e-02,\n",
      "         1.6551e-01,  3.7855e-02, -6.0114e-02, -1.3827e-01, -5.6918e-02,\n",
      "         1.0670e-01, -7.9216e-02, -1.0491e-01, -2.3805e-01,  8.3251e-03,\n",
      "         1.3263e-01, -9.5481e-02, -1.7719e-02, -1.6073e-01,  9.8186e-02,\n",
      "         5.8050e-02,  2.7620e-02,  4.0502e-02, -1.5563e-02,  1.2838e-01,\n",
      "         7.8628e-02,  9.2504e-03, -1.6748e-01,  1.1566e-01, -4.8483e-02,\n",
      "         1.2041e-01,  9.5760e-02,  2.2024e-02, -3.4612e-02,  2.7524e-01,\n",
      "         5.1966e-02,  1.5540e-01,  1.6028e-02, -2.8151e-02, -1.4496e-01,\n",
      "         1.5878e-01,  3.0849e-02,  2.7174e-02, -3.9080e-01, -1.5895e-01,\n",
      "         5.5650e-02,  4.8934e-02, -1.6669e-02,  6.3157e-02,  1.3993e-01,\n",
      "         2.2084e-01,  5.4679e-03,  6.5060e-02, -1.3698e-01,  1.1726e-01,\n",
      "        -1.4805e-02, -2.3146e-03, -1.4485e-01, -1.6527e-01,  9.2958e-02,\n",
      "        -1.8054e-01, -9.6869e-02,  1.7680e-01, -1.7547e-01,  1.8482e-02,\n",
      "         1.9951e-01,  1.6712e-01, -1.8699e-01, -1.0674e-01,  9.4269e-03,\n",
      "         2.0161e-02, -2.4928e-02, -4.7836e-02,  3.3999e-02,  1.4159e-02,\n",
      "         1.4313e-01, -1.0054e-01, -1.5549e-01, -2.0638e-01,  3.0042e-02,\n",
      "         4.9767e-02, -1.5346e-01,  9.4757e-02,  1.5062e-03, -1.1494e-01,\n",
      "         1.5666e-02, -1.4513e-02, -8.9630e-02, -4.1079e-02,  9.4702e-02,\n",
      "        -1.3350e-01,  6.9899e-02,  9.0087e-03, -1.1774e-01, -2.3389e-01,\n",
      "        -3.1758e-02,  2.7115e-02,  2.4665e-02,  5.7448e-02,  2.5012e-02,\n",
      "        -7.5525e-02, -3.2905e-02, -1.7550e-01, -1.4585e-01, -6.9645e-02,\n",
      "         1.5432e-01,  2.0123e-01,  2.3330e-03,  1.7362e-01,  2.9546e-02,\n",
      "        -5.1876e-02, -2.6323e-01,  1.1526e-01,  1.8513e-02, -1.4465e-01,\n",
      "        -5.9294e-02, -1.0277e-01,  4.6279e-02,  3.8605e-02, -6.7451e-02,\n",
      "         4.5734e-03,  1.3799e-01,  3.6589e-02,  1.0459e-01, -2.5704e-04,\n",
      "        -1.2343e-01,  1.3886e-01,  4.8290e-02, -9.5882e-02, -1.3953e-01,\n",
      "         1.9862e-01,  5.3242e-02, -1.0555e-01, -2.5655e-03,  6.2776e-02,\n",
      "        -6.2066e-02, -1.1184e-01,  1.1914e-01,  3.1240e-02, -1.1893e-01,\n",
      "         1.2010e-01, -9.5578e-02, -2.1526e-02,  5.0017e-02, -1.3283e-01,\n",
      "         1.0310e-01, -1.4745e-02, -1.2447e-01,  1.5872e-01, -8.6659e-02,\n",
      "         1.1784e-02,  1.3233e-01, -7.2358e-02, -6.0957e-02, -5.1518e-02,\n",
      "        -1.5012e-01,  2.3478e-01, -1.1730e-01,  1.1373e-01, -6.2591e-02,\n",
      "         1.0320e-01, -4.2056e-02, -6.0927e-02, -2.5581e-01, -4.4168e-02,\n",
      "         1.4263e-01,  5.8727e-02, -5.2775e-02, -2.6472e-02,  2.1500e-02,\n",
      "        -1.8913e-02,  1.8462e-02,  5.3025e-02,  2.3097e-02, -2.4917e-02,\n",
      "         2.8895e-01,  8.9501e-02,  5.5921e-03,  2.6293e-02, -5.1645e-02,\n",
      "        -2.4567e-02, -6.9070e-02, -8.1733e-03, -1.8582e-01,  5.3333e-02,\n",
      "         1.3853e-01,  4.6288e-02, -1.8938e-02,  7.0412e-02, -4.7773e-04,\n",
      "         8.6806e-02, -1.1945e-02,  5.3122e-02,  1.4790e-01, -1.8482e-02,\n",
      "        -5.7280e-02, -4.7583e-02,  2.9489e-02,  4.8438e-02,  9.9737e-02,\n",
      "        -4.2256e-02,  8.0422e-03,  4.1946e-02, -2.9371e-01, -1.0545e-01,\n",
      "        -1.3475e-01, -1.3673e-01,  4.2835e-02, -1.6899e-01,  1.1251e-01,\n",
      "         2.3472e-01, -3.8666e-02, -7.9837e-02, -2.4885e-02,  6.9786e-02,\n",
      "         1.1190e-01,  3.6094e-02,  1.8420e-02, -3.7577e-02,  1.1092e-02,\n",
      "        -1.4193e-02,  1.5974e-01, -6.1582e-02,  1.0113e-01, -1.5820e-01,\n",
      "         8.5717e-03,  4.9152e-02,  1.1998e-01,  6.6842e-02, -1.5594e-02,\n",
      "         5.0251e-03])\n",
      "Dataset 4: FAKE tensor([ 0.1188,  0.0384, -0.0630, -0.0038, -0.1759, -0.1651,  0.1295, -0.0069,\n",
      "        -0.1423,  0.0937,  0.2020,  0.0947,  0.0770,  0.0164,  0.0022,  0.1678,\n",
      "        -0.1554, -0.0152, -0.0870, -0.0766,  0.1764,  0.0303, -0.0499, -0.0952,\n",
      "        -0.0954, -0.1683,  0.0484, -0.0629,  0.0380, -0.0718,  0.0121,  0.1227,\n",
      "        -0.1021, -0.0032,  0.0184, -0.0042,  0.1193, -0.2051, -0.0519,  0.0788,\n",
      "         0.0113,  0.1380, -0.0903, -0.1438,  0.0951, -0.1713,  0.0534, -0.1346,\n",
      "         0.0528,  0.2625,  0.0767, -0.0745,  0.0196,  0.0007, -0.1945,  0.0623,\n",
      "         0.0349, -0.1234,  0.0620, -0.1499, -0.0388,  0.0085,  0.0718,  0.0890,\n",
      "        -0.1758,  0.1562,  0.1119, -0.0978,  0.0724,  0.1202, -0.0130, -0.0784,\n",
      "         0.0840,  0.0069, -0.3251,  0.0542, -0.0860, -0.1254, -0.0688,  0.0213,\n",
      "        -0.0758,  0.0161, -0.0230,  0.0405,  0.1030, -0.0766, -0.1500, -0.0732,\n",
      "        -0.0692,  0.1457, -0.0902, -0.1106, -0.0294,  0.0409,  0.0431, -0.0087,\n",
      "        -0.0554, -0.1235, -0.1268,  0.0809, -0.0106, -0.0779, -0.0423,  0.0094,\n",
      "        -0.1149,  0.0234, -0.0004,  0.0943, -0.1497, -0.0892,  0.1056,  0.0126,\n",
      "        -0.0481, -0.1298, -0.1202,  0.0664,  0.0093,  0.1335,  0.0257,  0.0095,\n",
      "         0.1580,  0.0789, -0.1674,  0.0908, -0.1243,  0.1207, -0.0931,  0.1645,\n",
      "         0.0761,  0.0283,  0.0015, -0.1120,  0.0231, -0.0527, -0.0283,  0.0620,\n",
      "        -0.0228,  0.2745, -0.0008, -0.0520, -0.0249,  0.0830, -0.0731,  0.2123,\n",
      "         0.0317, -0.0262, -0.0851, -0.1248, -0.0477, -0.1660,  0.0148,  0.0774,\n",
      "         0.1006, -0.2009,  0.1191,  0.0588, -0.1777,  0.1528, -0.0250, -0.0618,\n",
      "        -0.0344, -0.0801,  0.0408,  0.0596,  0.0724,  0.1082,  0.0301,  0.1466,\n",
      "         0.0836, -0.0702, -0.0170, -0.0689, -0.1174,  0.0047, -0.0372,  0.0753,\n",
      "        -0.0103,  0.1087, -0.2132,  0.0178, -0.0928,  0.0746,  0.0559,  0.0963,\n",
      "         0.0947,  0.0113, -0.0692, -0.0724,  0.1151, -0.0867,  0.1199,  0.1546,\n",
      "        -0.0219,  0.0626,  0.0336, -0.0603, -0.0009,  0.0381, -0.0549,  0.0411,\n",
      "        -0.0111, -0.0392,  0.0461, -0.0537,  0.0930, -0.0512, -0.0184, -0.0630,\n",
      "        -0.0034, -0.0075, -0.0272, -0.0354, -0.0465,  0.0407,  0.1164,  0.1641,\n",
      "        -0.1945, -0.1720,  0.0911,  0.2230,  0.0185, -0.0348,  0.1210, -0.0807,\n",
      "        -0.2079,  0.1507, -0.2248,  0.1791, -0.1592,  0.0223,  0.2119, -0.1401,\n",
      "         0.0027,  0.0650,  0.1497, -0.0826, -0.0599, -0.0956,  0.1902, -0.1589,\n",
      "        -0.1673, -0.0387, -0.0036,  0.0598,  0.1400, -0.0469, -0.1211,  0.0340,\n",
      "        -0.0336,  0.0148,  0.0005,  0.0158, -0.0470, -0.1216, -0.0947,  0.1425])\n"
     ]
    }
   ],
   "source": [
    "for dataset, index in zip(valid_dataset, range(5)):\n",
    "    print(f\"Dataset {index}: {'FAKE' if dataset[1][1] == torch.tensor([0]) else 'REAL'}\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4306aa86c180ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:01.011003Z",
     "start_time": "2024-07-04T17:29:01.004491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: ./test/TEST_00000.ogg\n"
     ]
    }
   ],
   "source": [
    "for dataset, index in zip(test_dataset, range(1)):\n",
    "    print(f\"Dataset {index}:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e0ffa984bb3a",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "    - DataLoader는 구축된 데이터셋에서 배치크기(batch_size)에 맞게 데이터를 추출하고, 필요에 따라 섞거나(shuffle=True) 순서대로 반환(shuffle=False)하는 역할을 합니다.\n",
    "    - 훈련 데이터(train_loader)는 일반적으로 섞어서 모델이 데이터에 덜 편향되게 학습하도록하며,\n",
    "      검증 데이터(val_loader)는 모델 성능 평가를 위해 순서대로 사용하고,\n",
    "      테스트 데이터(test_loader)는 최종적인 추론을 위해 사용합니다.\n",
    "\n",
    "    이렇게 DataLoader를 사용함으로써, 효율적인 데이터 처리와 모델 학습 및 평가가 가능해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dff1c7df-fbe7-4a61-9f66-c55138697eab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.595861Z",
     "start_time": "2024-07-04T17:29:11.500251Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = CONFIG.BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb3435-cdb7-4a31-b7ef-fc16237cfc4a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897283254be72389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.602763Z",
     "start_time": "2024-07-04T17:29:11.597770Z"
    }
   },
   "outputs": [],
   "source": [
    "class VoiceEncoder(nn.Module):\n",
    "    \"\"\" Voice Encoder Model \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        out = self.fc2(h1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba60869-b8a5-46c2-b185-00131161a158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.619134Z",
     "start_time": "2024-07-04T17:29:11.614253Z"
    }
   },
   "outputs": [],
   "source": [
    "class VoiceDiscriminator(nn.Module):\n",
    "    \"\"\" Voice Discriminator Model \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = VoiceEncoder(embedding_dim, hidden_size, latent_size)\n",
    "        self.classifier = nn.Linear(latent_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        out = self.classifier(encoded)\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b803a71eb2decea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.628524Z",
     "start_time": "2024-07-04T17:29:11.620181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_dim': 256, 'hidden_size': 512, 'latent_size': 128}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 파라미터 지정\n",
    "model_params = dict(\n",
    "    embedding_dim=len(train_dataset[0][0]),\n",
    "    hidden_size=512,\n",
    "    latent_size=128\n",
    ")\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8669c3af6468c5d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.644054Z",
     "start_time": "2024-07-04T17:29:11.629533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceDiscriminator(\n",
       "  (encoder): VoiceEncoder(\n",
       "    (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "voice_discrimination_model = VoiceDiscriminator(**model_params)\n",
    "discriminator = voice_discrimination_model\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c3d8aa65edbba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.650280Z",
     "start_time": "2024-07-04T17:29:11.645585Z"
    }
   },
   "outputs": [],
   "source": [
    "# BinaryCrossEntropy\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "958745152fe04f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:29:11.722520Z",
     "start_time": "2024-07-04T17:29:11.716751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=CONFIG.LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c4a8c-0219-46bd-bd46-09d0327fe7eb",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "859d4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28ec4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, valid_loader, device):\n",
    "    \"\"\"\n",
    "    1. 모델 평가 모드 설정\n",
    "        - model.eval()을 호출하여 모델을 평가 모드로 설정합니다.\n",
    "    2. 손실 계산 및 예측 수집\n",
    "        - val_loader에서 배치별로 스펙트로그램과 레이블을 불러와, 모델을 통해 예측을 수행합니다.\n",
    "        - 손실 값을 기록하고 예측된 확률과 실제 레이블을 수집합니다.\n",
    "    3. 성능 평가\n",
    "        - 수집된 모든 예측과 실제 레이블을 사용해 multiLabel_AUC 함수를 호출하여 평균 AUC 점수를 계산합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, _ in valid_loader:\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            probs = model(features)\n",
    "\n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "        # Calculate AUC score\n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "\n",
    "    return _val_loss, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77161583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, valid_loader, device):\n",
    "    \"\"\"\n",
    "    1. 모델 및 Loss 함수 설정\n",
    "        - 모델을 주어진 device(GPU 또는 CPU)로 이동하고, BCELoss(Binary Cross Entropy)를 Loss 함수로 설정합니다.\n",
    "    2. 최적의 모델 저장을 위한 변수 초기화\n",
    "        - 학습 과정동안 가장 좋은 검증 점수를 가진 모델을 추적하기 위해 최적의 점수(best_val_score)와 모델(best_model)을 저장할 변수를 초기화합니다.\n",
    "    3.에포크별 훈련\n",
    "        - 설정된 에포크 수(CONFIG.N_EPOCHS)만큼 반복하여 모델을 훈련합니다.\n",
    "        - train_loader에서 배치별로 스펙트로그램과 레이블을 불러옵니다.\n",
    "        - 모델을 통해 예측을 수행하고 , 손실을 계산합니다.\n",
    "        - loss.backward()를 수행하고, 파라미터를 업데이트 합니다.\n",
    "        - 이후, 훈련 손실을 기록합니다.\n",
    "    4. 검증 및 최적 모델 업데이트\n",
    "        - 각 에포크 후에 검증 함수를 호출해 모델의 성능을 평가하며, 현재 에포크에서 더 나은 검증 점수를 달성하면 이 모델을 최적의 모델로 간주하고 저장합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    train_length = len(train_loader)\n",
    "    valid_length = len(valid_loader)\n",
    "    \n",
    "    epochs = tqdm(range(1, CONFIG.N_EPOCHS+1), desc=\"Running Epochs\")\n",
    "    train_loader = tqdm(train_loader, desc=\"Training\")\n",
    "    valid_loader = tqdm(valid_loader, desc=\"Validation\")\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        train_loader.reset(total=train_length)\n",
    "        valid_loader.reset(total=valid_length)\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels, _ in train_loader:\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        _val_loss, _val_score = validation(model, criterion, valid_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f\"\\rEpoch [{epoch}], Train Loss : [{_train_loss:.6f}] Val Loss : [{_val_loss:.6f}] Val AUC : [{_val_score:.6%}]\", end=\"\\n\" if epoch % 10 == 0 or epoch == CONFIG.N_EPOCHS else \"\")\n",
    "\n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model, best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "394a0109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fbdaafec5a487daa0f5c65c14352cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972c722b6caf449daeaa491593196967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18ef6e0471d40bfaefb034075abd789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.156954] Val Loss : [0.131327] Val AUC : [99.498369%]\n"
     ]
    }
   ],
   "source": [
    "infer_model, ifer_score = train(discriminator, optimizer, train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f49b0f23f044b1f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:33:50.472135Z",
     "start_time": "2024-07-04T17:33:50.463040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models/baseline_wespeaker_model_acc_99.498369%.pt\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(os.path.join(\".\", \"models\")):\n",
    "    os.mkdir(os.path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = os.path.join(\".\", \"models\", f\"baseline_wespeaker_model_acc_{ifer_score:6%}.pt\")\n",
    "torch.save(discriminator.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b0e6-b773-423a-93e4-ce463f4d4d84",
   "metadata": {},
   "source": [
    "### Inference\n",
    "테스트 데이터셋에 대한 추론은 다음 순서로 진행됩니다.\n",
    "\n",
    "1. 모델 및 디바이스 설정\n",
    "    - 모델을 주어진 device(GPU 또는 CPU)로 이동시키고, 평가모드로 전환합니다.\n",
    "2. 예측 수행\n",
    "    - 예측 결과를 저장한 빈 리스트를 초기화하고 test_loader에서 배치별로 데이터를 불러와 예측을 수행합니다.\n",
    "    - 각 배치에 대해 스펙트로그램 데이터를 device로 이동시킵니다.\n",
    "    - 모델 예측 확률(probs)을 계산합니다.\n",
    "    - 예측 확률을 predictions리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "321d8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet152 Model Loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "def inference(model, test_loader, device, to_embedding=get_resnet152()):\n",
    "    get_pth = lambda path: os.path.join(CONFIG.ROOT_FOLDER, *path[1:].split(\"/\"))\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(test_loader):\n",
    "            path = get_pth(features[0])\n",
    "            features = to_embedding(path, CONFIG.SR).to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff139b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff25981153e436589fdbf59aaad7a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_labels = inference(discriminator, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e14a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = []\n",
    "for i in range(50000):\n",
    "    converted.append((predicted_labels[i*2], predicted_labels[i*2+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fae66d-8f54-46d5-9201-0f4b0db76e76",
   "metadata": {},
   "source": [
    "### Submission\n",
    "추론 결과를 제출 양식에 덮어 씌워 CSV 파일로 생성하는 과정은 다음과 같습니다.\n",
    "\n",
    "1. 제출 양식 로드\n",
    "    - pd.read_csv('./sample_submission.csv')를 사용하여 제출을 위한 샘플 형식 파일을 로드합니다.\n",
    "    - 이 파일은 일반적으로 각 테스트 샘플에 대한 ID와 예측해야 하는 필드가 포함된 템플릿 형태를 가지고 있습니다.\n",
    "2. 예측 결과 할당\n",
    "    - submit.iloc[:,1:] = preds 추론함수(inference)에서 반환된 예측결과(preds)를 샘플 제출 파일에 2번째 열부터 할당합니다.\n",
    "3. 제출 파일 저장\n",
    "    - 수정된 제출 파일을 baseline_submit 이란 이름의 CSV 파일로 저장합니다.\n",
    "    - index=False는 파일 저장시 추가적인 index가 발생하지 않도록 설정하여, 제작한 제출 파일과 동일한 형태의 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f8314c4-1dce-4f79-9f3d-77d320a3746e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:36:44.453396Z",
     "start_time": "2024-07-04T17:36:44.277845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.067856</td>\n",
       "      <td>0.926717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.080547</td>\n",
       "      <td>0.913810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.123815</td>\n",
       "      <td>0.869396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.895306</td>\n",
       "      <td>0.111138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.477364</td>\n",
       "      <td>0.515047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      fake      real\n",
       "0  TEST_00000  0.067856  0.926717\n",
       "1  TEST_00001  0.080547  0.913810\n",
       "2  TEST_00002  0.123815  0.869396\n",
       "3  TEST_00003  0.895306  0.111138\n",
       "4  TEST_00004  0.477364  0.515047"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(test_dataset.submission_form_path)\n",
    "submit.iloc[:, 1:] = converted\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e28d71bc-6703-40f7-9716-a0ef897eca83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:36:44.562175Z",
     "start_time": "2024-07-04T17:36:44.454403Z"
    }
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f\"{ifer_score}_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50182ad69c2bf4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732842,
     "sourceId": 8066583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1830.928153,
   "end_time": "2024-04-08T19:22:15.265404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T18:51:44.337251",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a8f214ec354c44b73d439565382278": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06a1ede084cd487ebf3c469be657b53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80013ce73542415e82be091acccb89fe",
        "IPY_MODEL_d280070ca871485fbd2b7d34b1c9fd10",
        "IPY_MODEL_8212bde7695f494cbabea66983e4cf29"
       ],
       "layout": "IPY_MODEL_c4da594b806c4c2bbff6e8cdaf6088eb"
      }
     },
     "37e28ba3d8564da4a3257c3729310584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80013ce73542415e82be091acccb89fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_95e72a34a4374fd5b4b147772085bb7c",
       "placeholder": "​",
       "style": "IPY_MODEL_37e28ba3d8564da4a3257c3729310584",
       "value": "model.safetensors: 100%"
      }
     },
     "8212bde7695f494cbabea66983e4cf29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9e4e04bb60e40d6b46d782f8156d05f",
       "placeholder": "​",
       "style": "IPY_MODEL_b1fa83d0511a4d8a910b8fdb40d32c29",
       "value": " 36.5M/36.5M [00:01&lt;00:00, 41.1MB/s]"
      }
     },
     "95e72a34a4374fd5b4b147772085bb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1fa83d0511a4d8a910b8fdb40d32c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4da594b806c4c2bbff6e8cdaf6088eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d280070ca871485fbd2b7d34b1c9fd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01a8f214ec354c44b73d439565382278",
       "max": 36494688,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcd2393d73d14514851a7d9ef50315fc",
       "value": 36494688
      }
     },
     "d9e4e04bb60e40d6b46d782f8156d05f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcd2393d73d14514851a7d9ef50315fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
