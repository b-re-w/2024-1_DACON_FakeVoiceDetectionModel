{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840dc88f7c14e1fa",
   "metadata": {},
   "source": [
    "# SW중심대학 디지털 경진대회_SW와 생성AI의 만남 : AI부문\n",
    " - 이 AI 경진대회에서는 5초 분량의 오디오 샘플에서 진짜 사람 목소리와 AI가 생성한 가짜 목소리를 정확하게 구분할 수 있는 모델을 개발하는 것이 목표입니다.\n",
    " - 이 작업은 보안, 사기 감지 및 오디오 처리 기술 향상 등 다양한 분야에서 매우 중요합니다."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    import wespeaker\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/wenet-e2e/wespeaker.git"
   ],
   "id": "bd7df6856755ceda"
  },
  {
   "cell_type": "markdown",
   "id": "361d73a1",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-04-08T18:51:47.130888",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.123877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "모델 학습 및 추론에 사용할 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "bbadbd56",
   "metadata": {
    "papermill": {
     "duration": 12.650384,
     "end_time": "2024-04-08T18:51:59.788340",
     "exception": false,
     "start_time": "2024-04-08T18:51:47.137956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.pipelines as pipelines\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c41377d36410af6b",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "4dc82cbe",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f64eb379-e527-46c4-8b12-ead8db628070",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1  # cpu\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0d2de5d",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2024-04-08T18:51:59.803571",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.796330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config\n",
    "- 딥러닝 모델을 학습하기 전에 설정해야하는 다양한 매개변수를 정의하는 설정 클래스입니다.\n",
    "- 클래스를 사용하여 학습에 필요한 설정 값을 미리 지정합니다.\n",
    "\n",
    "##### 오디오 신호\n",
    "- 우리가 듣는 소리는 공기의 압력 변화로, 이것을 디지털 신호로 변환한 것이 오디오 신호입니다.\n",
    "- 이 신호는 시간에 따라 변하는 진폭 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a32fb60",
   "metadata": {
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-04-08T18:51:59.828208",
     "exception": false,
     "start_time": "2024-04-08T18:51:59.811225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class Config:\n",
    "    \"\"\" Configuration Class \"\"\"\n",
    "    SEED = 20240719  # 재현성을 위해 랜덤 시드 고정\n",
    "    NB_NAME = \"transfer_learning\"  # ipython 노트북 이름 지정\n",
    "    ROOT_FOLDER = os.path.join(\".\", \"data\")\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LR = 1e-5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6700bf8e-7f43-4eac-9bea-25eb1d95fb12",
   "metadata": {},
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(Config.SEED)  # Seed 고정"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a682d49",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2024-04-08T18:52:31.507909",
     "exception": false,
     "start_time": "2024-04-08T18:52:31.500578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2459913-1bf6-40b9-b07d-402699590b8f",
   "metadata": {},
   "source": [
    "from torchvision.datasets import utils\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    download_url = \"https://drive.usercontent.google.com/download?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&export=download&authuser=0&confirm=t&uuid=c40c278b-d74b-4b75-bc79-09e8a3ccffa4&at=APZUnTUvIVFVM9gjGNUCmDb4YZCy%3A1719807236671\"\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root='./data', filename=\"download.zip\", md5=None):\n",
    "        cls.download_root = root\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            utils.download_and_extract_archive(cls.download_url, root, root, filename, md5)\n",
    "            print(\"Extraction completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists in {filepath}\")\n",
    "\n",
    "    @property\n",
    "    def get_dataset_path(self):\n",
    "        filename = \"train.csv\" if self.is_train else \"test.csv\"\n",
    "        if self.custom_csv:\n",
    "            filename = self.custom_csv + \".csv\"\n",
    "        return os.path.join(self.download_root, filename)\n",
    "\n",
    "    @property\n",
    "    def submission_form_path(cls):\n",
    "        return os.path.join(cls.download_root, \"sample_submission.csv\")\n",
    "\n",
    "    def __init__(self, root=\"./data\", train=True, split_ratio=1, transform=None, custom_csv=None):\n",
    "        \"\"\"\n",
    "        Voice Dataset for Contrastive Learning\n",
    "        \n",
    "        :param root: The path to the data directory\n",
    "        :param train: is train or test\n",
    "        :param split_ratio: split ratio for train(can be 0.5 or above) and valid(can be lower than 0.5) set\n",
    "        :param transform: data transformer\n",
    "        :param target_transform: label transformer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.download(root)\n",
    "        self.download_root = root\n",
    "        self.is_train = train\n",
    "        self.custom_csv = custom_csv\n",
    "        self.name = (\"train\" if train else \"test\") if not custom_csv else custom_csv\n",
    "\n",
    "        raw_data = self._load_data(self.get_dataset_path, split_ratio if split_ratio >= 0.5 else 1-split_ratio)\n",
    "        if not self.is_train or split_ratio >= 0.5:\n",
    "            self.raw_data, _ = raw_data\n",
    "        else:\n",
    "            _, self.raw_data = raw_data\n",
    "            if \"train\" not in self.name:\n",
    "                print(f\"Warning: The name of dataset should start with 'train' for training set. (current - {self.name})\")\n",
    "            self.name = self.name.replace(\"train\", \"valid\")\n",
    "\n",
    "        self.data0 = self.raw_data['path'].tolist()\n",
    "        self.data1 = self.raw_data['path'].tolist()\n",
    "\n",
    "        if 'label' in self.raw_data.columns:\n",
    "            self.label = [(0, 1) if lb == 'real' else (1, 0) for lb in self.raw_data['label'].tolist()]\n",
    "        else:\n",
    "            if 'real' in self.raw_data.columns and 'fake' in self.raw_data.columns:\n",
    "                f_label = self.raw_data['fake'].tolist()\n",
    "                r_label = self.raw_data['real'].tolist()\n",
    "                self.label = list(zip(f_label, r_label))\n",
    "            else:\n",
    "                self.label = None\n",
    "\n",
    "        self.transforms(transform)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_data(dataset_path, split_ratio=1):\n",
    "        random_state = 1  # fixed random_state\n",
    "\n",
    "        df = pd.read_csv(dataset_path)\n",
    "\n",
    "        if split_ratio == 1 or split_ratio == 0:\n",
    "            return (df, None) if split_ratio == 1 else (None, df)\n",
    "\n",
    "        if 'label' in df.columns:\n",
    "            df1, df2, _, _ = split(df, df['label'], test_size=1-split_ratio, random_state=random_state)\n",
    "        else:\n",
    "            df1, df2 = split(df, test_size=1-split_ratio, random_state=random_state)\n",
    "        return df1, df2\n",
    "\n",
    "    def transforms(self, transform=None):\n",
    "        if transform is not None:\n",
    "            if not isinstance(transform, list) and not isinstance(transform, tuple):\n",
    "                transform = [transform]\n",
    "            for t in transform:\n",
    "                self.data0, self.data1, self.label = t(self.data0, self.data1, self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.data0[index], self.data1[index], self.label[index]\n",
    "        return self.data0[index], self.data1[index]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8967c36f6abc224b",
   "metadata": {},
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "train_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=split_ratio)\n",
    "valid_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=True, split_ratio=1-split_ratio)\n",
    "unlabeled_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=False, custom_csv=\"unlabeled_data\")\n",
    "test_dataset = VoiceDataset(root=Config.ROOT_FOLDER, train=False)\n",
    "\n",
    "print(f\"Loaded Dataset - train({len(train_dataset)}), valid({len(valid_dataset)}), unlabeled({len(unlabeled_dataset)}) test({len(test_dataset)})\")\n",
    "print(\"Query Dataset for checking:\", train_dataset[0])\n",
    "train_dataset.raw_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7888fecea819346",
   "metadata": {},
   "source": [
    "#### Data Transformation\n",
    "By using \n",
    "[TorchAudio Models](https://pytorch.org/audio/stable/models.html) |\n",
    "[TorchAudio Pretrained Models](https://pytorch.org/audio/stable/pipelines.html#module-torchaudio.pipelines)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AudioPipelines:\n",
    "    \"\"\" Audio Pipelines - Pretrained Embeddings \"\"\"\n",
    "    \n",
    "    wav2vec_bundle = pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "    resnet_bundle = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    device_setting = (device, DEVICE_NUM)\n",
    "    \n",
    "    def __init__(self, audio_cache_dir=\"audio_cache\", nb_name=Config.NB_NAME):\n",
    "        self.audio_cache_dir = audio_cache_dir\n",
    "        if not os.path.isdir(audio_cache_dir):\n",
    "            os.mkdir(audio_cache_dir)\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name))\n",
    "        self.wav2vec = self.get_wav2vec(audio_cache_dir=audio_cache_dir)\n",
    "        self.resnet = self.get_resnet(audio_cache_dir=audio_cache_dir)\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name, self.wav2vec.name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name, self.wav2vec.name))\n",
    "        if not os.path.isdir(os.path.join(audio_cache_dir, nb_name, self.resnet.name)):\n",
    "            os.mkdir(os.path.join(audio_cache_dir, nb_name, self.resnet.name))\n",
    "\n",
    "    @classmethod\n",
    "    def get_wav2vec(cls, audio_cache_dir=\".\"):\n",
    "        sr = cls.wav2vec_bundle.sample_rate  # Wav2Vec2 Model uses sample rate 16kHz\n",
    "        wav2vec_model = cls.wav2vec_bundle.get_model()\n",
    "        wav2vec_model.to(cls.device_setting[0])\n",
    "        print(f\"INFO: Wav2Vec Model Loaded on {cls.device_setting[0]}:{cls.device_setting[1]}. - Bundle: {cls.wav2vec_bundle}\")\n",
    "        wav2vec_model.eval()\n",
    "        \n",
    "        def wav2vec(path):\n",
    "            waveform, sample_rate = torchaudio.load(path, normalize=True)\n",
    "            if sample_rate != sr:\n",
    "                resampler = T.Resample(sample_rate, sr)\n",
    "                waveform = resampler(waveform)\n",
    "            with torch.no_grad():\n",
    "                embedding, _ = wav2vec(waveform.to(cls.device_setting[0]))\n",
    "            return embedding\n",
    "        \n",
    "        wav2vec.__dict__['name'] = str(cls.wav2vec_bundle._path).split(\".\")[0]\n",
    "        wav2vec.__dict__['cache'] = audio_cache_dir\n",
    "        return wav2vec\n",
    "\n",
    "    @classmethod\n",
    "    def get_resnet(cls, audio_cache_dir=\".\"):\n",
    "        model_id = cls.resnet_bundle\n",
    "        model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "    \n",
    "        root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "        if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "            os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "        if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "            os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "    \n",
    "        resnet_model = wespeaker.load_model_local(root_dir)\n",
    "        resnet_model.set_gpu(-1 if cls.device_setting[0] == torch.device('cpu') else cls.device_setting[1])\n",
    "        print(f\"INFO: ResNet Model Loaded on {resnet_model.device}\")\n",
    "    \n",
    "        def resnet(path):\n",
    "            return resnet_model.extract_embedding(path)\n",
    "\n",
    "        resnet.__dict__['name'] = model_name\n",
    "        resnet.__dict__['cache'] = audio_cache_dir\n",
    "        return resnet"
   ],
   "id": "49b480f1089e2806"
  },
  {
   "cell_type": "code",
   "id": "5f6485434baef869",
   "metadata": {},
   "source": [
    "def to_embedding(dataset_name, pretrained, d_idx):\n",
    "    convert_path = lambda path: os.path.join(Config.ROOT_FOLDER, *path.replace(\"./\", \"\").split(\"/\"))\n",
    "    embedding_path = os.path.join(pretrained.cache, Config.NB_NAME, pretrained.name, f\"{dataset_name}.embedding\")\n",
    "\n",
    "    def convert(*args):\n",
    "        *datas_list, labels = args\n",
    "        if not os.path.isfile(embedding_path):\n",
    "            new_datas = [pretrained(convert_path(path)) for path in tqdm(datas_list[d_idx], desc=f\"Convert {dataset_name} dataset with {pretrained.name}\")]\n",
    "            torch.save(new_datas, embedding_path)\n",
    "            print(\"INFO: Voice Embedding saved.\")\n",
    "        else:\n",
    "            new_datas = torch.load(embedding_path)\n",
    "            print(f\"INFO: Pretrained {pretrained.name} embedding for {dataset_name} dataset is loaded.)\")\n",
    "        datas_list[d_idx] = new_datas\n",
    "        return *datas_list, labels\n",
    "    return convert"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "WV_DIM_SIZE = 1024\n",
    "\n",
    "def flatten_tensor(adaptive_pool=nn.AdaptiveAvgPool1d(WV_DIM_SIZE), d_idx=1):\n",
    "    def flatten(*args):\n",
    "        *datas_list, labels = args\n",
    "        datas_list[d_idx] = [adaptive_pool(torch.flatten(t).unsqueeze(0)).squeeze(0) for t in datas_list[d_idx]]\n",
    "        return *datas_list, labels\n",
    "    return flatten"
   ],
   "id": "6d04b035add7ee6a"
  },
  {
   "cell_type": "code",
   "id": "6a2a071509e465a3",
   "metadata": {},
   "source": [
    "to_tensor = lambda *args: (*args[:-1], list(map(torch.tensor, args[-1])))  # label to tensor\n",
    "\n",
    "apl = AudioPipelines()  # Create Audio Pipeline for converting audio to embeddings\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset]:\n",
    "    dataset.transforms(transform=[\n",
    "        to_embedding(dataset.name, apl.resnet, d_idx=0),\n",
    "        to_embedding(dataset.name, apl.wav2vec, d_idx=1),\n",
    "        flatten_tensor(d_idx=1),\n",
    "        to_tensor\n",
    "    ])\n",
    "\n",
    "for dataset in [unlabeled_dataset, test_dataset]:\n",
    "    dataset.transforms(transform=[\n",
    "        to_embedding(dataset.name, apl.resnet, d_idx=0),\n",
    "        to_embedding(dataset.name, apl.wav2vec, d_idx=1),\n",
    "        flatten_tensor(d_idx=1)\n",
    "    ])\n",
    "\n",
    "del apl  # release memory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4739bbf8ec8a6c50",
   "metadata": {},
   "source": [
    "for (data, label), i in zip(train_dataset, range(5)):\n",
    "    print(f\"Train Dataset {i}: {label}\", data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ef836d6cfa4590f",
   "metadata": {},
   "source": [
    "for (data, label), i in zip(valid_dataset, range(5)):\n",
    "    print(f\"Valid Dataset {i}: {label}\", data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4306aa86c180ad8",
   "metadata": {},
   "source": [
    "for dataset, i in zip(unlabeled_dataset, range(5)):\n",
    "    print(f\"UnLabeled Dataset {i}:\", dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset, i in zip(test_dataset, range(5)):\n",
    "    print(f\"Test Dataset {i}:\", dataset)"
   ],
   "id": "e03074038fd8835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "    - DataLoader는 구축된 데이터셋에서 배치크기(batch_size)에 맞게 데이터를 추출하고, 필요에 따라 섞거나(shuffle=True) 순서대로 반환(shuffle=False)하는 역할을 합니다.\n",
    "    - 훈련 데이터(train_loader)는 일반적으로 섞어서 모델이 데이터에 덜 편향되게 학습하도록하며,\n",
    "      검증 데이터(val_loader)는 모델 성능 평가를 위해 순서대로 사용하고,\n",
    "      테스트 데이터(test_loader)는 최종적인 추론을 위해 사용합니다.\n",
    "\n",
    "    이렇게 DataLoader를 사용함으로써, 효율적인 데이터 처리와 모델 학습 및 평가가 가능해집니다."
   ],
   "id": "994c359eed10e6ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = Config.BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "73f32489efd727c1"
  },
  {
   "cell_type": "markdown",
   "id": "25bf2531542bfda5",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Discriminator A: fake(0) ~ real(1)\n",
    "\n",
    "Detector B: no voice(0) ~ voice(1)\n",
    "\n",
    "---\n",
    "\n",
    "*power = lambda a: 1 - abs(0.5-a) / 0.5\n",
    "\n",
    "(give more weights to detector b)\n",
    "\n",
    "---\n",
    "\n",
    "| Case | Model Output | Label | Calculation |\n",
    "|---|---|---|---|\n",
    "| fake 1 real 1 | A: 0.5, B: 1.0 | 1, 1 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n",
    "| fake 2 real 0 | A: 0.0, B: 1.0 | 1, 0 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n",
    "| fake 0 real 2 | A: 1.0, B: 1.0 | 0, 1 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n",
    "| fake 1 real 0 | A: 0.0, B: 1.0 | 1, 0 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n",
    "| fake 0 real 1 | A: 1.0, B: 1.0 | 0, 1 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n",
    "| fake 0 real 0 | A: 0.5, B: 0.0 | 0, 0 | (1-A) * (1-power(A)) + B * power(A), A * (1-power(A)) + B * power(A) |\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VAD(nn.Module):\n",
    "    power = lambda a: 1 - abs(0.5-a) / 0.5\n",
    "    \n",
    "    def forward(self, is_real, is_voice):\n",
    "        power = self.power(is_real)\n",
    "        return torch.tensor([(1-is_real) * (1-power) + is_voice * power, is_real * (1-power) + is_voice * power])"
   ],
   "id": "b1916d34de76acd9"
  },
  {
   "cell_type": "code",
   "id": "de7d8d76",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multi_label_auc(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Test",
   "id": "f33c4bc7cda01359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size//2, latent_size)\n",
    "        )\n",
    "        self.fc = nn.Linear(latent_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        out = self.classifier(encoded)\n",
    "        return F.sigmoid(out)"
   ],
   "id": "24b9d559f4ecbe4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FakeVoiceDetectionModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.discriminator = FeatureExtractor(embedding_size[0], hidden_size, latent_size)\n",
    "        self.detector = FeatureExtractor(embedding_size[1], hidden_size, latent_size)\n",
    "        self.vad = VAD()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        reals = self.discriminator(x[0])\n",
    "        voices = self.detector(x[1])\n",
    "        return reals, voices, self.vad(reals, voices)"
   ],
   "id": "825ab1b9d9949940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 파라미터 지정\n",
    "model_params = dict(\n",
    "    embedding_size=[len(d) for d in train_dataset[0][0]],\n",
    "    hidden_size=1024,\n",
    "    latent_size=128\n",
    ")\n",
    "model_params"
   ],
   "id": "e38e5e60ef1fe01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "model = FakeVoiceDetectionModel(**model_params)\n",
    "model.to(device)"
   ],
   "id": "f3d59ca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BinaryCrossEntropy\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=Config.LR)"
   ],
   "id": "6561d47a"
  },
  {
   "cell_type": "code",
   "id": "1c91808b",
   "metadata": {},
   "source": [
    "# Set Epoch Count\n",
    "num_epochs = 100\n",
    "log_interval = 5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9de81161",
   "metadata": {},
   "source": [
    "last_val_score = 0\n",
    "train_len, valid_len = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Running Epochs\")\n",
    "with tqdm(total=train_len, desc=\"Training\") as train_progress, tqdm(total=valid_len, desc=\"Validation\") as valid_progress:\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_len)\n",
    "        valid_progress.reset(total=valid_len)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for i, inputs in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            *features, labels = (data.float().to(device) for data in inputs)\n",
    "            reals, voices, outputs = model(*features)\n",
    "            real_labels = torch.transpose(torch.argmax(labels, dim=1).unsqueeze(0), 1, 0).float()  # TODO: 0.5인 경우\n",
    "            voice_labels = torch.transpose((labels.sum(dim=1) > 0).unsqueeze(0), 1, 0).float()\n",
    "\n",
    "            loss = (criterion(reals, real_labels) + criterion(voices, voice_labels) + criterion(outputs, labels)) / 3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_loss, val_labels, val_outputs = 0, [], []\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in valid_loader:\n",
    "                *features, labels = (data.float().to(device) for data in inputs)\n",
    "                _, _, predicted = model(*features)\n",
    "\n",
    "                val_loss += criterion(predicted, labels).item() / valid_len\n",
    "                val_labels.append(labels.cpu().numpy())\n",
    "                val_outputs.append(predicted.cpu().numpy())\n",
    "\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        last_val_score = multi_label_auc(np.concatenate(val_labels, axis=0), np.concatenate(val_outputs, axis=0))\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{train_len}/{train_len}], Loss: {loss.item():.6f}, \"\n",
    "            + f\"Valid Acc: {last_val_score:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Encoder-Decoder Feature Extractor",
   "id": "d336396a46711fde"
  },
  {
   "cell_type": "code",
   "id": "3ca73bb2",
   "metadata": {},
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\" Feature Extractor Model \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embedding_size//2, embedding_size//4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embedding_size//4, latent_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, embedding_size//4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embedding_size//4, embedding_size//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embedding_size//2, embedding_size)\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.tanh(self.encoder(x))\n",
    "        restored = self.decoder(latent)\n",
    "        return latent, restored"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "710f8298",
   "metadata": {},
   "source": [
    "class RecurrentFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\" Recurrent-Feature Extractor Model \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size=1024, latent_size=32, embedding_dim=29):\n",
    "        super().__init__(embedding_size, latent_size)\n",
    "        self.resizer = nn.RNN(embedding_dim, embedding_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sequences, sequence_lengths = x\n",
    "        sequences_packed = pack_padded_sequence(sequences, sequence_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        resized, _ = self.resizer(sequences_packed)\n",
    "        resized, _ = pad_packed_sequence(resized, batch_first=True)\n",
    "        resized = torch.tanh(resized[:, -1, :])\n",
    "        \n",
    "        latent = self.encoder(resized)\n",
    "        restored = self.decoder(latent)\n",
    "        return latent, restored"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4f6541b",
   "metadata": {},
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return F.sigmoid(self.fc3(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "effb3435-cdb7-4a31-b7ef-fc16237cfc4a",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Real-Fake Voice Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cba7b39cc2a3da8",
   "metadata": {},
   "source": [
    "class RFVoiceDiscriminator(nn.Module):\n",
    "    \"\"\" Real-Fake Voice Discriminator Model \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.feature = FeatureExtractor(embedding_size, latent_size)\n",
    "        self.fc = Classifier(embedding_size, latent_size*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent, restored = self.feature(x)\n",
    "        return self.fc(x), self.fc(restored)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3799d0b64b30a54b",
   "metadata": {},
   "source": [
    "rf_disc = RFVoiceDiscriminator(embedding_size=EMBEDDING_SIZE, latent_size=32)\n",
    "rf_disc.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b33cc766e1b5c7b2",
   "metadata": {},
   "source": [
    "class RFVoiceDiscriminatorConfig:\n",
    "    num_epochs = 50\n",
    "    log_interval = 5\n",
    "    \n",
    "    # BinaryCrossEntropy\n",
    "    criterion_b = nn.BCELoss().to(device)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    criterion_m = nn.MSELoss().to(device)\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(params=rf_disc.parameters(), lr=Config.LR/10)\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = copy.deepcopy(train_dataset)\n",
    "    train_dataset.data.extend(train_noise_type1.data)\n",
    "    train_dataset.label.extend(train_noise_type1.label)\n",
    "    \n",
    "    valid_dataset = copy.deepcopy(valid_dataset)\n",
    "    valid_dataset.data.extend(valid_noise_type1.data)\n",
    "    valid_dataset.label.extend(valid_noise_type1.label)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)#, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)#, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b3ca46eb449317",
   "metadata": {},
   "source": [
    "rf_disc_config = RFVoiceDiscriminatorConfig\n",
    "\n",
    "num_epochs = rf_disc_config.num_epochs\n",
    "log_interval = rf_disc_config.log_interval\n",
    "train_len, valid_len = map(len, (rf_disc_config.train_loader, rf_disc_config.valid_loader))\n",
    "valid_dataset_len = len(rf_disc_config.valid_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f1a82a9138db055",
   "metadata": {},
   "source": [
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Running Epochs\")\n",
    "with tqdm(total=train_len, desc=\"Training\") as train_progress, tqdm(total=valid_len, desc=\"Validation\") as valid_progress:\n",
    "    optimizer = rf_disc_config.optimizer\n",
    "    criterion_b = rf_disc_config.criterion_b\n",
    "    criterion_m = rf_disc_config.criterion_m\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_len)\n",
    "        valid_progress.reset(total=valid_len)\n",
    "\n",
    "        # Train\n",
    "        rf_disc.train()\n",
    "        for i, (features, labels) in enumerate(rf_disc_config.train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.to(device)\n",
    "            preds1, preds2 = rf_disc(features)\n",
    "            labels = torch.transpose(torch.argmax(labels, dim=1).unsqueeze(0), 1, 0).float().to(device)\n",
    "\n",
    "            loss = (criterion_b(preds1, labels) + criterion_b(preds2, labels) + criterion_b(preds1, preds2)) / 3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc1, val_acc2, val_loss = 0, 0, 0\n",
    "\n",
    "        # Validation\n",
    "        rf_disc.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in rf_disc_config.valid_loader:\n",
    "                preds1, preds2 = rf_disc(features)\n",
    "                labels = torch.transpose(torch.argmax(labels, dim=1).unsqueeze(0), 1, 0).float().to(device)\n",
    "\n",
    "                val_loss += (criterion_b(preds1, labels).item() + criterion_b(preds2, labels).item()) / (2 * valid_len)\n",
    "                val_acc1 += ((preds1 >= 0.5).float() == labels).sum() / valid_dataset_len\n",
    "                val_acc2 += ((preds2 >= 0.5).float() == labels).sum() / valid_dataset_len\n",
    "\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{train_len}/{train_len}], Loss: {loss.item():.6f}, Valid Acc1: {val_acc1:.6%}, \"\n",
    "            + f\"Valid Acc2: {val_acc2:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29a04c39f9556c46",
   "metadata": {},
   "source": [
    "### 3. Voice Presence Detector"
   ]
  },
  {
   "cell_type": "code",
   "id": "62cd6a10ad3d7db5",
   "metadata": {},
   "source": [
    "class VoiceDetector(nn.Module):\n",
    "    \"\"\" Voice presence detection module \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.feature = FeatureExtractor(embedding_size=embedding_size, latent_size=latent_size)\n",
    "        self.fc1 = Classifier(latent_size, latent_size//2, 1)\n",
    "        self.fc2 = Classifier(embedding_size, latent_size*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent, restored = self.feature(x)\n",
    "        return restored, self.fc1(latent), self.fc2(restored)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7b681c2e44049be",
   "metadata": {},
   "source": [
    "detector = VoiceDetector(embedding_size=EMBEDDING_SIZE, latent_size=32)\n",
    "detector.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c29a70ee58c0c86a",
   "metadata": {},
   "source": [
    "class VoiceDetectorConfig:\n",
    "    num_epochs = 100\n",
    "    log_interval = 10\n",
    "\n",
    "    # BinaryCrossEntropy\n",
    "    criterion_b = nn.BCELoss().to(device)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    criterion_m = nn.MSELoss().to(device)\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(params=rf_disc.parameters(), lr=Config.LR)\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = train_augmented\n",
    "    self_train_dataset = unlabeled_dataset\n",
    "    valid_dataset = valid_augmented\n",
    "    \n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ccb041a69677ad4",
   "metadata": {},
   "source": [
    "vdet_config = VoiceDetectorConfig\n",
    "\n",
    "num_epochs = vdet_config.num_epochs\n",
    "log_interval = vdet_config.log_interval\n",
    "train_len, valid_len = map(len, (vdet_config.train_loader, vdet_config.valid_loader))\n",
    "valid_dataset_len = len(vdet_config.valid_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "723b975aa9d65ebd",
   "metadata": {},
   "source": [
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Running Epochs\")\n",
    "with tqdm(total=train_len, desc=\"Training\") as train_progress, tqdm(total=valid_len, desc=\"Validation\") as valid_progress:\n",
    "    optimizer = vdet_config.optimizer\n",
    "    criterion_b = vdet_config.criterion_b\n",
    "    criterion_m = vdet_config.criterion_m\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_len)\n",
    "        valid_progress.reset(total=valid_len)\n",
    "\n",
    "        # Train\n",
    "        detector.train()\n",
    "        for i, (features, _) in enumerate(vdet_config.train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.to(device)\n",
    "            restoreds, preds1, preds2 = detector(features)\n",
    "            labels = torch.softmax(features, dim=1)\n",
    "            labels = torch.transpose((torch.max(labels, dim=1)[0] >= 0.5).unsqueeze(0), 1, 0).float()\n",
    "            \n",
    "            loss = (criterion_b(preds1, labels) + criterion_b(preds2, labels) + criterion_b(preds1, preds2)*4) / 6\n",
    "\n",
    "            #loss = (criterion_m(features, restoreds)/20 + criterion_b(preds1, labels) + criterion_b(preds2, labels)) / 3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc1, val_acc2, val_loss = 0, 0, 0\n",
    "\n",
    "        # Validation\n",
    "        detector.eval()\n",
    "        with torch.no_grad():\n",
    "            for (features, labels) in vdet_config.valid_loader:\n",
    "                _, preds1, preds2 = detector(features)\n",
    "                labels = torch.transpose((labels.to(device).sum(dim=1) > 0).unsqueeze(0), 1, 0).float().to(device)\n",
    "\n",
    "                val_loss += (criterion_b(preds1, labels).item() + criterion_b(preds2, labels).item()) / (2 * valid_len)\n",
    "                val_acc1 += ((preds1 >= 0.5).float() == labels).sum() / valid_dataset_len\n",
    "                val_acc2 += ((preds2 >= 0.5).float() == labels).sum() / valid_dataset_len\n",
    "\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{train_len}/{train_len}], Loss: {loss.item():.6f}, Valid Acc1: {val_acc1:.6%}, \"\n",
    "            + f\"Valid Acc2: {val_acc2:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28c4a8c-0219-46bd-bd46-09d0327fe7eb",
   "metadata": {},
   "source": [
    "### 4. Total FakeVoiceDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "id": "859d4608",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multi_label_auc(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a73b49b86a7216f6",
   "metadata": {},
   "source": [
    "class FakeVoiceDetectionModel(nn.Module):\n",
    "    \"\"\" Fake Voice Detection Model \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.latent_size = rf_disc.latent_size + detector.latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder1 = rf_disc.feature.encoder\n",
    "        self.encoder2 = detector.feature.encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent1, latent2 = self.encoder1(x), self.encoder2(x)\n",
    "        latent = latent1.cat(latent2, dim=1)\n",
    "        return self.classifier(latent)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a30d7dd37483f2",
   "metadata": {},
   "source": [
    "model = FakeVoiceDetectionModel(hidden_size=256)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c26cc03f6f361d9",
   "metadata": {},
   "source": [
    "class FakeVoiceDetectionModelConfig:\n",
    "    num_epochs = 5\n",
    "    log_interval = 1\n",
    "\n",
    "    # BinaryCrossEntropy\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(params=rf_disc.parameters(), lr=Config.LR)\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = copy.deepcopy(train_dataset)\n",
    "    train_dataset.data.extend(train_noise_type1.data)\n",
    "    train_dataset.data.extend(train_augmented.data)\n",
    "    self_train_dataset = unlabeled_dataset\n",
    "    valid_dataset = copy.deepcopy(valid_dataset)\n",
    "    valid_dataset.data.extend(valid_noise_type1.data)\n",
    "    valid_dataset.data.extend(valid_augmented.data)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    self_train_loader = DataLoader(self_train_dataset, batch_size=1, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_config = FakeVoiceDetectionModelConfig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77161583",
   "metadata": {},
   "source": [
    "last_val_score = 0\n",
    "self_labels = []  # do not shuffle data\n",
    "num_epochs = model_config.num_epochs\n",
    "log_interval = model_config.log_interval\n",
    "train_len, self_len, valid_len = map(len, (model_config.train_loader, model_config.self_train_loader, model_config.valid_loader))\n",
    "\n",
    "epochs = tqdm(range(1, num_epochs+1), desc=\"Total Model Running Epochs\")\n",
    "with (\n",
    "    tqdm(total=train_len, desc=\"Self-Supervised Learning\") as self_progress,\n",
    "    tqdm(total=train_len, desc=\"Supervised Learning\") as train_progress,\n",
    "    tqdm(total=valid_len, desc=\"Validation\") as valid_progress\n",
    "    ):\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_len)\n",
    "        self_progress.reset(total=self_len)\n",
    "        valid_progress.reset(total=valid_len)\n",
    "\n",
    "        # Self-Supervised Learning\n",
    "        model.train()\n",
    "        for i, inputs in enumerate(zip(model_config.self_train_loader, self_labels)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, labels = (data.float().to(device) for data in inputs)\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = model_config.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        # Supervised Learning\n",
    "        model.train()\n",
    "        for i, inputs in enumerate(model_config.train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, labels = (data.float().to(device) for data in inputs)\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = model_config.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch}/{num_epochs}], Step [{i+1}/{train_len}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        # Self-Supervised Labeling\n",
    "        self_loss, _self_labels, self_outputs = 0, [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in zip(model_config.self_train_loader, self_labels):\n",
    "                features, labels = (data.float().to(device) for data in inputs)\n",
    "                predicted = model(features)\n",
    "\n",
    "                self_loss += model_config.criterion(predicted, labels).item() / self_len\n",
    "                _self_labels.append(labels.cpu().numpy())\n",
    "                self_outputs.append(predicted.cpu().numpy())\n",
    "\n",
    "                self_progress.update(1)\n",
    "\n",
    "        self_score = multi_label_auc(np.concatenate(_self_labels, axis=0), np.concatenate(self_outputs, axis=0))\n",
    "        self_labels = (_self_labels >= 0.5).float()\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Loss: {loss.item():.6f}, Self Acc: {self_score:.6%}, Self Loss: {self_loss:.6f}\", end=\"\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_labels, val_outputs = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs in model_config.valid_loader:\n",
    "                features, labels = (data.float().to(device) for data in inputs)\n",
    "                predicted = model(features)\n",
    "\n",
    "                val_loss += model_config.criterion(predicted, labels).item() / valid_len\n",
    "                val_labels.append(labels.cpu().numpy())\n",
    "                val_outputs.append(predicted.cpu().numpy())\n",
    "\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        last_val_score = multi_label_auc(np.concatenate(val_labels, axis=0), np.concatenate(val_outputs, axis=0))  # Calculate AUC score\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch}/{num_epochs}], Loss: {loss.item():.6f}, Self Acc: {self_score:.6%}, Self Loss: {self_loss:.6f}, \"\n",
    "            + f\"Valid Acc: {last_val_score:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if epoch % log_interval == 0 or epoch == num_epochs else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e034ea7ae3f5d1a",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "id": "f49b0f23f044b1f2",
   "metadata": {},
   "source": [
    "if not os.path.isdir(os.path.join(\".\", \"models\")):\n",
    "    os.mkdir(os.path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = os.path.join(\".\", \"models\", f\"{Config.NB_NAME}_acc_{last_val_score*100:.6f}.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a978b0e6-b773-423a-93e4-ce463f4d4d84",
   "metadata": {},
   "source": [
    "## Inference\n",
    "테스트 데이터셋에 대한 추론은 다음 순서로 진행됩니다.\n",
    "\n",
    "1. 모델 및 디바이스 설정\n",
    "    - 모델을 주어진 device(GPU 또는 CPU)로 이동시키고, 평가모드로 전환합니다.\n",
    "2. 예측 수행\n",
    "    - 예측 결과를 저장한 빈 리스트를 초기화하고 test_loader에서 배치별로 데이터를 불러와 예측을 수행합니다.\n",
    "    - 각 배치에 대해 스펙트로그램 데이터를 device로 이동시킵니다.\n",
    "    - 모델 예측 확률(probs)을 계산합니다.\n",
    "    - 예측 확률을 predictions리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f2c38ede8ead0c7",
   "metadata": {},
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "321d8ce1",
   "metadata": {},
   "source": [
    "predicted_labels = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features in tqdm(test_loader):\n",
    "        probs = model(features.to(device))\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        predicted_labels += probs.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8fae66d-8f54-46d5-9201-0f4b0db76e76",
   "metadata": {},
   "source": [
    "### Submission\n",
    "추론 결과를 제출 양식에 덮어 씌워 CSV 파일로 생성하는 과정은 다음과 같습니다.\n",
    "\n",
    "1. 제출 양식 로드\n",
    "    - pd.read_csv('./sample_submission.csv')를 사용하여 제출을 위한 샘플 형식 파일을 로드합니다.\n",
    "    - 이 파일은 일반적으로 각 테스트 샘플에 대한 ID와 예측해야 하는 필드가 포함된 템플릿 형태를 가지고 있습니다.\n",
    "2. 예측 결과 할당\n",
    "    - submit.iloc[:,1:] = preds 추론함수(inference)에서 반환된 예측결과(preds)를 샘플 제출 파일에 2번째 열부터 할당합니다.\n",
    "3. 제출 파일 저장\n",
    "    - 수정된 제출 파일을 baseline_submit 이란 이름의 CSV 파일로 저장합니다.\n",
    "    - index=False는 파일 저장시 추가적인 index가 발생하지 않도록 설정하여, 제작한 제출 파일과 동일한 형태의 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f8314c4-1dce-4f79-9f3d-77d320a3746e",
   "metadata": {},
   "source": [
    "submit = pd.read_csv(test_dataset.submission_form_path)\n",
    "submit.iloc[:, 1:] = predicted_labels\n",
    "submit.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e28d71bc-6703-40f7-9716-a0ef897eca83",
   "metadata": {},
   "source": [
    "submission_dir = \"submissions\"\n",
    "if not os.path.isdir(submission_dir):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = os.path.join(\".\", submission_dir, f\"{Config.NB_NAME}_acc_{last_val_score*100:.6f}_submit.csv\")\n",
    "submit.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50182ad69c2bf4f9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732842,
     "sourceId": 8066583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1830.928153,
   "end_time": "2024-04-08T19:22:15.265404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T18:51:44.337251",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a8f214ec354c44b73d439565382278": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06a1ede084cd487ebf3c469be657b53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80013ce73542415e82be091acccb89fe",
        "IPY_MODEL_d280070ca871485fbd2b7d34b1c9fd10",
        "IPY_MODEL_8212bde7695f494cbabea66983e4cf29"
       ],
       "layout": "IPY_MODEL_c4da594b806c4c2bbff6e8cdaf6088eb"
      }
     },
     "37e28ba3d8564da4a3257c3729310584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80013ce73542415e82be091acccb89fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_95e72a34a4374fd5b4b147772085bb7c",
       "placeholder": "​",
       "style": "IPY_MODEL_37e28ba3d8564da4a3257c3729310584",
       "value": "model.safetensors: 100%"
      }
     },
     "8212bde7695f494cbabea66983e4cf29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9e4e04bb60e40d6b46d782f8156d05f",
       "placeholder": "​",
       "style": "IPY_MODEL_b1fa83d0511a4d8a910b8fdb40d32c29",
       "value": " 36.5M/36.5M [00:01&lt;00:00, 41.1MB/s]"
      }
     },
     "95e72a34a4374fd5b4b147772085bb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1fa83d0511a4d8a910b8fdb40d32c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4da594b806c4c2bbff6e8cdaf6088eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d280070ca871485fbd2b7d34b1c9fd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01a8f214ec354c44b73d439565382278",
       "max": 36494688,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcd2393d73d14514851a7d9ef50315fc",
       "value": 36494688
      }
     },
     "d9e4e04bb60e40d6b46d782f8156d05f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcd2393d73d14514851a7d9ef50315fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
